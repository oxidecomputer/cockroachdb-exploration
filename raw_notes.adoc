// Include a Table of Contents on the left hand side.
:toc: left
// ":icons: font" is needed for adminition and callout icons.
:icons: font

= Raw notes

== Up to 2020-08-25

* basic research: AWS, Terraform, CockroachDB deployment, OmniOS
** find / build a simple load generator. (will use "cockroach workload" for now)
** calculate how much it will cost to run a small cluster on AWS for a while.
*** three instances
*** Prometheus
*** Grafana
*** 1-3 load generators
* Successfully used Terraform to provision a 3-node CockroachDB cluster on AWS, using OmniOS and https://sysmgr.org/~jclulow/tmp/cockroach.tar.gz[Joshua's CRDB binaries].  You need to manually run `cockroach init --insecure --host ...` on one of them to get the cluster to come up.

== 2020-08-26

* fixed bugs in Terraform config
** cockroachdb SMF service was disabled on reboot (was using `svcadm enable -t`)
** `terraform apply` could fail if the VPC subnet wound up in us-west-2d because our instance types aren't supported there
** it would be convenient if the instance names didn't have spaces
** it would be convenient if there were a single tag for all of our instances so
we could select them without relying on my specific key
* successful cold start
* lots of NTP issues: see GitHub issue #1.  These appear to be mitigated.

== 2020-08-27

Summary of the day:

* Ran into a lot of issues with NTP.  Installed Chrony.  The issues appear
  resolved.
* Got workloads running.  Exercised a bunch of the options for duration, ramp-up time, percent reads, etc.

Details follow.

* Three databases, 1 load generator.  Each load generator can only be pointed at one database, so this shouldn't be too heavy for the whole cluster, but let's see what happens.
* I'm going to start with the "kv" worklaod.

 /cockroachdb/bin/cockroach workload init kv postgres://root@192.168.1.152:26257?sslmode=disable
/cockroachdb/bin/cockroach workload run kv --histograms kv-histograms-$(date +%FT%TZ).out postgres://root@192.168.1.152:26257?sslmode=disable

Things to play with:

--ramp
--max-rate
--max-ops
--read-percent
--tolerate-errors

I let that run for about 25-30 minutes.  End of the run:

[source,text]
----
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
 1661.0s        0          830.6          802.5      4.7      6.3      9.4     24.1 write
 1662.0s        0          827.3          802.5      4.7      6.3      9.4     16.8 write
 1663.0s        0          820.8          802.5      4.7      6.6     12.1     17.8 write
 1664.0s        0          808.1          802.5      4.7      6.6     11.5     16.8 write
 1665.0s        0          789.3          802.5      5.0      7.1      9.4     16.3 write
 1666.0s        0          764.4          802.5      5.0      7.3     11.0     16.8 write
 1667.0s        0          806.0          802.5      5.0      6.8      8.9     15.7 write
 1668.0s        0          803.0          802.5      4.7      6.6     11.0     23.1 write
 1669.0s        0          787.9          802.5      5.0      6.8      8.4     18.9 write
 1670.0s        0          809.2          802.5      5.0      6.8      9.4     12.1 write
 1671.0s        0          799.8          802.5      5.0      7.1      9.4     15.7 write
 1672.0s        0          838.8          802.5      4.7      6.3     11.0     19.9 write
 1673.0s        0          840.4          802.5      4.5      6.3     11.0     16.3 write
 1674.0s        0          806.9          802.5      4.7      7.3      9.4     14.7 write
^CHighest sequence written: 1343922. Can be passed as --write-seq=R1343922 to the next run.

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
 1674.6s        0        1343922          802.5      5.0      4.7      6.8     11.5     65.0  write

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__result
 1674.6s        0        1343922          802.5      5.0      4.7      6.8     11.5     65.0
----

This created kv-histograms-2020-08-27T17:29:16Z.out.

I'm going to try it again for a few minutes to see if the initial spike in latency is one-time or not.

[source,text]
----
$ /cockroachdb/bin/cockroach workload run kv --histograms kv-histograms-$(date +%FT%TZ).out --duration 5m postgres://root@192.168.1.152:26257?sslmode=disable 

...

Highest sequence written: 239288. Can be passed as --write-seq=R239288 to the next run.

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
  300.0s        0         239284          797.6      5.0      4.7      6.8     12.6    125.8  write

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__result
  300.0s        0         239284          797.6      5.0      4.7      6.8     12.6    125.8
----

This created kv-histograms-2020-08-27T17:59:04Z.out.

The latency spike up front happened again.

Let's try out the --max-rate option to place a cap at 500 operations.  (I accidentally used --max-ops first, which exited quickly!)

cockroachdb@ip-192-168-1-192:~$ /cockroachdb/bin/cockroach workload run kv --histograms kv-histograms-$(date +%FT%TZ).out --max-rate 500 postgres://root@192.168.1.152:26257?sslmode=disable 

That seemed to work reasonably well.  There are a ton of metrics in the Admin UI dashboard!

[source,text]
----
_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
  362.6s        0         178192          491.4      4.0      3.7      5.8     12.1     88.1  write

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__result
  362.6s        0         178192          491.4      4.0      3.7      5.8     12.1     88.1  
----

This created kv-histograms-2020-08-27T18:08:37Z.out.

Let's try `--ramp`.  I used 30s first, but that's too fast to really see the effect.  I'm going to try this again with 5m.

[source,text]
----
cockroachdb@ip-192-168-1-192:~$ /cockroachdb/bin/cockroach workload run kv --histograms kv-histograms-$(date +%FT%TZ).out --ramp=5m --max-rate 700 postgres://root@192.168.1.152:26257?sslmode=disable 
...
^CHighest sequence written: 588373. Can be passed as --write-seq=R588373 to the next run.

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
  655.4s        0         435950          665.2      4.7      4.5      6.6     11.5     56.6  write

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__result
  655.4s        0         435950          665.2      4.7      4.5      6.6     11.5     56.6
----

This created kv-histograms-2020-08-27T18:18:53Z.out.  That seemed to do what I expected -- ramped up over several minutes and capped around 700.

The histogram file looks to be per-second histograms.

I want to throw some reads into the mix, but one of the nodes has become "suspect" because its clock is too far off.  I'm starting to get:

[source,text]
----
W200827 18:41:29.280504 1064 kv/kvserver/replica_range_lease.go:555  [n2,s2,r10/
3:/Table/1{4-5}] can't determine lease status of (n2,s2):3 due to node liveness
error: node not in the liveness table
(1) attached stack trace
  | github.com/cockroachdb/cockroach/pkg/kv/kvserver.init
  |     /ws/cockroach/gopath/src/github.com/cockroachdb/cockroach/pkg/kv/kvserve
r/node_liveness.go:44
  | runtime.doInit
  |     /opt/go/1.14.4/src/runtime/proc.go:5420
  | runtime.doInit
  |     /opt/go/1.14.4/src/runtime/proc.go:5415
  | runtime.doInit
  |     /opt/go/1.14.4/src/runtime/proc.go:5415
  | runtime.doInit
  |     /opt/go/1.14.4/src/runtime/proc.go:5415
  | runtime.doInit
  |     /opt/go/1.14.4/src/runtime/proc.go:5415
  | runtime.main
  |     /opt/go/1.14.4/src/runtime/proc.go:190
  | runtime.goexit
  |     /opt/go/1.14.4/src/runtime/asm_amd64.s:1373
----

Two of them have gone into maintenance now.

Several hours later: I've built and deployed chrony to these boxes to see if
this goes better.  Let's go ahead and run that mixed workload I wanted to do
next.

[source,text]
----
$ /cockroachdb/bin/cockroach workload run kv --histograms kv-histograms-$(date +%FT%TZ).out --read-percent=30 --ramp=5m postgres://root@192.168.1.152:26257?sslmode=disable 
...
^CNumber of reads that didn't return any results: 2.
Highest sequence written: 2550079. Can be passed as --write-seq=R2550079 to the next run.

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
 3465.5s        0        1028361          296.7      2.0      1.9      3.0      5.0     67.1  read

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
 3465.5s        0        2395944          691.4      4.9      4.7      6.8     11.0    201.3  write

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__result
 3465.5s        0        3424305          988.1      4.0      4.5      6.6     10.0    201.3
----

I let this run for about an hour.  This created kv-histograms-2020-08-27T22:54:19Z.out.  Note that this file has two lines per second -- one for reads and ones for writes.

The clocks are consistently within 1ms of each other now (!).  This workload is running well.

At about 2020-08-27T23:16Z, I activated statement diagnostics for the UPSERT that this thing is running to see what it does.  This produced a bundle that was 23 bytes (0 bytes downloaded, for some reason).  This looks like this bug fixed in https://www.cockroachlabs.com/docs/releases/v20.2.0-alpha.3.html[v20.2.0-alpha.3]:

> Fixed a bug causing the raw trace file collected inside a statement diagnostics bundle to be sometimes empty when the cluster setting sql.trace.txn.enable_threshold was in use. #50914

although in our case `sql.trace.txn.enable_threshold` is 0 (disabled).  Maybe not the same issue.

== 2020-08-31

Went through:

* https://www.cockroachlabs.com/docs/v20.1/learn-cockroachdb-sql.html[Learn CockroachDB SQL] (this was just basic SQL)
** https://www.cockroachlabs.com/docs/v20.1/developer-guide-overview.html[Developer Guide]
** Skipped exercises under https://www.cockroachlabs.com/docs/v20.1/deploy-a-test-cluster.html[Test deployment] -- these were too basic or exercised K8s behavior.
** Skimmed the https://www.cockroachlabs.com/docs/v20.1/performance.html[Performance Guide]
** https://www.cockroachlabs.com/docs/v20.1/monitoring-and-alerting.html[Prometheus stuff]
** Skimmed https://www.cockroachlabs.com/docs/v20.1/configure-replication-zones.html[Replication Zones]
** https://www.cockroachlabs.com/docs/v20.1/manage-long-running-queries.html[Long-running queries]
** Read through https://www.cockroachlabs.com/docs/v20.1/remove-nodes.html[Decommision nodes]
** Read through https://www.cockroachlabs.com/docs/v20.1/disaster-recovery.html[disaster recovery]
** Skimmed through https://www.cockroachlabs.com/docs/v20.1/troubleshooting-overview.html[Troubleshooting section]

Exercised replication + rebalancing tutorial:

* Started with a cluster with 65 ranges: internal data + some poking around with the "movr" dataset.
* That's 65 ranges with replication factor 3 divided across 3 nodes = 65 replicas per node (confirmed).
* Started a fourth node: expect ~48 replicas per node (65 ranges times replication factor 3 divided by 4 nodes)
* Final state: between 46 - 50 replicas per node.  Stopped slightly before I expected, but well within reasonable.

Now I want to decommission that fourth node.

```
/cockroachdb/bin/cockroach node decommission 4 --insecure --host 192.168.1.46
...
  id | is_live | replicas | is_decommissioning |   membership   | is_draining
-----+---------+----------+--------------------+----------------+--------------
   4 |  true   |        0 |        true        | decommissioned |    false
(1 row)

No more data reported on target nodes. Please verify cluster health before removing the nodes.
```

For good measure, I drained it before disabling it:

```
root@ip-192-168-1-46:~# /cockroachdb/bin/cockroach node drain --insecure --host 192.168.1.46
node is draining... remaining: 1
node is draining... remaining: 0 (complete)
ok
root@ip-192-168-1-46:~# svcadm disable -s cockroachdb
root@ip-192-168-1-46:~#
```

Then I removed it with Terraform.  (Fortunately, just decrementing the count of db nodes caused Terraform to want to destroy this one and not some other one.)

After a few minutes, the UI reports the node as decommissioned.

---

I'm now switching over to fleshing out more of the deployment: Prometheus + Grafana for better situational awareness, plus haproxy so I can do more interesting load testing like shutting off individual nodes.

---

Prometheus:
* building from scratch for illumos
** need: golang, nodejs, yarn
*** added OmniOSce "extra" publisher
*** installed golang 1.14 (plus add path)
*** installed nodejs 12
*** used `npm install -g yarn` (plus add path)
*** needed to install gnu-tar and put that onto PATH before tar
*** needed to set TMPDIR=/var/tmp because /tmp isn't big enough.
*** needed to build `promu` first because the build doesn't have a binary for that but doesn't handle that case.  See https://elatov.github.io/2020/04/monitoring-other-targets-with-prometheus/#compiling-node_exporter-on-omnios[here].  Worked around as described there, by pulling `promu` source.
*** also needed to apply patch below to client_unix.go.

[source,text]
----
diff --git a/vendor/github.com/docker/docker/client/client_unix.go b/vendor/github.com/docker/docker/client/client_unix.go
index 178ff6740..69fb1b48f 100644
--- a/vendor/github.com/docker/docker/client/client_unix.go
+++ b/vendor/github.com/docker/docker/client/client_unix.go
@@ -1,4 +1,4 @@
-// +build linux freebsd openbsd netbsd darwin dragonfly
+// +build linux freebsd openbsd netbsd darwin dragonfly illumos

 package client // import "github.com/docker/docker/client"
----

Grafana: huge pain, but ultimately:
* need at least 8G of memory (!)
* install yarn, node, go, etc.
* git clone
* git checkout # tag you want
* `rm -rf packages/grafana-e2e`
* `yarn install --pure-lockfile` or whatever
* `yarn start` or whatever (might be able to use `go run build.go build-frontend` instead)
* `go run build.go build`
* `go run build.go pkg-archive`
* (appeared to be missing `make build` (for `make build-js`) there?)

== 2020-09-01

* Set up elastic IP for my dev zone.  This looks like about $44/year if my instance were off the whole year, which seems reasonable.
* Set up manual deployment of Prometheus and Grafana in "mon" VM
** use user called "mon" for Prometheus and Grafana
** /export/home/mon/{bin,etc,grafana,var/prometheus/data}

So it will look like:

[source,text]
----
/export/home/mon/bin/prometheus
/export/home/mon/etc/prometheus.yml
/export/home/mon/var/prometheus/data/...
/export/home/mon/grafana/
----

(note: I changed this on 9/2 to separate Prometheus and Grafana into their own directories because they seem more oriented around that approach and it's not clear there's much value in following the traditional system package manager layout here.)

prometheus to be invoked as: prometheus --storage.tsdb.path=... --config.file=... &
refresh: kill -HUP?

NOTE: cockroachdb on one node went into maintenance on boot again because of clock issues.  This time, chrony had definitely finished starting before cockroachdb went into maintenance.  Is this going to be a serious problem?

I did eventually get Prometheus set up pulling from CockroachDB.

I tried running Grafana, but found that my build was busted in a way that only fails when you go to configure a data source in the web UI.

Finally got that fixed and updated instructions above.

== 2020-09-02 and 2020-09-03

Working to automate the deployment of Prometheus and Grafana to a dev zone.  This included a bunch of changes:

* refactored "vminit" directory and created a janky build that creates a "common" tarball for chrony and role-specific tarballs for the database/loadgen and monitoring VMs.
** refactored directory structure of "mon" VM from what's above
** built "fetcher" command to fetch asset from S3
** updated Terraform to configure IAM to support this
** updated vminit.sh to use "fetcher" and reflect the rest of these changes
* incorporated Prometheus
** with config to automatically discover EC2 instances in this project
** with config to scrape Grafana too
** updated Terraform to configure IAM to support this
* incorporated Grafana
** including our Prometheus data source
** including stock Prometheus, Grafana, and CockroachDB dashboards.  This involved manually fixing them to remove DS_PROMETHEUS/DS_NAME inputs -- see the README in that directory.
* various improvements:
** more useful hostnames for VMs (though this is not currently persistent)
** created "env.sh" file with various useful aliases

== 2020-09-04

* added Prometheus node_exporter (see [prometheus/node_exporter#1836](https://github.com/prometheus/node_exporter/issues/1836))
* built out a Grafana dashboard to show key metrics.  Discovered [prometheus/node_exporter#1837](https://github.com/prometheus/node_exporter/issues/1837).

Still, I think I'm just about ready to do some more serious testing.

== 2020-09-08

Summary:

* Switched to Joshua's OmniOS image running his metadata agent: AMI
  `ami-012f34b61b75182e8`.
* Updated Terraform config to deploy much larger root disks.
* Spent some time automating disk and zpool expansion to match provisioned size before realizing that Josh's image already does this.
* Recreated dashboard from Friday
* Ran a bunch of tests:
** ycsb workload: increasing levels of concurrency
** The workload appeared largely bottlenecked on one db node, so I went to experiment with a much larger DB and adding splits.
** I ran into a lot of different errors trying to make this work.  I'm not sure what the root cause really was except stuff being really busy?
** The "kv" workload might be easier to run and just as useful a next step.

Around 9am PT, ran:

[source,text]
----
$ cockroach workload run ycsb --concurrency=1 --drop --histograms histograms-ycsbA-c=1-"$(date +%FT%TZ)".out --tolerate-errors --workload A
----

I let this run for an hour.

Around 1pm PT, I ran:

[source,text]
----
$ cockroach workload run ycsb --concurrency=1 --drop --histograms histograms-ycsbA-c=1-"$(date +%FT%TZ)".out --tolerate-errors --workload A --duration 5m
----

Around 1:16PM, I'm running:

[source,text]
----
$ cockroach workload run ycsb --concurrency=2 --drop --histograms histograms-ycsbA-c=2-"$(date +%FT%TZ)".out --tolerate-errors --workload A --duration 5m
----

Around 1:23PM, I'm running:

[source,text]
----
$ cockroach workload run ycsb --concurrency=4 --drop --histograms histograms-ycsbA-c=4-"$(date +%FT%TZ)".out --tolerate-errors --workload A --duration 5m
----

Around 1:34PM, I'm running:

[source,text]
----
$ cockroach workload run ycsb --concurrency=8 --drop --histograms histograms-ycsbA-c=8-"$(date +%FT%TZ)".out --tolerate-errors --workload A --duration 5m
----

Around 1:42PM, I'm running:

[source,text]
----
$ cockroach workload run ycsb --concurrency=16 --drop --histograms histograms-ycsbA-c=16-"$(date +%FT%TZ)".out --tolerate-errors --workload A --duration 5m
----

At this point, db0 CPUs exceeded 90% CPU utilization.  It's busier than all the other nodes, by a lot.  Let's see what happens if we go further.

Around 1:51PM:

[source,text]
----
$ cockroach workload run ycsb --concurrency=32 --drop --histograms histograms-ycsbA-c=32-"$(date +%FT%TZ)".out --tolerate-errors --workload A --duration 5m
----

It's actually degraded okay at this point, by which I mean that throughput did actually increase and p95/p99 didn't get extremely bad.  I note that in the CRDB Admin UI, nearly all of the queries have hit the "n1" store today.  Only during this last workload did we see any queries hit another store, and it was n3.  Maybe CRDB is dynamically splitting by load?

Note that during this workload is where we start seeing replica errors and more "not leaseholder" errors than before.

Digging further into AdminUI, this database is only 128 MiB, with 4 ranges.  It's not shocking that it's not that distributed.

What if we go further?

At 1:58PM PT:

[source,text]
----
$ cockroach workload run ycsb --concurrency=64 --drop --histograms histograms-ycsbA-c=64-"$(date +%FT%TZ)".out --tolerate-errors --workload A --duration 5m
----

The results look similar to the previous one.  Throughput is less consistent, but hasn't gotten below the point where it was previously consistent.  We did seem to see some SQL 'exec_error's, but I don't see them in the client.  This graph in Grafana also doesn't seem totally consistent with the one in CockroachDB's Admin UI.  It's correlated, though.

Why not go further and see how this goes?

At 2:08 PM PT:

[source,text]
----
$ cockroach workload run ycsb --concurrency=128 --drop --histograms histograms-ycsbA-c=128-"$(date +%FT%TZ)".out --tolerate-errors --workload A --duration 5m
----

This one definitely saw spikes in SQL "exec_error", and potentially worse throughput than the previous one.  All db nodes are pretty tapped at this point.

I want to see what happens with this workload if I tune up the split count and total row count, since it seems pretty pokey right now.  I'm going to go back to concurrency 16, which is one step past 8, which was the stablest and most consistent.

[source,text]
----
$ cockroach workload init ycsb --splits 8 --concurrency=16 --drop --insert-count=1000000 --workload A
$ cockroach workload  run ycsb --splits 8 --concurrency=16 --drop --insert-count=1000000 --workload A --histograms histograms-ycsbA-c=16-"$(date +%FT%TZ)".out --tolerate-errors --duration 1h
----

The loading step is taking quite a while.  It's hammering both CPUs on one database node (so, concurrency=1, I guess)?

While this was going on, I was able to:

[source,text]
----
root@192.168.1.118:26257/ycsb> select count(*) from usertable;
  count
----------
  438000
(1 row)

Time: 51.558088946s
----

But when I tried this later, I got a strange error:

[source,text]
----
root@192.168.1.118:26257/ycsb> select count(*) from usertable;
ERROR: driver: bad connection
warning: connection lost!
opening new connection: all session settings will be lost
root@192.168.1.118:26257/ycsb>
----

I'm not sure which host I was connected to.  I checked all three logs but didn't see anything obvious.

The `init` command failed after 20 minutes with:

[source,text]
----
cockroachdb@loadgen0:~$ time cockroach workload init ycsb --splits 8 --concurrency=16 --drop --insert-count=1000000 --workload A
Error: failed insert into usertable: pq: split failed while applying backpressure to [txn: 4705c25f], ConditionalPut [/Table/81/1/"user4211402063788639270"/0,/Min), ConditionalPut [/Table/81/1/"user4211402063788639270"/1/1,/Min), ConditionalPut [/Table/81/1/"user4211402063788639270"/2/1,/Min), ConditionalPut [/Table/81/1/"user4211402063788639270"/3/1,/Min), ConditionalPut [/Table/81/1/"user4211402063788639270"/4/1,/Min), ConditionalPut [/Table/81/1/"user4211402063788639270"/5/1,/Min), ConditionalPut [/Table/81/1/"user4211402063788639270"/6/1,/Min), ConditionalPut [/Table/81/1/"user4211402063788639270"/7/1,/Min), ConditionalPut [/Table/81/1/"user4211402063788639270"/8/1,/Min), ConditionalPut [/Table/81/1/"user4211402063788639270"/9/1,/Min), ConditionalPut [/Table/81/1/"user4211402063788639270"/10/1,/Min), ConditionalPut [/Table/81/1/"user8166519625965030401"/0,/Min), ConditionalPut [/Table/81/1/"user8166519625965030401"/1/1,/Min), ConditionalPut [/Table/81/1/"user8166519625965030401"/2/1,/Min), ConditionalPut [/Table/81/1/"user8166519625965030401"/3/1,/Min), ConditionalPut [/Table/81/1/"user8166519625965030401"/4/1,/Min), ConditionalPut [/Table/81/1/"user8166519625965030401"/5/1,/Min), ConditionalPut [/Table/81/1/"user8166519625965030401"/6/1,/Min), ConditionalPut [/Table/81/1/"user8166519625965030401"/7/1,/Min), ConditionalPut [/Table/81/1/"user8166519625965030401"/8/1,/Min), ... 10976 skipped ..., ConditionalPut [/Table/81/1/"user6890362626482376666"/7/1,/Min), ConditionalPut [/Table/81/1/"user6890362626482376666"/8/1,/Min), ConditionalPut [/Table/81/1/"user6890362626482376666"/9/1,/Min), ConditionalPut [/Table/81/1/"user6890362626482376666"/10/1,/Min), EndTxn(commit:true tsflex:true) [/Table/81/1/"user4211402063788639270"/0]  on range r101:/{Table/81-Max} [(n1,s1):1, (n2,s2):2, (n3,s3):3, next=4, gen=42]: operation "split queue process replica 101" timed out after 1m0s: split at key /Table/81/1/"user1430647350823960411" failed: context deadline exceeded

real    19m28.125s
user    0m15.002s
sys     0m2.580s
----

Amusing sideshow:

[source,text]
----
root@192.168.1.118:26257/ycsb> select count(*) from usertable;
invalid syntax: statement ignored: unexpected error: read tcp 192.168.1.118:54604->192.168.1.118:26257: read: connection reset by peer
warning: error retrieving the transaction status: driver: bad connection
warning: connection lost!
opening new connection: all session settings will be lost
root@192.168.1.118:26257/ycsb ?>
----

But ultimate it had created 714,000 rows:

[source,text]
----
select count(*) from usertable;
  count
----------
  714000
(1 row)

Time: 12.48402931s
----

Details on that https://www.cockroachlabs.com/docs/stable/common-errors.html#context-deadline-exceeded[context deadline exceeded] error.

The database is at least 4 GiB now, although ycsb is only 1.6 GiB (maybe that's logical?)

The database is at least 4 GiB now, although ycsb is only 1.6 GiB (maybe that's logical?).  There's only one range, though.

Resuming with:

[source,text]
----
$ time cockroach workload init ycsb --splits 8 --concurrency=16 --insert-start 714000 --insert-count=1000000 --workload A
----

I realized that isn't right -- the insert count needs to be adjusted.  Tried to get a new count and got:

[source,text]
----
root@192.168.1.118:26257/ycsb> select count(*) from usertable;
ERROR: driver: bad connection
warning: connection lost!
opening new connection: all session settings will be lost
----

This is repeatable.  Is this an haproxy timeout?  I didn't reproduce it (one time) hitting a CRDB node directly.

So now:

[source,text]
----
$ time cockroach workload init ycsb --splits 8 --concurrency=16 --insert-start 714000 --insert-count=286000 --workload A
Error: failed insert into usertable: pq: duplicate key value (ycsb_key)=('user10357802244052365217') violates unique constraint "primary"

real    1m7.990s
user    0m0.787s
sys     0m0.831s
----

Yeesh.

I'm seeing this repeatedly now, even when I bump the count up.  When I bumped it way up:

[source,text]
----
cockroachdb@loadgen0:~$ time cockroach workload init ycsb --splits 8 --concurrency=16 --insert-start 800000 --insert-count=200000 --workload A
Error: failed insert into usertable: driver: bad connection

real    3m1.377s
user    0m1.018s
sys     0m0.641s
----

I'm going to try without going through haproxy.

[source,text]
----
$ time cockroach workload init ycsb --splits 8 --concurrency=16 --insert-start 900000 --insert-count=100000 --workload A postgresql://root@192.168.1.104:26257/ycsb?sslmode=disable
----

This ultimately failed with another constraint violation error.  There are now 721,000 rows in `usertable`.

For kicks, I'm going to start the above workload anyway to see how it goes.  Tomorrow, I'll probably reset and do the "kv" workload.  This should have a few advantages because it doesn't do so much work during the "init" phase.  That's good because this phase is harder to observe and not parallelized, as far as I can tell.

It may still be worth digging into the ycsb issues to better understand how things fail when they go wrong.  It would be good to better understand what SQL it's running (how many rows is it trying to insert at once?), with what concurrency, how long those INSERTs are taking, etc.

I realized as I started this that I wasn't sure the splits had been applied.  So I'll run this:

[source,text]
----
$ time cockroach workload init ycsb --splits 8 --concurrency=16 --insert-count=0 --workload A
I200908 22:38:20.458190 1 workload/workloadsql/workloadsql.go:113  starting 8 splits

real    0m1.885s
user    0m0.106s
sys     0m0.058s
$ cockroach workload  run ycsb --splits 8 --concurrency=16 --workload A --histograms histograms-ycsbA-c=16-"$(date +%FT%TZ)".out --tolerate-errors --duration 1h
----

Incidentally, this command's documentation is rather confused.  Some of these (like `--splits`) apply at init time, but that's not clear.  Other things are just documented wrong (`--insert-start` vs. `--initial-count`).

A few minutes into this workload (around 3:47pm PT), the Grafana metrics tanked.  Activity went to zero, CPU utilization is no longer reported.  All services in all VMs appear to be running as normal.  The workload is reporting a bunch of successful operations per second!

It looks like the "mon" zone ran out of disk space.  It's still got a 2 GiB disk for some reason, even though the disk is 10 GiB.  The other nodes had this problem earlier, and rebooting fixed it because Joshua's image automatically expands the pool to match the physical size.  Maybe I forgot to reboot this one?  Anyway, I made the mistake of trying to fix this by rebooting it.  I doubt this will work because it probably won't be able to come up with 0 bytes available.  I may have to redeploy this VM, in which case I'll have lost today's testing data.  I do have screenshots and the client-side data, if it's really important.  It's also presumably reproducible.

I redeployed this zone (having saved the dashboard JSON!).  As the workload is running now (see above): CPU utilization is high for all CPUs on all db nodes (77%-90%).  db1 is a little lower -- closer to the 77% level.  Queries aren't perfectly distributed across the nodes, but it's not bad.  Average throughput is about 1K selects + 1K updates per second, which is a little less than c=16 earlier today, but the database is much bigger now.

Throughput dropped to zero for a while and spat this out:

[source,text]
----
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
 2101.0s        0         1157.6         1059.4      3.3      7.3     13.1     21.0 read
 2101.0s        0         1139.6         1058.5     10.0     17.8     28.3     46.1 update
 2102.0s        0         1123.4         1059.4      3.4      7.9     18.9     28.3 read
 2102.0s        0         1092.3         1058.6     10.5     18.9     33.6     52.4 update
 2103.0s        0         1117.0         1059.5      3.3      6.8     15.7     37.7 read
 2103.0s        0         1117.0         1058.6     10.0     19.9     28.3     35.7 update
 2104.0s        0         1229.1         1059.5      3.5      6.8     11.0     23.1 read
 2104.0s        0         1145.1         1058.6     10.0     16.3     24.1     30.4 update
 2105.0s        0         1192.1         1059.6      3.4      8.4     14.2     26.2 read
 2105.0s        0         1083.1         1058.6     10.0     19.9     29.4     39.8 update
 2106.0s        0         1160.1         1059.6      3.4      7.1     14.2     26.2 read
 2106.0s        0         1146.1         1058.7     10.0     17.8     26.2     35.7 update
 2107.0s        0         1131.9         1059.7      3.4      7.6     13.6     27.3 read
 2107.0s        0         1129.9         1058.7     10.0     18.9     28.3     35.7 update
 2108.0s        0         1142.0         1059.7      3.4      8.1     14.2     35.7 read
 2108.0s        0         1120.0         1058.7     10.0     18.9     26.2     39.8 update
 2109.0s        0         1155.2         1059.8      3.4      7.1     10.5     21.0 read
 2109.0s        0         1207.2         1058.8     10.0     16.3     21.0     31.5 update
 2110.0s        0         1154.4         1059.8      3.4      8.4     16.3     23.1 read
 2110.0s        0         1056.4         1058.8     10.0     21.0     32.5     48.2 update
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
 2111.0s        0         1134.6         1059.8      3.3      6.8     17.8     35.7 read
 2111.0s        0         1124.6         1058.8     10.0     17.8     28.3     60.8 update
 2112.0s        0            0.0         1059.3      0.0      0.0      0.0      0.0 read
 2112.0s        0            0.0         1058.3      0.0      0.0      0.0      0.0 update
 2113.0s        0            0.0         1058.8      0.0      0.0      0.0      0.0 read
 2113.0s        0            0.0         1057.8      0.0      0.0      0.0      0.0 update
 2114.0s        0            0.0         1058.3      0.0      0.0      0.0      0.0 read
 2114.0s        0            0.0         1057.3      0.0      0.0      0.0      0.0 update
 2115.0s        0            0.0         1057.8      0.0      0.0      0.0      0.0 read
 2115.0s        0            0.0         1056.8      0.0      0.0      0.0      0.0 update
 2116.0s        0            0.0         1057.3      0.0      0.0      0.0      0.0 read
 2116.0s        0            0.0         1056.3      0.0      0.0      0.0      0.0 update
 2117.0s        0            0.0         1056.8      0.0      0.0      0.0      0.0 read
 2117.0s        0            0.0         1055.8      0.0      0.0      0.0      0.0 update
 2118.0s        0            0.0         1056.3      0.0      0.0      0.0      0.0 read
 2118.0s        0            0.0         1055.3      0.0      0.0      0.0      0.0 update
 2119.0s        0            0.0         1055.8      0.0      0.0      0.0      0.0 read
 2119.0s        0            0.0         1054.8      0.0      0.0      0.0      0.0 update
 2120.0s        0            0.0         1055.3      0.0      0.0      0.0      0.0 read
 2120.0s        0            0.0         1054.3      0.0      0.0      0.0      0.0 update
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
 2121.0s        0            0.0         1054.8      0.0      0.0      0.0      0.0 read
 2121.0s        0            0.0         1053.8      0.0      0.0      0.0      0.0 update
 2122.0s        0            0.0         1054.4      0.0      0.0      0.0      0.0 read
 2122.0s        0            0.0         1053.3      0.0      0.0      0.0      0.0 update
 2123.0s        0            0.0         1053.9      0.0      0.0      0.0      0.0 read
 2123.0s        0            0.0         1052.9      0.0      0.0      0.0      0.0 update
 2124.0s        0            0.0         1053.4      0.0      0.0      0.0      0.0 read
 2124.0s        0            0.0         1052.4      0.0      0.0      0.0      0.0 update
 2125.0s        0            0.0         1052.9      0.0      0.0      0.0      0.0 read
 2125.0s        0            0.0         1051.9      0.0      0.0      0.0      0.0 update
 2126.0s        0            0.0         1052.4      0.0      0.0      0.0      0.0 read
 2126.0s        0            0.0         1051.4      0.0      0.0      0.0      0.0 update
 2127.0s        0            0.0         1051.9      0.0      0.0      0.0      0.0 read
 2127.0s        0            0.0         1050.9      0.0      0.0      0.0      0.0 update
 2128.0s        0            0.0         1051.4      0.0      0.0      0.0      0.0 read
 2128.0s        0            0.0         1050.4      0.0      0.0      0.0      0.0 update
 2129.0s        0            0.0         1050.9      0.0      0.0      0.0      0.0 read
 2129.0s        0            0.0         1049.9      0.0      0.0      0.0      0.0 update
 2130.0s        0            0.0         1050.4      0.0      0.0      0.0      0.0 read
 2130.0s        0            0.0         1049.4      0.0      0.0      0.0      0.0 update
E200908 23:14:11.770407 1 workload/cli/run.go:445  pq: result is ambiguous (error=rpc error: code = Unavailable desc = transport is closing [propagate])
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
 2131.0s        1            0.0         1049.9      0.0      0.0      0.0      0.0 read
 2131.0s        1            0.0         1048.9      0.0      0.0      0.0      0.0 update
 2132.0s        3          745.8         1049.8      3.3      7.1     12.6  20401.1 read
 2132.0s        3          772.8         1048.8     10.0     18.9     27.3  20401.1 update
 2133.0s        3          836.1         1049.7      3.0      6.3      8.9     12.1 read
 2133.0s        3          864.1         1048.7      9.4     16.3     28.3     35.7 update
 2134.0s        3          815.0         1049.5      3.1      6.6     11.5     26.2 read
 2134.0s        3          808.0         1048.6      9.4     18.9     33.6  22548.6 update
 2135.0s        3          879.2         1049.5      3.0      6.6      8.9     13.1 read
 2135.0s        3          854.1         1048.5      9.4     14.7     22.0     37.7 update
 2136.0s        3          856.1         1049.4      3.1      6.3      7.6     12.1 read
 2136.0s        3          849.1         1048.4      9.4     16.3     23.1     27.3 update
 2137.0s        3          834.9         1049.3      3.0      6.6     11.0     13.6 read
 2137.0s        3          805.9         1048.3     10.0     17.8     24.1     30.4 update
 2138.0s        3          930.8         1049.2      3.1      6.3     10.5     18.9 read
 2138.0s        3          864.8         1048.2      8.9     14.7     21.0     29.4 update
 2139.0s        3          829.2         1049.1      2.9      6.0     10.0     62.9 read
 2139.0s        3          875.2         1048.1      9.4     16.3     21.0     32.5 update
 2140.0s        3          833.7         1049.0      3.0      6.8      9.4     21.0 read
 2140.0s        3          840.7         1048.0      9.4     17.8     23.1     31.5 update
----

Another one I saw was:

[source,text]
----
E200908 23:15:23.487617 1 workload/cli/run.go:445  pq: result is ambiguous (error=unable to dial n1: breaker open [exhausted])
----

Maybe I'm running too close to saturation?  Until this point, p95 latency was very steady around 18ms across all three nodes.  p99 was very steady at around 30ms across all three nodes.  Now the thing is falling apart.  I wonder if this would work better with three different load generator instances (processes, not VMs) instead of haproxy?  But these look like internal errors.


A few minutes later, the workload has recovered to where it was before.  It seems like we triggered a crash?  But the uptime on all of them shows 6 hours.  That said, there was a loss of connections to .236 and a bunch of ranges reported being uner-replicated for a minute.  CockroachDB did not actually restart on that node.  I do see some errors in the logs:

[source,text]
----
W200908 23:15:19.027322 198 kv/kvserver/node_liveness.go:592  [n3,liveness-hb] failed node liveness heartbeat: oper
ation "node liveness heartbeat" timed out after 4.5s
(1) operation "node liveness heartbeat" timed out after 4.5s
Wraps: (2) context deadline exceeded
Error types: (1) *contextutil.TimeoutError (2) context.deadlineExceededError

An inability to maintain liveness will prevent a node from participating in a
cluster. If this problem persists, it may be a sign of resource starvation or
of network connectivity problems. For help troubleshooting, visit:

    https://www.cockroachlabs.com/docs/stable/cluster-setup-troubleshooting.html#node-liveness-issues

...

I200908 23:15:19.062947 196 server/status/runtime.go:504  [n3] runtime stats: 0 B RSS, 242 goroutines, 108 MiB/1004
 MiB/269 MiB GO alloc/idle/total, 174 MiB/221 MiB CGO alloc/total, 187.1 CGO/sec, 0.0/0.0 %(u/s)time, 0.0 %gc (1x),
 0 B/0 B (r/w)net
W200908 23:15:19.482447 98 kv/kvserver/closedts/provider/provider.go:155  [ct-closer] unable to move closed timesta
mp forward: not live
(1) attached stack trace
  | github.com/cockroachdb/cockroach/pkg/kv/kvserver.init
  |     /ws/cockroach/gopath/src/github.com/cockroachdb/cockroach/pkg/kv/kvserver/node_liveness.go:60
  | runtime.doInit
  |     /opt/go/1.14.4/src/runtime/proc.go:5420
  | runtime.doInit
  |     /opt/go/1.14.4/src/runtime/proc.go:5415
  | runtime.doInit
  |     /opt/go/1.14.4/src/runtime/proc.go:5415
  | runtime.doInit
  |     /opt/go/1.14.4/src/runtime/proc.go:5415
  | runtime.doInit
  |     /opt/go/1.14.4/src/runtime/proc.go:5415
  | runtime.main
  |     /opt/go/1.14.4/src/runtime/proc.go:190
  | runtime.goexit
  |     /opt/go/1.14.4/src/runtime/asm_amd64.s:1373
Wraps: (2) not live
Error types: (1) *withstack.withStack (2) *errors.errorString
----

== 2020-09-09

Switching to "kv" workload (see yesterday's notes).

cockroach workload init kv --concurrency 4 --max-block-bytes=4096 --min-block-bytes=3072
for c in 4 8 16 32 64 128; do
    cockroach workload run kv --concurrency $c --duration 10m --histograms histograms-kv-c=$c-$(date +%FT%TZ)Z.out  --max-block-bytes=4096 --min-block-bytes=3072 --read-percent=50 --tolerate-errors
done

Notes:

* This is a considerably larger record size than I had been testing previously.
* The database is getting bigger each time this way so it's not a totally fair test among different levels of concurrency.

Results:

* The total number of SQL connections and active queries scales up with the concurrency as we'd expect.
* Starting with c=8:
** the CPU utilization graphs look about the same for all runs.
** the distribution of SQL queries to each node looks about the same.
* The SQL query throughput looks about the same among all these runs.
* The SQL query p95 latency increases with each run.
* According to `iostat`, the disk is quite busy much of the time (essentially 100% at c=64).  Occasionally, the wait time at the zpool level is upwards of 100 (ms?), but it never gets nearly that high on the actual disk.
* There was one spike in p99 SQL latency of 9s on one node (192.168.1.236 @ 15:48:30Z).

Conclusions:

* The system is basically saturated at c=4.
* Extreme outliers start around c=32.  Things really start getting nonlinear around c=128.

Possible way to go next: stick with c=16 and expand the cluster while all this is going on.  From initial 3 nodes -> 6 nodes -> 9 nodes -> 12 nodes.

17:10Z: deployed node#4.
17:14Z: the new node is definitely in service.  CPU utilization of other nodes has gone down a bit, as has query throughput.  p95/p99 latency spiked a lot.  Heartbeat latency spiked to over 5s.  Big spike in exec errors over 4Kps.
17:17Z: another spike in p95/p99 to 10s.  I don't know why this is happening -- the client isn't even updated to establish new connections so it shouldn't be using the new node.

[source,text]
----
  760.0s        0            0.0          366.8      0.0      0.0      0.0      0.0 write
E200909 17:13:18.084065 1 workload/cli/run.go:445  ERROR: result is ambiguous (error=unable to dial n2: breaker open [exhausted]) (SQLSTATE 40003)
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
  761.0s        1           21.0          364.9    130.0  13421.8  13421.8  13421.8 read
...
  969.0s        4            0.0          304.6      0.0      0.0      0.0      0.0 write
E200909 17:16:47.529391 1 workload/cli/run.go:445  EOF
  970.0s        5            6.0          303.1     13.1     35.7     35.7     35.7 read
  970.0s        5            5.0          304.3     18.9  60129.5  60129.5  60129.5 write
E200909 17:16:48.580680 1 workload/cli/run.go:445  EOF
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
  971.0s       15          115.0          302.9     17.8  60129.5  60129.5  60129.5 read
  971.0s       15          111.0          304.1     15.2    113.2  60129.5  60129.5 write
----

As of 17:19Z: consistently seeing 20 errors per second with 100-200 ops per second.

Note: Prometheus didn't pick up the new node right away.  Maybe a better methodology is to preprovision everything, then shut down a bunch?

17:22:45: restarted Prometheus

Note: replication started around 17:12:30 and finished around 17:25:Z.

At 17:30Z, I'm going to restart the load generator to force it to pick up all four nodes.

Perhaps not surprisingly?  This only made some performance worse because some ranges moved to a node that's not handling any requests.

17:28:50Z: restarted client workloads  However, despite having sent SIGHUP to haproxy, it doesn't seem to have picked up the fourth server.
17:30:40Z: restart both haproxy and load generator.
Now we do see 4 active servers in haproxy and all four CRDB nodes have connections.

Note: I just checked the CRDB Admin UI to look at "queries per store", which has previously been a proxy for whether queries were being served equally by the different nodes, and it does look well distributed both before and after the new node was added.

The performance now is decidedly worse than before I added the new node.  p95/p99 latency is definitely higher for two nodes, and about the same for the other two (even the one which wasn't up, which is weird).  Right now, the disks on db0 are 100% pegged all the time.  Disks on the other two original nodes have a fair bit of headroom.  Why is that one so busy?  Is this because the kv workload is not random?

In the Admin UI, I see that the UPSERT statements have seen 10 retries, but that's cumulative -- doesn't seem important.

I can see that all four nodes are serving a comparable number of queries, and their CPU utilization isn't to far off (and all under 30% per CPU).  But db0 is still being hammered on I/O.  db1 is going through periods where it's busy too, but not nearly as much.  Note that average service time on this disk is worse than disk on db1, even when db1's is busy too, by a factor of 3 or so (6ms for db1, 20ms for db0).  A few minutes later: the 100% busy seems to have moved to db2 and db0 was idle for a little while.  A few seconds later we're back to db0.

Maybe it would be more interesting to do just 20% writes / 80% reads.

CRDB does recommend:

> Disks must be able to achieve 500 IOPS and 30 MB/s per vCPU....Monitor IOPS for higher service times. If they exceed 1-5 ms, you will need to add more devices or expand the cluster to reduce the disk latency.

Anyway, the preformance has been the same for a while.  I'm going to shut this down and drop both databases for now in prep for future runs.  I'm also going to decommission that fourth node.

This failed:

[source,text]
----
# cockroach node decommission  4

  id | is_live | replicas | is_decommissioning | membership | is_draining
-----+---------+----------+--------------------+------------+--------------
   4 |  true   |       53 |       false        |   active   |    false
(1 row)

  id | is_live | replicas | is_decommissioning |   membership    | is_draining
-----+---------+----------+--------------------+-----------------+--------------
   4 |  true   |       53 |        true        | decommissioning |    false
(1 row)
..........
  id | is_live | replicas | is_decommissioning |   membership    | is_draining
-----+---------+----------+--------------------+-----------------+--------------
   4 |  true   |       52 |        true        | decommissioning |    false
(1 row)
...........
ERROR: connection lost.

while trying to mark as decommissioning: rpc error: code = Unavailable desc = transport is closing
Failed running "node decommission"
----

More haproxy woes?

== 2020-09-10

Not much testing today, but I'm reflecting on the issues I've hit so far.  I've hit a bunch of different client issues that seem potentially related to overload, and seen symptoms of overloaded servers (e.g., missed heartbeats).  I've potentially been pushing the system beyond its intended capacity, particularly in terms of I/O.  It _should_ handle that okay, but maybe isn't a great first test.

Maybe try a few simplifying changes:

* Provision 6 database nodes up front, but don't start CockroachDB on three of them.  (Or, disable cockroachdb on the last three before running `cockroach init`.)  This way I eliminate any disruptive change to the initially-running three (like restarting them, which my Terraform config normally does, although I had commented that out yesterday).  And I know exactly when each one starts.
* Factor out haproxy: instead of one load generator process using haproxy to talk to CRDB nodes, maybe use separate client processes pointed at specific CRDB nodes.  They won't automatically start using new nodes this way so I will have to start more up again.
* Let's not start by pushing the cluster to its limit.  Instead, let's separate out a few different questions:
** try to replicate something close to the https://www.cockroachlabs.com/docs/stable/performance.html#throughput[basic sysbench numbers that they got on AWS]?  These are much bigger machines, but even if we can just achieve that latency at a lower level of concurrency, that'd be useful.
** demonstrate horizontal scalability (_not_ necessarily online): maybe the way to think about this is: ramp up load generators until p95 latency reaches some target.  See how that point differs at different cluster sizes.
** demonstrate expanding the cluster under modest load (largely ignoring performance -- it would be enough that it doesn't get worse or experience errors)
** demonstrate shrinking the cluster under modest load (similar to expansion)
** demonstrate the impact of failures on a modest load (again, largely ignoring performance)

