// Include a Table of Contents on the left hand side.
:toc: left
// ":icons: font" is needed for admonition and callout icons.
:icons: font

= Raw notes

== Up to 2020-08-25

* basic research: AWS, Terraform, CockroachDB deployment, OmniOS
** find / build a simple load generator. (will use "cockroach workload" for now)
** calculate how much it will cost to run a small cluster on AWS for a while.
*** three instances
*** Prometheus
*** Grafana
*** 1-3 load generators
* Successfully used Terraform to provision a 3-node CockroachDB cluster on AWS, using OmniOS and https://sysmgr.org/~jclulow/tmp/cockroach.tar.gz[Joshua's CRDB binaries].  You need to manually run `cockroach init --insecure --host ...` on one of them to get the cluster to come up.

=== Planning notes

Will use AWS to start playing with it.

* Will use https://www.cockroachlabs.com/docs/v20.1/topology-basic-production["Basic Production"].
* Make sure to use the https://www.cockroachlabs.com/docs/v20.1/cockroach-start#locality[locality] flag if we end up using different AZs.

In terms of https://www.cockroachlabs.com/docs/v20.1/recommended-production-settings#software[host operating system]:

> We recommend running a glibc-based Linux distribution and Linux kernel version from the last 5 years, such as Ubuntu, Red Hat Enterprise Linux (RHEL), CentOS, or Container-Optimized OS.

We'll try illumos to see how it goes.

https://www.cockroachlabs.com/docs/v20.1/recommended-production-settings#basic-hardware-recommendations[Basic hardware recommendations]: for each vCPU, it's recommended to expect 4 GiB of RAM, 60 GiB of storage, 500 disk IOPS, and 30 MBps of disk I/O.  Recommend at least 2 vCPUS and better would be 4 vCPUs per node.  Price out a 4-vCPU node?  Avoid "burstable" or "shared-core".  Use "m" (general-purpose) or "c" ("compute-optimized").  Recommend "c5d" for use with EBS using SSD instances.

If we want to save cost significantly, we should shut down these instances when we don't actively need them to be running.  If we use "c5d", we'll probably lose local storage.  This would be a good reason to use "c5" with an EBS volume.  The perf will presumably be worse, but presumably not pathologically so, and we're more interested in ballpark / pathological figures than absolute best perf.  We probably don't need fantastic performance out of the gate to do basic fault testing, but we also don't want to see pathological behavior (e.g., due to starvation).

Adam points out that illumos won't currently run on "c5" or other generations that require ENA networking, so we should stick with "c4" for now.

https://www.cockroachlabs.com/docs/v20.1/recommended-production-settings#connection-pooling[Recommended connection pool size:] 2 * core count + ssd count.  It's unclear if this is a server-side figure or a client-side figure or what?

Considerations for later:

- file descriptor limit
- cache size

Load generators: There are several https://www.cockroachlabs.com/docs/v20.1/cockroach-workload.html[workload options].  Note that the workloads have a `--tolerate-errors` option.  Most promising seem like "bank", "kv", "tpcc", "ycsb".

In terms of images, it looks like https://omniosce.org/setup/aws[AWS AMI images are available for recent versions of OmniOS].

=== Calculating cost on AWS

Requirements:

* Use "c4large" for db and load generators (see above).
* Grafana recommends 256 MiB memory + 1 CPU.
* Prometheus seems to want 3 GiB of memory.
* Do this all in "us-west-2" (cheaper than some other regions)

Let's put Grafana + Prometheus in a single t3.medium instance.

https://calculator.aws/#/estimate?id=16e6ed9a0102c9e24880a0175edaa9eef88ac8c9[Estimate:]

* 6 "c4large" instances (3xCRDB + 3xload generators) with 60 GiB "gp2" storage each: $474 / month
* 1 "t3.medium" instance (Prometheus + Grafana): $36 / month

Total: $510 / month.  If we only use it for, say, 10 hours a week, that's only $30 / month.


== 2020-08-26

* fixed bugs in Terraform config
** cockroachdb SMF service was disabled on reboot (was using `svcadm enable -t`)
** `terraform apply` could fail if the VPC subnet wound up in us-west-2d because our instance types aren't supported there
** it would be convenient if the instance names didn't have spaces
** it would be convenient if there were a single tag for all of our instances so
we could select them without relying on my specific key
* successful cold start
* lots of NTP issues: see GitHub issue #1.  These appear to be mitigated.

== 2020-08-27

Summary of the day:

* Ran into a lot of issues with NTP.  Installed Chrony.  The issues appear
  resolved.
* Got workloads running.  Exercised a bunch of the options for duration, ramp-up time, percent reads, etc.

Details follow.

* Three databases, 1 load generator.  Each load generator can only be pointed at one database, so this shouldn't be too heavy for the whole cluster, but let's see what happens.
* I'm going to start with the "kv" worklaod.

 /cockroachdb/bin/cockroach workload init kv postgres://root@192.168.1.152:26257?sslmode=disable
/cockroachdb/bin/cockroach workload run kv --histograms kv-histograms-$(date +%FT%TZ).out postgres://root@192.168.1.152:26257?sslmode=disable

Things to play with:

--ramp
--max-rate
--max-ops
--read-percent
--tolerate-errors

I let that run for about 25-30 minutes.  End of the run:

[source,text]
----
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
 1661.0s        0          830.6          802.5      4.7      6.3      9.4     24.1 write
 1662.0s        0          827.3          802.5      4.7      6.3      9.4     16.8 write
 1663.0s        0          820.8          802.5      4.7      6.6     12.1     17.8 write
 1664.0s        0          808.1          802.5      4.7      6.6     11.5     16.8 write
 1665.0s        0          789.3          802.5      5.0      7.1      9.4     16.3 write
 1666.0s        0          764.4          802.5      5.0      7.3     11.0     16.8 write
 1667.0s        0          806.0          802.5      5.0      6.8      8.9     15.7 write
 1668.0s        0          803.0          802.5      4.7      6.6     11.0     23.1 write
 1669.0s        0          787.9          802.5      5.0      6.8      8.4     18.9 write
 1670.0s        0          809.2          802.5      5.0      6.8      9.4     12.1 write
 1671.0s        0          799.8          802.5      5.0      7.1      9.4     15.7 write
 1672.0s        0          838.8          802.5      4.7      6.3     11.0     19.9 write
 1673.0s        0          840.4          802.5      4.5      6.3     11.0     16.3 write
 1674.0s        0          806.9          802.5      4.7      7.3      9.4     14.7 write
^CHighest sequence written: 1343922. Can be passed as --write-seq=R1343922 to the next run.

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
 1674.6s        0        1343922          802.5      5.0      4.7      6.8     11.5     65.0  write

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__result
 1674.6s        0        1343922          802.5      5.0      4.7      6.8     11.5     65.0
----

This created kv-histograms-2020-08-27T17:29:16Z.out.

I'm going to try it again for a few minutes to see if the initial spike in latency is one-time or not.

[source,text]
----
$ /cockroachdb/bin/cockroach workload run kv --histograms kv-histograms-$(date +%FT%TZ).out --duration 5m postgres://root@192.168.1.152:26257?sslmode=disable 

...

Highest sequence written: 239288. Can be passed as --write-seq=R239288 to the next run.

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
  300.0s        0         239284          797.6      5.0      4.7      6.8     12.6    125.8  write

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__result
  300.0s        0         239284          797.6      5.0      4.7      6.8     12.6    125.8
----

This created kv-histograms-2020-08-27T17:59:04Z.out.

The latency spike up front happened again.

Let's try out the --max-rate option to place a cap at 500 operations.  (I accidentally used --max-ops first, which exited quickly!)

cockroachdb@ip-192-168-1-192:~$ /cockroachdb/bin/cockroach workload run kv --histograms kv-histograms-$(date +%FT%TZ).out --max-rate 500 postgres://root@192.168.1.152:26257?sslmode=disable 

That seemed to work reasonably well.  There are a ton of metrics in the Admin UI dashboard!

[source,text]
----
_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
  362.6s        0         178192          491.4      4.0      3.7      5.8     12.1     88.1  write

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__result
  362.6s        0         178192          491.4      4.0      3.7      5.8     12.1     88.1  
----

This created kv-histograms-2020-08-27T18:08:37Z.out.

Let's try `--ramp`.  I used 30s first, but that's too fast to really see the effect.  I'm going to try this again with 5m.

[source,text]
----
cockroachdb@ip-192-168-1-192:~$ /cockroachdb/bin/cockroach workload run kv --histograms kv-histograms-$(date +%FT%TZ).out --ramp=5m --max-rate 700 postgres://root@192.168.1.152:26257?sslmode=disable 
...
^CHighest sequence written: 588373. Can be passed as --write-seq=R588373 to the next run.

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
  655.4s        0         435950          665.2      4.7      4.5      6.6     11.5     56.6  write

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__result
  655.4s        0         435950          665.2      4.7      4.5      6.6     11.5     56.6
----

This created kv-histograms-2020-08-27T18:18:53Z.out.  That seemed to do what I expected -- ramped up over several minutes and capped around 700.

The histogram file looks to be per-second histograms.

I want to throw some reads into the mix, but one of the nodes has become "suspect" because its clock is too far off.  I'm starting to get:

[source,text]
----
W200827 18:41:29.280504 1064 kv/kvserver/replica_range_lease.go:555  [n2,s2,r10/
3:/Table/1{4-5}] can't determine lease status of (n2,s2):3 due to node liveness
error: node not in the liveness table
(1) attached stack trace
  | github.com/cockroachdb/cockroach/pkg/kv/kvserver.init
  |     /ws/cockroach/gopath/src/github.com/cockroachdb/cockroach/pkg/kv/kvserve
r/node_liveness.go:44
  | runtime.doInit
  |     /opt/go/1.14.4/src/runtime/proc.go:5420
  | runtime.doInit
  |     /opt/go/1.14.4/src/runtime/proc.go:5415
  | runtime.doInit
  |     /opt/go/1.14.4/src/runtime/proc.go:5415
  | runtime.doInit
  |     /opt/go/1.14.4/src/runtime/proc.go:5415
  | runtime.doInit
  |     /opt/go/1.14.4/src/runtime/proc.go:5415
  | runtime.main
  |     /opt/go/1.14.4/src/runtime/proc.go:190
  | runtime.goexit
  |     /opt/go/1.14.4/src/runtime/asm_amd64.s:1373
----

Two of them have gone into maintenance now.

Several hours later: I've built and deployed chrony to these boxes to see if
this goes better.  Let's go ahead and run that mixed workload I wanted to do
next.

[source,text]
----
$ /cockroachdb/bin/cockroach workload run kv --histograms kv-histograms-$(date +%FT%TZ).out --read-percent=30 --ramp=5m postgres://root@192.168.1.152:26257?sslmode=disable 
...
^CNumber of reads that didn't return any results: 2.
Highest sequence written: 2550079. Can be passed as --write-seq=R2550079 to the next run.

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
 3465.5s        0        1028361          296.7      2.0      1.9      3.0      5.0     67.1  read

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
 3465.5s        0        2395944          691.4      4.9      4.7      6.8     11.0    201.3  write

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__result
 3465.5s        0        3424305          988.1      4.0      4.5      6.6     10.0    201.3
----

I let this run for about an hour.  This created kv-histograms-2020-08-27T22:54:19Z.out.  Note that this file has two lines per second -- one for reads and ones for writes.

The clocks are consistently within 1ms of each other now (!).  This workload is running well.

At about 2020-08-27T23:16Z, I activated statement diagnostics for the UPSERT that this thing is running to see what it does.  This produced a bundle that was 23 bytes (0 bytes downloaded, for some reason).  This looks like this bug fixed in https://www.cockroachlabs.com/docs/releases/v20.2.0-alpha.3.html[v20.2.0-alpha.3]:

> Fixed a bug causing the raw trace file collected inside a statement diagnostics bundle to be sometimes empty when the cluster setting sql.trace.txn.enable_threshold was in use. #50914

although in our case `sql.trace.txn.enable_threshold` is 0 (disabled).  Maybe not the same issue.

== 2020-08-31

Went through:

* https://www.cockroachlabs.com/docs/v20.1/learn-cockroachdb-sql.html[Learn CockroachDB SQL] (this was just basic SQL)
** https://www.cockroachlabs.com/docs/v20.1/developer-guide-overview.html[Developer Guide]
** Skipped exercises under https://www.cockroachlabs.com/docs/v20.1/deploy-a-test-cluster.html[Test deployment] -- these were too basic or exercised K8s behavior.
** Skimmed the https://www.cockroachlabs.com/docs/v20.1/performance.html[Performance Guide]
** https://www.cockroachlabs.com/docs/v20.1/monitoring-and-alerting.html[Prometheus stuff]
** Skimmed https://www.cockroachlabs.com/docs/v20.1/configure-replication-zones.html[Replication Zones]
** https://www.cockroachlabs.com/docs/v20.1/manage-long-running-queries.html[Long-running queries]
** Read through https://www.cockroachlabs.com/docs/v20.1/remove-nodes.html[Decommision nodes]
** Read through https://www.cockroachlabs.com/docs/v20.1/disaster-recovery.html[disaster recovery]
** Skimmed through https://www.cockroachlabs.com/docs/v20.1/troubleshooting-overview.html[Troubleshooting section]

Exercised replication + rebalancing tutorial:

* Started with a cluster with 65 ranges: internal data + some poking around with the "movr" dataset.
* That's 65 ranges with replication factor 3 divided across 3 nodes = 65 replicas per node (confirmed).
* Started a fourth node: expect ~48 replicas per node (65 ranges times replication factor 3 divided by 4 nodes)
* Final state: between 46 - 50 replicas per node.  Stopped slightly before I expected, but well within reasonable.

Now I want to decommission that fourth node.

```
/cockroachdb/bin/cockroach node decommission 4 --insecure --host 192.168.1.46
...
  id | is_live | replicas | is_decommissioning |   membership   | is_draining
-----+---------+----------+--------------------+----------------+--------------
   4 |  true   |        0 |        true        | decommissioned |    false
(1 row)

No more data reported on target nodes. Please verify cluster health before removing the nodes.
```

For good measure, I drained it before disabling it:

```
root@ip-192-168-1-46:~# /cockroachdb/bin/cockroach node drain --insecure --host 192.168.1.46
node is draining... remaining: 1
node is draining... remaining: 0 (complete)
ok
root@ip-192-168-1-46:~# svcadm disable -s cockroachdb
root@ip-192-168-1-46:~#
```

Then I removed it with Terraform.  (Fortunately, just decrementing the count of db nodes caused Terraform to want to destroy this one and not some other one.)

After a few minutes, the UI reports the node as decommissioned.

---

I'm now switching over to fleshing out more of the deployment: Prometheus + Grafana for better situational awareness, plus haproxy so I can do more interesting load testing like shutting off individual nodes.

---

Prometheus:
* building from scratch for illumos
** need: golang, nodejs, yarn
*** added OmniOSce "extra" publisher
*** installed golang 1.14 (plus add path)
*** installed nodejs 12
*** used `npm install -g yarn` (plus add path)
*** needed to install gnu-tar and put that onto PATH before tar
*** needed to set TMPDIR=/var/tmp because /tmp isn't big enough.
*** needed to build `promu` first because the build doesn't have a binary for that but doesn't handle that case.  See https://elatov.github.io/2020/04/monitoring-other-targets-with-prometheus/#compiling-node_exporter-on-omnios[here].  Worked around as described there, by pulling `promu` source.
*** also needed to apply patch below to client_unix.go.

[source,text]
----
diff --git a/vendor/github.com/docker/docker/client/client_unix.go b/vendor/github.com/docker/docker/client/client_unix.go
index 178ff6740..69fb1b48f 100644
--- a/vendor/github.com/docker/docker/client/client_unix.go
+++ b/vendor/github.com/docker/docker/client/client_unix.go
@@ -1,4 +1,4 @@
-// +build linux freebsd openbsd netbsd darwin dragonfly
+// +build linux freebsd openbsd netbsd darwin dragonfly illumos

 package client // import "github.com/docker/docker/client"
----

Grafana: huge pain, but ultimately:
* need at least 8G of memory (!)
* install yarn, node, go, etc.
* git clone
* git checkout # tag you want
* `rm -rf packages/grafana-e2e`
* `yarn install --pure-lockfile` or whatever
* `yarn start` or whatever (might be able to use `go run build.go build-frontend` instead)
* `go run build.go build`
* `go run build.go pkg-archive`
* (appeared to be missing `make build` (for `make build-js`) there?)

== 2020-09-01

* Set up elastic IP for my dev zone.  This looks like about $44/year if my instance were off the whole year, which seems reasonable.
* Set up manual deployment of Prometheus and Grafana in "mon" VM
** use user called "mon" for Prometheus and Grafana
** /export/home/mon/{bin,etc,grafana,var/prometheus/data}

So it will look like:

[source,text]
----
/export/home/mon/bin/prometheus
/export/home/mon/etc/prometheus.yml
/export/home/mon/var/prometheus/data/...
/export/home/mon/grafana/
----

(note: I changed this on 9/2 to separate Prometheus and Grafana into their own directories because they seem more oriented around that approach and it's not clear there's much value in following the traditional system package manager layout here.)

prometheus to be invoked as: prometheus --storage.tsdb.path=... --config.file=... &
refresh: kill -HUP?

NOTE: cockroachdb on one node went into maintenance on boot again because of clock issues.  This time, chrony had definitely finished starting before cockroachdb went into maintenance.  Is this going to be a serious problem?

I did eventually get Prometheus set up pulling from CockroachDB.

I tried running Grafana, but found that my build was busted in a way that only fails when you go to configure a data source in the web UI.

Finally got that fixed and updated instructions above.

== 2020-09-02 and 2020-09-03

Working to automate the deployment of Prometheus and Grafana to a dev zone.  This included a bunch of changes:

* refactored "vminit" directory and created a janky build that creates a "common" tarball for chrony and role-specific tarballs for the database/loadgen and monitoring VMs.
** refactored directory structure of "mon" VM from what's above
** built "fetcher" command to fetch asset from S3
** updated Terraform to configure IAM to support this
** updated vminit.sh to use "fetcher" and reflect the rest of these changes
* incorporated Prometheus
** with config to automatically discover EC2 instances in this project
** with config to scrape Grafana too
** updated Terraform to configure IAM to support this
* incorporated Grafana
** including our Prometheus data source
** including stock Prometheus, Grafana, and CockroachDB dashboards.  This involved manually fixing them to remove DS_PROMETHEUS/DS_NAME inputs -- see the README in that directory.
* various improvements:
** more useful hostnames for VMs (though this is not currently persistent)
** created "env.sh" file with various useful aliases

== 2020-09-04

* added Prometheus node_exporter (see [prometheus/node_exporter#1836](https://github.com/prometheus/node_exporter/issues/1836))
* built out a Grafana dashboard to show key metrics.  Discovered [prometheus/node_exporter#1837](https://github.com/prometheus/node_exporter/issues/1837).

Still, I think I'm just about ready to do some more serious testing.

== 2020-09-08

Summary:

* Switched to Joshua's OmniOS image running his metadata agent: AMI
  `ami-012f34b61b75182e8`.
* Updated Terraform config to deploy much larger root disks.
* Spent some time automating disk and zpool expansion to match provisioned size before realizing that Josh's image already does this.
* Recreated dashboard from Friday
* Ran a bunch of tests:
** ycsb workload: increasing levels of concurrency
** The workload appeared largely bottlenecked on one db node, so I went to experiment with a much larger DB and adding splits.
** I ran into a lot of different errors trying to make this work.  I'm not sure what the root cause really was except stuff being really busy?
** The "kv" workload might be easier to run and just as useful a next step.

Around 9am PT, ran:

[source,text]
----
$ cockroach workload run ycsb --concurrency=1 --drop --histograms histograms-ycsbA-c=1-"$(date +%FT%TZ)".out --tolerate-errors --workload A
----

I let this run for an hour.

Around 1pm PT, I ran:

[source,text]
----
$ cockroach workload run ycsb --concurrency=1 --drop --histograms histograms-ycsbA-c=1-"$(date +%FT%TZ)".out --tolerate-errors --workload A --duration 5m
----

Around 1:16PM, I'm running:

[source,text]
----
$ cockroach workload run ycsb --concurrency=2 --drop --histograms histograms-ycsbA-c=2-"$(date +%FT%TZ)".out --tolerate-errors --workload A --duration 5m
----

Around 1:23PM, I'm running:

[source,text]
----
$ cockroach workload run ycsb --concurrency=4 --drop --histograms histograms-ycsbA-c=4-"$(date +%FT%TZ)".out --tolerate-errors --workload A --duration 5m
----

Around 1:34PM, I'm running:

[source,text]
----
$ cockroach workload run ycsb --concurrency=8 --drop --histograms histograms-ycsbA-c=8-"$(date +%FT%TZ)".out --tolerate-errors --workload A --duration 5m
----

Around 1:42PM, I'm running:

[source,text]
----
$ cockroach workload run ycsb --concurrency=16 --drop --histograms histograms-ycsbA-c=16-"$(date +%FT%TZ)".out --tolerate-errors --workload A --duration 5m
----

At this point, db0 CPUs exceeded 90% CPU utilization.  It's busier than all the other nodes, by a lot.  Let's see what happens if we go further.

Around 1:51PM:

[source,text]
----
$ cockroach workload run ycsb --concurrency=32 --drop --histograms histograms-ycsbA-c=32-"$(date +%FT%TZ)".out --tolerate-errors --workload A --duration 5m
----

It's actually degraded okay at this point, by which I mean that throughput did actually increase and p95/p99 didn't get extremely bad.  I note that in the CRDB Admin UI, nearly all of the queries have hit the "n1" store today.  Only during this last workload did we see any queries hit another store, and it was n3.  Maybe CRDB is dynamically splitting by load?

Note that during this workload is where we start seeing replica errors and more "not leaseholder" errors than before.

Digging further into AdminUI, this database is only 128 MiB, with 4 ranges.  It's not shocking that it's not that distributed.

What if we go further?

At 1:58PM PT:

[source,text]
----
$ cockroach workload run ycsb --concurrency=64 --drop --histograms histograms-ycsbA-c=64-"$(date +%FT%TZ)".out --tolerate-errors --workload A --duration 5m
----

The results look similar to the previous one.  Throughput is less consistent, but hasn't gotten below the point where it was previously consistent.  We did seem to see some SQL 'exec_error's, but I don't see them in the client.  This graph in Grafana also doesn't seem totally consistent with the one in CockroachDB's Admin UI.  It's correlated, though.

Why not go further and see how this goes?

At 2:08 PM PT:

[source,text]
----
$ cockroach workload run ycsb --concurrency=128 --drop --histograms histograms-ycsbA-c=128-"$(date +%FT%TZ)".out --tolerate-errors --workload A --duration 5m
----

This one definitely saw spikes in SQL "exec_error", and potentially worse throughput than the previous one.  All db nodes are pretty tapped at this point.

I want to see what happens with this workload if I tune up the split count and total row count, since it seems pretty pokey right now.  I'm going to go back to concurrency 16, which is one step past 8, which was the stablest and most consistent.

[source,text]
----
$ cockroach workload init ycsb --splits 8 --concurrency=16 --drop --insert-count=1000000 --workload A
$ cockroach workload  run ycsb --splits 8 --concurrency=16 --drop --insert-count=1000000 --workload A --histograms histograms-ycsbA-c=16-"$(date +%FT%TZ)".out --tolerate-errors --duration 1h
----

The loading step is taking quite a while.  It's hammering both CPUs on one database node (so, concurrency=1, I guess)?

While this was going on, I was able to:

[source,text]
----
root@192.168.1.118:26257/ycsb> select count(*) from usertable;
  count
----------
  438000
(1 row)

Time: 51.558088946s
----

But when I tried this later, I got a strange error:

[source,text]
----
root@192.168.1.118:26257/ycsb> select count(*) from usertable;
ERROR: driver: bad connection
warning: connection lost!
opening new connection: all session settings will be lost
root@192.168.1.118:26257/ycsb>
----

I'm not sure which host I was connected to.  I checked all three logs but didn't see anything obvious.

The `init` command failed after 20 minutes with:

[source,text]
----
cockroachdb@loadgen0:~$ time cockroach workload init ycsb --splits 8 --concurrency=16 --drop --insert-count=1000000 --workload A
Error: failed insert into usertable: pq: split failed while applying backpressure to [txn: 4705c25f], ConditionalPut [/Table/81/1/"user4211402063788639270"/0,/Min), ConditionalPut [/Table/81/1/"user4211402063788639270"/1/1,/Min), ConditionalPut [/Table/81/1/"user4211402063788639270"/2/1,/Min), ConditionalPut [/Table/81/1/"user4211402063788639270"/3/1,/Min), ConditionalPut [/Table/81/1/"user4211402063788639270"/4/1,/Min), ConditionalPut [/Table/81/1/"user4211402063788639270"/5/1,/Min), ConditionalPut [/Table/81/1/"user4211402063788639270"/6/1,/Min), ConditionalPut [/Table/81/1/"user4211402063788639270"/7/1,/Min), ConditionalPut [/Table/81/1/"user4211402063788639270"/8/1,/Min), ConditionalPut [/Table/81/1/"user4211402063788639270"/9/1,/Min), ConditionalPut [/Table/81/1/"user4211402063788639270"/10/1,/Min), ConditionalPut [/Table/81/1/"user8166519625965030401"/0,/Min), ConditionalPut [/Table/81/1/"user8166519625965030401"/1/1,/Min), ConditionalPut [/Table/81/1/"user8166519625965030401"/2/1,/Min), ConditionalPut [/Table/81/1/"user8166519625965030401"/3/1,/Min), ConditionalPut [/Table/81/1/"user8166519625965030401"/4/1,/Min), ConditionalPut [/Table/81/1/"user8166519625965030401"/5/1,/Min), ConditionalPut [/Table/81/1/"user8166519625965030401"/6/1,/Min), ConditionalPut [/Table/81/1/"user8166519625965030401"/7/1,/Min), ConditionalPut [/Table/81/1/"user8166519625965030401"/8/1,/Min), ... 10976 skipped ..., ConditionalPut [/Table/81/1/"user6890362626482376666"/7/1,/Min), ConditionalPut [/Table/81/1/"user6890362626482376666"/8/1,/Min), ConditionalPut [/Table/81/1/"user6890362626482376666"/9/1,/Min), ConditionalPut [/Table/81/1/"user6890362626482376666"/10/1,/Min), EndTxn(commit:true tsflex:true) [/Table/81/1/"user4211402063788639270"/0]  on range r101:/{Table/81-Max} [(n1,s1):1, (n2,s2):2, (n3,s3):3, next=4, gen=42]: operation "split queue process replica 101" timed out after 1m0s: split at key /Table/81/1/"user1430647350823960411" failed: context deadline exceeded

real    19m28.125s
user    0m15.002s
sys     0m2.580s
----

Amusing sideshow:

[source,text]
----
root@192.168.1.118:26257/ycsb> select count(*) from usertable;
invalid syntax: statement ignored: unexpected error: read tcp 192.168.1.118:54604->192.168.1.118:26257: read: connection reset by peer
warning: error retrieving the transaction status: driver: bad connection
warning: connection lost!
opening new connection: all session settings will be lost
root@192.168.1.118:26257/ycsb ?>
----

But ultimate it had created 714,000 rows:

[source,text]
----
select count(*) from usertable;
  count
----------
  714000
(1 row)

Time: 12.48402931s
----

Details on that https://www.cockroachlabs.com/docs/stable/common-errors.html#context-deadline-exceeded[context deadline exceeded] error.

The database is at least 4 GiB now, although ycsb is only 1.6 GiB (maybe that's logical?).  There's only one range, though.

Resuming with:

[source,text]
----
$ time cockroach workload init ycsb --splits 8 --concurrency=16 --insert-start 714000 --insert-count=1000000 --workload A
----

I realized that isn't right -- the insert count needs to be adjusted.  Tried to get a new count and got:

[source,text]
----
root@192.168.1.118:26257/ycsb> select count(*) from usertable;
ERROR: driver: bad connection
warning: connection lost!
opening new connection: all session settings will be lost
----

This is repeatable.  Is this an haproxy timeout?  I didn't reproduce it (one time) hitting a CRDB node directly.

So now:

[source,text]
----
$ time cockroach workload init ycsb --splits 8 --concurrency=16 --insert-start 714000 --insert-count=286000 --workload A
Error: failed insert into usertable: pq: duplicate key value (ycsb_key)=('user10357802244052365217') violates unique constraint "primary"

real    1m7.990s
user    0m0.787s
sys     0m0.831s
----

Yeesh.

I'm seeing this repeatedly now, even when I bump the count up.  When I bumped it way up:

[source,text]
----
cockroachdb@loadgen0:~$ time cockroach workload init ycsb --splits 8 --concurrency=16 --insert-start 800000 --insert-count=200000 --workload A
Error: failed insert into usertable: driver: bad connection

real    3m1.377s
user    0m1.018s
sys     0m0.641s
----

I'm going to try without going through haproxy.

[source,text]
----
$ time cockroach workload init ycsb --splits 8 --concurrency=16 --insert-start 900000 --insert-count=100000 --workload A postgresql://root@192.168.1.104:26257/ycsb?sslmode=disable
----

This ultimately failed with another constraint violation error.  There are now 721,000 rows in `usertable`.

For kicks, I'm going to start the above workload anyway to see how it goes.  Tomorrow, I'll probably reset and do the "kv" workload.  This should have a few advantages because it doesn't do so much work during the "init" phase.  That's good because this phase is harder to observe and not parallelized, as far as I can tell.

It may still be worth digging into the ycsb issues to better understand how things fail when they go wrong.  It would be good to better understand what SQL it's running (how many rows is it trying to insert at once?), with what concurrency, how long those INSERTs are taking, etc.

I realized as I started this that I wasn't sure the splits had been applied.  So I'll run this:

[source,text]
----
$ time cockroach workload init ycsb --splits 8 --concurrency=16 --insert-count=0 --workload A
I200908 22:38:20.458190 1 workload/workloadsql/workloadsql.go:113  starting 8 splits

real    0m1.885s
user    0m0.106s
sys     0m0.058s
$ cockroach workload  run ycsb --splits 8 --concurrency=16 --workload A --histograms histograms-ycsbA-c=16-"$(date +%FT%TZ)".out --tolerate-errors --duration 1h
----

Incidentally, this command's documentation is rather confused.  Some of these (like `--splits`) apply at init time, but that's not clear.  Other things are just documented wrong (`--insert-start` vs. `--initial-count`).

A few minutes into this workload (around 3:47pm PT), the Grafana metrics tanked.  Activity went to zero, CPU utilization is no longer reported.  All services in all VMs appear to be running as normal.  The workload is reporting a bunch of successful operations per second!

It looks like the "mon" zone ran out of disk space.  It's still got a 2 GiB disk for some reason, even though the disk is 10 GiB.  The other nodes had this problem earlier, and rebooting fixed it because Joshua's image automatically expands the pool to match the physical size.  Maybe I forgot to reboot this one?  Anyway, I made the mistake of trying to fix this by rebooting it.  I doubt this will work because it probably won't be able to come up with 0 bytes available.  I may have to redeploy this VM, in which case I'll have lost today's testing data.  I do have screenshots and the client-side data, if it's really important.  It's also presumably reproducible.

I redeployed this zone (having saved the dashboard JSON!).  As the workload is running now (see above): CPU utilization is high for all CPUs on all db nodes (77%-90%).  db1 is a little lower -- closer to the 77% level.  Queries aren't perfectly distributed across the nodes, but it's not bad.  Average throughput is about 1K selects + 1K updates per second, which is a little less than c=16 earlier today, but the database is much bigger now.

Throughput dropped to zero for a while and spat this out:

[source,text]
----
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
 2101.0s        0         1157.6         1059.4      3.3      7.3     13.1     21.0 read
 2101.0s        0         1139.6         1058.5     10.0     17.8     28.3     46.1 update
 2102.0s        0         1123.4         1059.4      3.4      7.9     18.9     28.3 read
 2102.0s        0         1092.3         1058.6     10.5     18.9     33.6     52.4 update
 2103.0s        0         1117.0         1059.5      3.3      6.8     15.7     37.7 read
 2103.0s        0         1117.0         1058.6     10.0     19.9     28.3     35.7 update
 2104.0s        0         1229.1         1059.5      3.5      6.8     11.0     23.1 read
 2104.0s        0         1145.1         1058.6     10.0     16.3     24.1     30.4 update
 2105.0s        0         1192.1         1059.6      3.4      8.4     14.2     26.2 read
 2105.0s        0         1083.1         1058.6     10.0     19.9     29.4     39.8 update
 2106.0s        0         1160.1         1059.6      3.4      7.1     14.2     26.2 read
 2106.0s        0         1146.1         1058.7     10.0     17.8     26.2     35.7 update
 2107.0s        0         1131.9         1059.7      3.4      7.6     13.6     27.3 read
 2107.0s        0         1129.9         1058.7     10.0     18.9     28.3     35.7 update
 2108.0s        0         1142.0         1059.7      3.4      8.1     14.2     35.7 read
 2108.0s        0         1120.0         1058.7     10.0     18.9     26.2     39.8 update
 2109.0s        0         1155.2         1059.8      3.4      7.1     10.5     21.0 read
 2109.0s        0         1207.2         1058.8     10.0     16.3     21.0     31.5 update
 2110.0s        0         1154.4         1059.8      3.4      8.4     16.3     23.1 read
 2110.0s        0         1056.4         1058.8     10.0     21.0     32.5     48.2 update
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
 2111.0s        0         1134.6         1059.8      3.3      6.8     17.8     35.7 read
 2111.0s        0         1124.6         1058.8     10.0     17.8     28.3     60.8 update
 2112.0s        0            0.0         1059.3      0.0      0.0      0.0      0.0 read
 2112.0s        0            0.0         1058.3      0.0      0.0      0.0      0.0 update
 2113.0s        0            0.0         1058.8      0.0      0.0      0.0      0.0 read
 2113.0s        0            0.0         1057.8      0.0      0.0      0.0      0.0 update
 2114.0s        0            0.0         1058.3      0.0      0.0      0.0      0.0 read
 2114.0s        0            0.0         1057.3      0.0      0.0      0.0      0.0 update
 2115.0s        0            0.0         1057.8      0.0      0.0      0.0      0.0 read
 2115.0s        0            0.0         1056.8      0.0      0.0      0.0      0.0 update
 2116.0s        0            0.0         1057.3      0.0      0.0      0.0      0.0 read
 2116.0s        0            0.0         1056.3      0.0      0.0      0.0      0.0 update
 2117.0s        0            0.0         1056.8      0.0      0.0      0.0      0.0 read
 2117.0s        0            0.0         1055.8      0.0      0.0      0.0      0.0 update
 2118.0s        0            0.0         1056.3      0.0      0.0      0.0      0.0 read
 2118.0s        0            0.0         1055.3      0.0      0.0      0.0      0.0 update
 2119.0s        0            0.0         1055.8      0.0      0.0      0.0      0.0 read
 2119.0s        0            0.0         1054.8      0.0      0.0      0.0      0.0 update
 2120.0s        0            0.0         1055.3      0.0      0.0      0.0      0.0 read
 2120.0s        0            0.0         1054.3      0.0      0.0      0.0      0.0 update
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
 2121.0s        0            0.0         1054.8      0.0      0.0      0.0      0.0 read
 2121.0s        0            0.0         1053.8      0.0      0.0      0.0      0.0 update
 2122.0s        0            0.0         1054.4      0.0      0.0      0.0      0.0 read
 2122.0s        0            0.0         1053.3      0.0      0.0      0.0      0.0 update
 2123.0s        0            0.0         1053.9      0.0      0.0      0.0      0.0 read
 2123.0s        0            0.0         1052.9      0.0      0.0      0.0      0.0 update
 2124.0s        0            0.0         1053.4      0.0      0.0      0.0      0.0 read
 2124.0s        0            0.0         1052.4      0.0      0.0      0.0      0.0 update
 2125.0s        0            0.0         1052.9      0.0      0.0      0.0      0.0 read
 2125.0s        0            0.0         1051.9      0.0      0.0      0.0      0.0 update
 2126.0s        0            0.0         1052.4      0.0      0.0      0.0      0.0 read
 2126.0s        0            0.0         1051.4      0.0      0.0      0.0      0.0 update
 2127.0s        0            0.0         1051.9      0.0      0.0      0.0      0.0 read
 2127.0s        0            0.0         1050.9      0.0      0.0      0.0      0.0 update
 2128.0s        0            0.0         1051.4      0.0      0.0      0.0      0.0 read
 2128.0s        0            0.0         1050.4      0.0      0.0      0.0      0.0 update
 2129.0s        0            0.0         1050.9      0.0      0.0      0.0      0.0 read
 2129.0s        0            0.0         1049.9      0.0      0.0      0.0      0.0 update
 2130.0s        0            0.0         1050.4      0.0      0.0      0.0      0.0 read
 2130.0s        0            0.0         1049.4      0.0      0.0      0.0      0.0 update
E200908 23:14:11.770407 1 workload/cli/run.go:445  pq: result is ambiguous (error=rpc error: code = Unavailable desc = transport is closing [propagate])
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
 2131.0s        1            0.0         1049.9      0.0      0.0      0.0      0.0 read
 2131.0s        1            0.0         1048.9      0.0      0.0      0.0      0.0 update
 2132.0s        3          745.8         1049.8      3.3      7.1     12.6  20401.1 read
 2132.0s        3          772.8         1048.8     10.0     18.9     27.3  20401.1 update
 2133.0s        3          836.1         1049.7      3.0      6.3      8.9     12.1 read
 2133.0s        3          864.1         1048.7      9.4     16.3     28.3     35.7 update
 2134.0s        3          815.0         1049.5      3.1      6.6     11.5     26.2 read
 2134.0s        3          808.0         1048.6      9.4     18.9     33.6  22548.6 update
 2135.0s        3          879.2         1049.5      3.0      6.6      8.9     13.1 read
 2135.0s        3          854.1         1048.5      9.4     14.7     22.0     37.7 update
 2136.0s        3          856.1         1049.4      3.1      6.3      7.6     12.1 read
 2136.0s        3          849.1         1048.4      9.4     16.3     23.1     27.3 update
 2137.0s        3          834.9         1049.3      3.0      6.6     11.0     13.6 read
 2137.0s        3          805.9         1048.3     10.0     17.8     24.1     30.4 update
 2138.0s        3          930.8         1049.2      3.1      6.3     10.5     18.9 read
 2138.0s        3          864.8         1048.2      8.9     14.7     21.0     29.4 update
 2139.0s        3          829.2         1049.1      2.9      6.0     10.0     62.9 read
 2139.0s        3          875.2         1048.1      9.4     16.3     21.0     32.5 update
 2140.0s        3          833.7         1049.0      3.0      6.8      9.4     21.0 read
 2140.0s        3          840.7         1048.0      9.4     17.8     23.1     31.5 update
----

Another one I saw was:

[source,text]
----
E200908 23:15:23.487617 1 workload/cli/run.go:445  pq: result is ambiguous (error=unable to dial n1: breaker open [exhausted])
----

Maybe I'm running too close to saturation?  Until this point, p95 latency was very steady around 18ms across all three nodes.  p99 was very steady at around 30ms across all three nodes.  Now the thing is falling apart.  I wonder if this would work better with three different load generator instances (processes, not VMs) instead of haproxy?  But these look like internal errors.


A few minutes later, the workload has recovered to where it was before.  It seems like we triggered a crash?  But the uptime on all of them shows 6 hours.  That said, there was a loss of connections to .236 and a bunch of ranges reported being under-replicated for a minute.  CockroachDB did not actually restart on that node.  I do see some errors in the logs:

[source,text]
----
W200908 23:15:19.027322 198 kv/kvserver/node_liveness.go:592  [n3,liveness-hb] failed node liveness heartbeat: oper
ation "node liveness heartbeat" timed out after 4.5s
(1) operation "node liveness heartbeat" timed out after 4.5s
Wraps: (2) context deadline exceeded
Error types: (1) *contextutil.TimeoutError (2) context.deadlineExceededError

An inability to maintain liveness will prevent a node from participating in a
cluster. If this problem persists, it may be a sign of resource starvation or
of network connectivity problems. For help troubleshooting, visit:

    https://www.cockroachlabs.com/docs/stable/cluster-setup-troubleshooting.html#node-liveness-issues

...

I200908 23:15:19.062947 196 server/status/runtime.go:504  [n3] runtime stats: 0 B RSS, 242 goroutines, 108 MiB/1004
 MiB/269 MiB GO alloc/idle/total, 174 MiB/221 MiB CGO alloc/total, 187.1 CGO/sec, 0.0/0.0 %(u/s)time, 0.0 %gc (1x),
 0 B/0 B (r/w)net
W200908 23:15:19.482447 98 kv/kvserver/closedts/provider/provider.go:155  [ct-closer] unable to move closed timesta
mp forward: not live
(1) attached stack trace
  | github.com/cockroachdb/cockroach/pkg/kv/kvserver.init
  |     /ws/cockroach/gopath/src/github.com/cockroachdb/cockroach/pkg/kv/kvserver/node_liveness.go:60
  | runtime.doInit
  |     /opt/go/1.14.4/src/runtime/proc.go:5420
  | runtime.doInit
  |     /opt/go/1.14.4/src/runtime/proc.go:5415
  | runtime.doInit
  |     /opt/go/1.14.4/src/runtime/proc.go:5415
  | runtime.doInit
  |     /opt/go/1.14.4/src/runtime/proc.go:5415
  | runtime.doInit
  |     /opt/go/1.14.4/src/runtime/proc.go:5415
  | runtime.main
  |     /opt/go/1.14.4/src/runtime/proc.go:190
  | runtime.goexit
  |     /opt/go/1.14.4/src/runtime/asm_amd64.s:1373
Wraps: (2) not live
Error types: (1) *withstack.withStack (2) *errors.errorString
----

== 2020-09-09

Switching to "kv" workload (see yesterday's notes).

cockroach workload init kv --concurrency 4 --max-block-bytes=4096 --min-block-bytes=3072
for c in 4 8 16 32 64 128; do
    cockroach workload run kv --concurrency $c --duration 10m --histograms histograms-kv-c=$c-$(date +%FT%TZ)Z.out  --max-block-bytes=4096 --min-block-bytes=3072 --read-percent=50 --tolerate-errors
done

Notes:

* This is a considerably larger record size than I had been testing previously.
* The database is getting bigger each time this way so it's not a totally fair test among different levels of concurrency.

Results:

* The total number of SQL connections and active queries scales up with the concurrency as we'd expect.
* Starting with c=8:
** the CPU utilization graphs look about the same for all runs.
** the distribution of SQL queries to each node looks about the same.
* The SQL query throughput looks about the same among all these runs.
* The SQL query p95 latency increases with each run.
* According to `iostat`, the disk is quite busy much of the time (essentially 100% at c=64).  Occasionally, the wait time at the zpool level is upwards of 100 (ms?), but it never gets nearly that high on the actual disk.
* There was one spike in p99 SQL latency of 9s on one node (192.168.1.236 @ 15:48:30Z).

Conclusions:

* The system is basically saturated at c=4.
* Extreme outliers start around c=32.  Things really start getting nonlinear around c=128.

Possible way to go next: stick with c=16 and expand the cluster while all this is going on.  From initial 3 nodes -> 6 nodes -> 9 nodes -> 12 nodes.

17:10Z: deployed node#4.
17:14Z: the new node is definitely in service.  CPU utilization of other nodes has gone down a bit, as has query throughput.  p95/p99 latency spiked a lot.  Heartbeat latency spiked to over 5s.  Big spike in exec errors over 4Kps.
17:17Z: another spike in p95/p99 to 10s.  I don't know why this is happening -- the client isn't even updated to establish new connections so it shouldn't be using the new node.

[source,text]
----
  760.0s        0            0.0          366.8      0.0      0.0      0.0      0.0 write
E200909 17:13:18.084065 1 workload/cli/run.go:445  ERROR: result is ambiguous (error=unable to dial n2: breaker open [exhausted]) (SQLSTATE 40003)
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
  761.0s        1           21.0          364.9    130.0  13421.8  13421.8  13421.8 read
...
  969.0s        4            0.0          304.6      0.0      0.0      0.0      0.0 write
E200909 17:16:47.529391 1 workload/cli/run.go:445  EOF
  970.0s        5            6.0          303.1     13.1     35.7     35.7     35.7 read
  970.0s        5            5.0          304.3     18.9  60129.5  60129.5  60129.5 write
E200909 17:16:48.580680 1 workload/cli/run.go:445  EOF
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
  971.0s       15          115.0          302.9     17.8  60129.5  60129.5  60129.5 read
  971.0s       15          111.0          304.1     15.2    113.2  60129.5  60129.5 write
----

As of 17:19Z: consistently seeing 20 errors per second with 100-200 ops per second.

Note: Prometheus didn't pick up the new node right away.  Maybe a better methodology is to preprovision everything, then shut down a bunch?

17:22:45: restarted Prometheus

Note: replication started around 17:12:30 and finished around 17:25Z.

At 17:30Z, I'm going to restart the load generator to force it to pick up all four nodes.

Perhaps not surprisingly?  This only made some performance worse because some ranges moved to a node that's not handling any requests.

17:28:50Z: restarted client workloads  However, despite having sent SIGHUP to haproxy, it doesn't seem to have picked up the fourth server.
17:30:40Z: restart both haproxy and load generator.
Now we do see 4 active servers in haproxy and all four CRDB nodes have connections.

Note: I just checked the CRDB Admin UI to look at "queries per store", which has previously been a proxy for whether queries were being served equally by the different nodes, and it does look well distributed both before and after the new node was added.

The performance now is decidedly worse than before I added the new node.  p95/p99 latency is definitely higher for two nodes, and about the same for the other two (even the one which wasn't up, which is weird).  Right now, the disks on db0 are 100% pegged all the time.  Disks on the other two original nodes have a fair bit of headroom.  Why is that one so busy?  Is this because the kv workload is not random?

In the Admin UI, I see that the UPSERT statements have seen 10 retries, but that's cumulative -- doesn't seem important.

I can see that all four nodes are serving a comparable number of queries, and their CPU utilization isn't to far off (and all under 30% per CPU).  But db0 is still being hammered on I/O.  db1 is going through periods where it's busy too, but not nearly as much.  Note that average service time on this disk is worse than disk on db1, even when db1's is busy too, by a factor of 3 or so (6ms for db1, 20ms for db0).  A few minutes later: the 100% busy seems to have moved to db2 and db0 was idle for a little while.  A few seconds later we're back to db0.

Maybe it would be more interesting to do just 20% writes / 80% reads.

CRDB does recommend:

> Disks must be able to achieve 500 IOPS and 30 MB/s per vCPU....Monitor IOPS for higher service times. If they exceed 1-5 ms, you will need to add more devices or expand the cluster to reduce the disk latency.

Anyway, the preformance has been the same for a while.  I'm going to shut this down and drop both databases for now in prep for future runs.  I'm also going to decommission that fourth node.

This failed:

[source,text]
----
# cockroach node decommission  4

  id | is_live | replicas | is_decommissioning | membership | is_draining
-----+---------+----------+--------------------+------------+--------------
   4 |  true   |       53 |       false        |   active   |    false
(1 row)

  id | is_live | replicas | is_decommissioning |   membership    | is_draining
-----+---------+----------+--------------------+-----------------+--------------
   4 |  true   |       53 |        true        | decommissioning |    false
(1 row)
..........
  id | is_live | replicas | is_decommissioning |   membership    | is_draining
-----+---------+----------+--------------------+-----------------+--------------
   4 |  true   |       52 |        true        | decommissioning |    false
(1 row)
...........
ERROR: connection lost.

while trying to mark as decommissioning: rpc error: code = Unavailable desc = transport is closing
Failed running "node decommission"
----

More haproxy woes?

== 2020-09-10

Not much testing today, but I'm reflecting on the issues I've hit so far.  I've hit a bunch of different client issues that seem potentially related to overload, and seen symptoms of overloaded servers (e.g., missed heartbeats).  I've potentially been pushing the system beyond its intended capacity, particularly in terms of I/O.  It _should_ handle that okay, but maybe isn't a great first test.

Maybe try a few simplifying changes:

* Provision 6 database nodes up front, but don't start CockroachDB on three of them.  (Or, disable cockroachdb on the last three before running `cockroach init`.)  This way I eliminate any disruptive change to the initially-running three (like restarting them, which my Terraform config normally does, although I had commented that out yesterday).  And I know exactly when each one starts.
* Factor out haproxy: instead of one load generator process using haproxy to talk to CRDB nodes, maybe use separate client processes pointed at specific CRDB nodes.  They won't automatically start using new nodes this way so I will have to start more up again.
* Let's not start by pushing the cluster to its limit.  Instead, let's separate out a few different questions:
** try to replicate something close to the https://www.cockroachlabs.com/docs/stable/performance.html#throughput[basic sysbench numbers that they got on AWS]?  These are much bigger machines, but even if we can just achieve that latency at a lower level of concurrency, that'd be useful.
** demonstrate horizontal scalability (_not_ necessarily online): maybe the way to think about this is: ramp up load generators until p95 latency reaches some target.  See how that point differs at different cluster sizes.
** demonstrate expanding the cluster under modest load (largely ignoring performance -- it would be enough that it doesn't get worse or experience errors)
** demonstrate shrinking the cluster under modest load (similar to expansion)
** demonstrate the impact of failures on a modest load (again, largely ignoring performance)

Last item of the day: trying to get %busy and average I/O time metrics in Grafana.  This is a little tricky from the kstats.

== 2020-09-14

Conclusions from today:

* Built sysbench and started using its oltp_insert workload for testing.
* Ran into major interference from AWS "gp2" (storage volume) performance, which falls off a cliff potentially hours after starting a workload.  Confirmed this with CloudWatch "burst" metric.  Will work around this with "io1" volumes instead of "gp2".
* Aside from that, performance was reasonably stable.  I successfully expanded the cluster a few times.  Performance got better, but not linearly so, and load was not perfectly distributed with n=4 or n=5.  (Did not get to n=6 because of the I/O problem.)
* Ran into minor issue with the image I'm using: can't install packages with `pkg`, apparently due to missing SSL certs.

Details:

* I've brought up a cluster with 6 database nodes, but only three had cockroachdb running when I initialized the cluster.
* Made a build of sysbench:
** in my build machine, had to install postgresql-12, autotools, libtool
** note: could not do this in Joshua's image because pkg tools can't do anything because they're looking for /etc/openssl/certs.  In my build zone, that appears to be /etc/{ssl,crypto}/certs.
** set --prefix=/opt/sysbench, tarred up directory, and copied to "loadgen0".  Also needed to add libpq.so, which I did by hand afterwards.
** `LDFLAGS='-R /opt/sysbench/lib'  ./configure --without-mysql --with-pgsql --prefix=/opt/sysbench`

Around 9:23AM PT:

[source,text]
----
# sysbench --threads=1 --time=0 --pgsql-host=192.168.1.227 --pgsql-port=26257 --pgsql-user=root --pgsql-db=sbtest oltp_insert run
----

from loadgen0.  That settled around 200 inserts per second (all handled by .227, which is db0), p95 latency=6ms, p99 latency=13ms, about 50% CPU utilization in all four CPUs of db0 and db1, just over 40% disk busy time in all three db nodes.  That's all writes, about 4 MBps, with spikes up to almost 8.

By 9:37, this has been quite stable.  Let's start another load generator aimed at db1:

[source,text]
----
# sysbench --threads=1 --time=0 --pgsql-host=192.168.1.66 --pgsql-port=26257 --pgsql-user=root --pgsql-db=sbtest oltp_insert run
----

By 9:47, this has been quite stable in throughput, with some variation in latency.  We're at 300 inserts / second, evenly split between db0 and db1.  CPU utilization for those nodes is 50%-60% (per CPU).  p95 latency (both nodes) is around 8ms, p99 latency ranging from 16-17.5ms.  Disks almost 50% busy on all nodes.  Average disk I/O time is unchanged, largely maxing around 800us -- pretty good!  CockroachDB heartbeat p99 latency is pretty consistently under 10ms.

At 9:50AM, I started a third load generator (same loadgen VM):

[source,text]
----
# sysbench --threads=1 --time=0 --pgsql-host=192.168.1.214 --pgsql-port=26257 --pgsql-user=root --pgsql-db=sbtest oltp_insert run
----

This has settled at a throughput of about 400 inserts per second, evenly split among all three db hosts.  CPU utilizationr anges from 36-60% (per CPU) on the db nodes, with db2 closer to 40% (lower than the other two).  p95 ranges from 8-10ms, p99 15-22ms.  Disks are about 50% busy.  The disk write IOPS and busy time haven't changed a lot with the last addition.  I'm seeing CockroachDB p99 heartbeat outliers up to 110ms, but that's still quite quick.

I'm going to let this run a little while longer to see what happens.

At 10:47AM: this has been fairly stable at the above numbers.  There are a couple of ways to go from here:

* could add more write load (an additional thread for each load generator)
* could add a read workload
* could try turning off one node, but we're not going to be able to take writes that way anyway since the replication factor is 3
* try expanding the cluster without changing the load
** more conservative: do it without the workload running and see if it affects anything
** more aggressive: do this with the workload running

I'm leaning towards online expansion of the cluster.  At 10:51am: enabled cockroachdb on db3.

10:57am: there was a burst of errors (peaking at 1 error per second) when I enabled CRDB, though the load generators didn't complain at all.  Cockroach heartbeat p99 latency peaked at 40ms.  I can see the new node took on some of the write workload, particularly from db1 (looking at a shift in the write IOPS graph and percent-busy graph).  Average I/O time is still well under 1ms, and disks remain about 50% busy on the busy nodes (less so on db1 and the new db3).  Overall average throughput is slightly increased (just under 400 inserts per second to about 420).  p95 and p99 decreased a few ms each but are largely the same.  Next I will add the next database node.

11:01AM PT: added next node.

11:13AM PT: average throughput essentially unchanged, though there was a momentary crash at 18:01:30Z to just under 300 inserts per second.  That correlated with a spike in p95 on all nodes to about 15-19ms and p99 to about 35-41ms.  This seems to have shifted load from db2 (in terms of CPU utilization and disk usage and write IOPS).  Heartbeat latency p99 peaked at 204ms on the newly-added node.  Again, we had a small spike in error rate.  It's not clear if the client saw theses.

11:17AM PT: I'm going to turn off the load generators and turn them on again in order to get summary reports from them and to make sure the load isn't somehow state-dependent.  Before I do that, I see why had another small crash in throughput at 18:18Z.  This affected all nodes, like the previous one.  Note that there's plenty of CPU headroom on each CPU (though utilization spiked to 80% on a few CPUs at the time of the crash).  Similarly, disk %busy never exceeded 60% on the peak node, and it actually dipped at this time.  Average disk I/O latency was unchanged around this time.  Network throughput had a small dip.  There's no spike in heartbeat latency.  The only thing I see is a small spike in CRDB "exec_error", but it's less even than when adding new nodes before.  Well, as I said, I'm going to restart the load generators to get their numbers.

11:22AM PT: I killed the load generators, but they did not report any numbers, unfortunately.  I restarted these all around 11:23:31 PT.

11:48AM PT: noticed a major reduction in throughput that started around 11:41.  This seems to have been caused by a very sudden spike in average disk I/O latency on db0, from about about 300us to about 3ms.  Other VMs were not affected by this, but naturally the %busy on db0 shot up, to about 88%.  Write IOPS went down everywhere by a factor of ~4-5.  Net throughput dropped significantly too.  No spike in CRDB heartbeat latency nor errors.  CPU utilization down across the board.  p95 CRDB latency shot from about 10ms to about 40ms across the board, and p99 from about 20ms to about 50ms.  This is all consistent with a sudden, terrible degradation in performance from EBS, and I can't think of an obvious cause in the application.

Digging into this, there's some https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html[documentation on this].

> Each volume receives an initial I/O credit balance of 5.4 million I/O credits, which is enough to sustain the maximum burst performance of 3,000 IOPS for 30 minutes. This initial credit balance is designed to provide a fast initial boot cycle for boot volumes and to provide a good bootstrapping experience for other applications. Volumes earn I/O credits at the baseline performance rate of 3 IOPS per GiB of volume size. For example, a 100 GiB gp2 volume has a baseline performance of 300 IOPS. 
> The maximum I/O credit balance for a volume is equal to the initial credit balance (5.4 million I/O credits). 

Baseline performance for my 60 GiB volume would be 180 IOPS.

Based on their equation:

[source,text]
----
burst duration = (credit balance) / (burst IOPS - 3 * volsize)
    = 5.4M / (1600 IOPS - 3 * 60GiB)
    = 5.4M / (1600 - 180)
    = 5.4M / 1420
    = 3800
----

Presumably that's 3800 seconds, or 63 minutes.  I confirmed with CloudWatch that this instance ran out of its credit around 18:40Z.

How to work around this?  It seems very hard to manage this in a benchmarking environment.  Even if I could spend all the credits up front, it'd be hard to make sure it was constantly zero -- and across all six database hosts.

Here are a few different pricing options:

* https://calculator.aws/#/estimate?id=16e6ed9a0102c9e24880a0175edaa9eef88ac8c9[Original estimate] (6 c4.large instances with 60 GiB gp2 volumes (180 IOPS)): $474 / month
* https://calculator.aws/#/estimate?id=184d382407f5e4a64b296ec69c374f3155419801[Estimate using 167GiB gp2 volumes] to get 500 IOPS: $538.20 / month
* https://calculator.aws/#/estimate?id=efaf0e10e9cf496d4dfcc95a26dbbf4cecef56b7[Estimate using 60 GiB io1 volumes] with 500 IOPS: $678 / month

It's cheaper to just get bigger "gp2" volumes than to buy provisioned IOPS.  The problem is that I actually kind of want the consistency: I don't want the performance to plummet like it did today, even if the low value is actually fine.  It sure sucks to pay more for the same IOPS and less storage, though.

Note that the load generator does not need this.  It can stay with "gp2".

At 3:10PM PT, I'm re-evaluating performance from the last three hours:

* p95 has been pretty consistently around 42ms
* p99 has been 50-100ms with lots of spikes (peak spike was 125ms)
* CPU utilization has been consistently low
* Query throughput has consistently averaged about 125 inserts per second, though it's been rocky ranging from 100-125 qps.
* Average I/O time for all disks has been under 1ms except for db0, which has averaged 3.4ms (see above).  All disks except that one have been under 20% busy, while that one averaged 90%.
* Write IOPS differs across hosts and ranges from about 200 to 550.  Read IOPS are negligible.
* Network throughput is negligible -- averaged under 400 KBps for either inbound or outbound for all hosts.
* NTP: according to Cockroach, the mean RPC clock offset has maxed at around 200us.
* Average p99 CockroachDB heartbeat latency for the worst node is 44ms.  Peak was 354ms.
* Peak error rate was 0.35 errors per second at one point.

Overall I'd say it's been fairly stable, for a system that's maxxed out at I/O capacity.

I used Terraform to update the storage class from "gp2" to "io1" (that can be done online) and redeployed the load generator completely to get the sysbench binaries.

Plan for tomorrow:

* Run a similar sequence of steps now that I'm on "io1" storage.

== 2020-09-16

Summary of the day:

* Restarted testing on sysbench database, now that I'm using provisioned IOPS.
* Forgot to scale-down the cluster to 3 nodes, so I decided to try this dynamically.  Found some surprising replication behavior and posted to the forum about this.
* Also found that the workload got very suddenly faster (2x) after an hour or so.  Resource utilization went down, but I/O latency didn't get any faster.  It's as though it just got twice as efficient.  Relatedly, there was a range merge when this happened, so maybe some writes avoided some round-trips after that?  This is great (load-based range merges), but also makes testing performance harder.

Plan for today:

* drop previous sysbench database
* disable db3, db4 to get back to a 3-node cluster
* ramp up workload:
** start one load generator for each db node, about 10 minutes apart
* expand cluster, one node at a time, about 10 minutes apart

17:12Z: dropped database.  This was fairly cheap and didn't seem to do much work (I/O or otherwise).  Somewhat surprisingly, this didn't affect percentage of capacity used or the total number of ranges.  This https://forum.cockroachlabs.com/t/reclaiming-storage-capacity/1024[appears to be a result of the delayed GC process].  This might be an opportunity to reconfigure the TTL period as a test.

At around 17:19:30Z:

[source,text]
----
root@192.168.1.227:26257/defaultdb> SHOW ZONE CONFIGURATION FOR RANGE default;
     target     |              raw_config_sql
----------------+-------------------------------------------
  RANGE default | ALTER RANGE default CONFIGURE ZONE USING
                |     range_min_bytes = 134217728,
                |     range_max_bytes = 536870912,
                |     gc.ttlseconds = 90000,
                |     num_replicas = 3,
                |     constraints = '[]',
                |     lease_preferences = '[]'
(1 row)

Time: 1.926221ms

root@192.168.1.227:26257/defaultdb> ALTER RANGE default CONFIGURE ZONE USING gc.ttlseconds=300 ;
CONFIGURE ZONE 1

Time: 27.301437ms

root@192.168.1.227:26257/defaultdb>
----

After this, I see:

- small spikes in CPU utilization (way more than before, but peaking at about 18% per CPU)
- some disk I/O and disk writes.
- a reduction in ranges per node from 38 to 34
- a reduction in capacity used per node from about 2.4% back to about 0.6%

That seems to have worked as expected.  I'm going to configure it back to the default:


[source,text]
----
root@192.168.1.227:26257/defaultdb> ALTER RANGE default CONFIGURE ZONE USING gc.ttlseconds=90000;
CONFIGURE ZONE 1

Time: 26.959762ms

root@192.168.1.227:26257/defaultdb> SHOW ZONE CONFIGURATION FOR RANGE default;
     target     |              raw_config_sql
----------------+-------------------------------------------
  RANGE default | ALTER RANGE default CONFIGURE ZONE USING
                |     range_min_bytes = 134217728,
                |     range_max_bytes = 536870912,
                |     gc.ttlseconds = 90000,
                |     num_replicas = 3,
                |     constraints = '[]',
                |     lease_preferences = '[]'
(1 row)

Time: 1.984135ms
----

So that's good.  On to the workloads.

I forgot (and hadn't previously noted) that I needed to run:

[source,text]
----
root@192.168.1.227:26257/defaultdb> CREATE DATABASE sbtest;
CREATE DATABASE

Time: 28.410777ms

root@192.168.1.227:26257/defaultdb> ^D
root@loadgen0:~# sysbench --threads=1 --time=0 --pgsql-host=192.168.1.227 --pgsql-port=26257 --pgsql-user=root --pgsql-db=sbtest oltp_insert prepare
WARNING: Both event and time limits are disabled, running an endless test
sysbench 1.1.0-bbee5d5 (using bundled LuaJIT 2.1.0-beta3)

Creating table 'sbtest1'...
Inserting 10000 records into 'sbtest1'
Creating a secondary index on 'sbtest1'...
----

Now the work:

[source,text]
----
sysbench --threads=1 --time=0 --pgsql-host=192.168.1.227 --pgsql-port=26257 --pgsql-user=root --pgsql-db=sbtest oltp_insert run
----

That started at 17:27:15.

17:34Z.  Oops.  I forgot to shut down the two database nodes.  I will try that now, while the workload is running.  Did that around 17:35:15.  Both were graceful shutdowns (well, `svcadm disable`).  As expected, we see a bunch of under-replicated ranges.  In about 5 minutes we should see that number go back to zero.  Note that after doing this:

* CPU utilization changed as expected: the two nodes I shut down went very low.  One of the remaining three nodes went up.  For whatever reason, db1 remains pretty idle.
* Transaction throughput is holding steady around 125 inserts / second.  It's unchanged after this change.
* p95 is about 11-12ms.  p99 is around 18ms.  These are unchanged after this change.
* I'm not sure why, but disk utilization on db0 went down, and db2 went up.
* db0 and db2 are doing almost exactly 1000 write IOPS, unchanged by the change.  db4 had been doing a lot, but that crashed (as expected) when I brought it offline).

Around 17:41Z, we see the cluster rebalance itself:

* the count of under-replicated ranges starts falling
* db1 quickly starts hitting 1000 write IOPS and its percent busy increases
* db0's disk utilization (%busy) goes down a bit from about 50% to about 40% (eyeballing it)
* db2's disk utilization (%busy) goes up by a comparable amount.
* CPU utilization increases on db1 from about idle to about 40% -- less than the other two, but doing a bunch of work now.
* In terms of impact: throughput was a little less consistent, but not much, and holding steady around 125 inserts / second.
* p95 and p99 are not visibly different.
* p99 heartbeat latency has peaked at about 95ms through this whole activity.

For some reason I don't understand, the system seems to have settled at 33 under-replicated ranges.  It's not clear why this would be.  I dug into the admin UI a bit and found that the `sbtest` database has one table, `sbtest`, which uses 95 MiB and 2 total ranges.  That may explain why only two nodes were busy earlier.  There are 22 ranges in the "system" database.  I'm not sure where the other 12 ranges come from, sicne the system reports 36 total ranges.  And I'm not sure why 33 are under-replicated.

In the "Advanced Debug" "Problem Ranges Report", I can see that the under-replicated ranges are 1-34, except for range 4.  I picked range r3 arbitrarily.  The leader is n3, which is also the leaseholder.  That node is still online.  The replicas appear to be on n1 and n2, also both up.  There's a neat log for the range.  Key events:

15:43 (long before I started): looks like the range is on n1, n2, and n3.  It already says "reason: range under-replicated".
17:51:26Z: begin adding n4 because of rebalance
17:51:26Z: begin removing n3 because rebalance (that seems weird)
17:51:26Z: seem to be related to adding n4 again (VOTER_INCOMING vs. LEARNER)
17:51:26Z: removed n3 ("abandoned learner replica")
18:01:24Z: begin adding n5 because range under-replicated
18:01:24Z: finish adding n5? (how is this possible?)
18:01:24Z: begin adding n3 as a replica because range under-replicated
18:01:24Z: finish adding replica n3

Things I don't understand about this:

* n4 and n5 should be suspect from 17:36 to 17:41 and dead after that.  How did we pick them as new replicas at 17:51?
* why did we abandon n3 at 17:51Z?
* how did we finish adding n5 as a replica at 18:01 if it's offline?
* why aren't we trying to fix the fact that it's under-replicated?
* (what are all the different states for replicas?)

Of note, according to https://www.cockroachlabs.com/docs/v20.1/cluster-setup-troubleshooting#admin-ui-shows-under-replicated-unavailable-ranges[this section in the docs]:

>  The number of failures that can be tolerated is equal to (Replication factor - 1)/2. Thus CockroachDB requires (n-1)/2 nodes to achieve quorum. For example, with 3x replication, one failure can be tolerated; with 5x replication, two failures, and so on.

In this case, we had five nodes, but the replication factor was only 3, which means we can only tolerate one failure.  Given that, I suppose it wasn't reasonable to expect that we could retain cluster liveness after this operation; however, it's a little surprising the data remains under-replicated given that at least one copy is available, and this range was never leased to a node that was down.

The link above has some useful debugging instructions, but they require you to look at the "Simulated Allocator Output".  I get an error accessing that:

> An error was encountered while loading this data: This information is not available due to the current value of the 'server.remote_debugging.mode' setting. Insufficient privileges to view this resource.

There's a "Learn more" link, but the content there implies that on an insecure cluster, there should be no privilege issue.  That setting is documented thus:

> set to enable remote debugging, localhost-only or disable (any, local, off)

with a default of "local".  I guess the problem here is that I'm not coming in over localhost.

Here, I updated it:

[source,text]
----
root@loadgen0:~# cockroach sql --host 192.168.1.227
#
# Welcome to the CockroachDB SQL shell.
# All statements must be terminated by a semicolon.
# To exit, type: \q.
#
# Server version: CockroachDB CCL v20.2.0-alpha.1-1729-ge9c7cc561c-dirty (x86_64-sun-solaris2.11, built 2020/08/04 04:08:24, go1.14.4) (same version as client)
# Cluster ID: cfb6ffc3-3553-4629-a174-beb9328b4f57
#
# Enter \? for a brief introduction.
#
root@192.168.1.227:26257/defaultdb> SHOW CLUSTER SETTING server.remote_debugging.mode;
  server.remote_debugging.mode
--------------------------------
  local
(1 row)

Time: 905.67µs

root@192.168.1.227:26257/defaultdb> SET CLUSTER SETTING server.remote_debugging.mode = "any";
SET CLUSTER SETTING

Time: 37.307198ms

root@192.168.1.227:26257/defaultdb> SHOW CLUSTER SETTING server.remote_debugging.mode;
  server.remote_debugging.mode
--------------------------------
  any
(1 row)

Time: 804.065µs

root@192.168.1.227:26257/defaultdb>
----

Now that web page works.  The messages are:

> kv/kvserver/allocator.go:402 [n3,status] replace dead - replacement for 2 dead replicas priority=12000.00
> kv/kvserver/replicate_queue.go:343 [n3,status] next replica action: replace dead
> kv/kvserver/allocator.go:508 [n3,status] allocate candidates: []
> kv/kvserver/store.go:2630 [n3,status] error simulating allocator on replica [n3,s3,r3/6:/System/{NodeLive…-tsd}]: 0 of 3 live stores are able to take a new replica for the range (3 already have a replica); likely not enough nodes in cluster

This is consistent with the documentation, but a little surprising: if the live nodes have replicas (which they do), why is it under-replicated?  If they don't, why can't we create a replica there?

Stopped to debug this a bit further.
Found https://godoc.org/github.com/cockroachdb/cockroach/pkg/roachpb#ReplicaType[documentation about the replica states].

Note also that performance skyrocketed at 18:27Z, from 125 inserts per second to just over 300.  They're still all being served from the same node.  CPU utilization and disk busy time went down.  Naturally, p95 and p99 went down.  However, average I/O time didn't go down, and bytes written per second _did_ -- it's as though the thing got suddenly more efficient.  Did we have a split?  Is that why it got better?  If so, how?

I did see that in Admin UI, under "KV Transactions", we had been doing 100% "committed" transactions and 0 "fast-path committed".  At about this time, we went to 330 of both "committed" and "fast-path committed".  At the same time, we went from a fair number of "partial batches" to 0.

One big change is that from about 17:30 to 18:30, queries were split evenly between stores n1 and n2.  At 18:30, they were all on n2.  This was about the time the cluster dropped one range (from 36 to 35).  There was a merge around this time, and "sbtest1" is now in one range.  This kind of makes sense -- no more round-trip latency?  But it's going to make testing hard again.

Back to the replication question, I'm looking through forum posts:

- Here's one about https://forum.cockroachlabs.com/t/resurrect-broken-cluster/3477/2[truly bad DR].
- Here's an https://forum.cockroachlabs.com/t/under-replicated-range-in-the-cluster/3558[interesting tool: manually re-replicate a range].

I submitted a post to the forum about this that's currently awaiting moderation.  My notes and screenshots are in https://gist.github.com/davepacheco/5f6dcf64104bfdf49802504c2f30feb1#file-notes-md[this gist].

Out of convenience, I let this workload run for several more hours.  As of 23:40Z (5 hours since the jump in performance at 18:30Z), over the last five hours:

* Throughput has been stable at just over 300 inserts / second.
* p95 has been stable at just over 3ms.
* p99 has been stable at 4-7ms.
* Disk %busy has been stable at 15-20% (max).
* With one exceptional spike to almost 6ms on db5 (which is down!), average I/O latency has been stable under 1ms (mostly 300-400us on the active db nodes)
* Disk write IOPS hover around 700 on the active nodes.  Less than 1 read IOPS.
* There was only one p99 heartbeat latency above about 200ms, and that was about 1.7ms on one node around 18:50.  Average p99 is under 20ms.

This is not an impressive load, but that seems pretty good behavior.

== 2020-09-17

Summary for today:

* brought the down nodes back up and answered a bunch of my questions from yesterday.

Details:

* dropped `sbtest` database and recreated it
* temporarily set gc.ttlseconds=30 to purge that data
* at this point, the problematic ranges were not affected (still 33 under-replicated ranges)
* enabled cockroachdb on db3, db4 at 21:17Z.  Under-replicated ranges quickly dropped to zero.

Out of curiosity, what happened on our range 3 that we inspected yesterday?

* The extra two columns show up in the top table for the replicas on n1 - n5.
* There's nothing new in the range log since 9/14.  Wait, that seems like the wrong date!

Was everything I was looking at yesterday garbage?  It seems like it was.  This answers most of our questions from yesterday, which were (taken from my post):

> Why is this range considered “under-replicated” at all? As far as I can tell from the report, it has three replicas, one on each of the remaining available nodes. Relatedly, it seems contradictory that there could be no “live stores able to take a new replica” because all of them already have a replica (and given that there are as many live stores as the replication factor).

This range has five replicas to begin with.  I checked the zone configuration, and the system ranges are all configured for 5 replicas:

```
root@192.168.1.227:26257/defaultdb> SHOW ZONE CONFIGURATIONS;
                       target                      |                               raw_config_sql
---------------------------------------------------+------------------------------------------------------------------------------
  RANGE default                                    | ALTER RANGE default CONFIGURE ZONE USING
                                                   |     range_min_bytes = 134217728,
                                                   |     range_max_bytes = 536870912,
                                                   |     gc.ttlseconds = 90000,
                                                   |     num_replicas = 3,
                                                   |     constraints = '[]',
                                                   |     lease_preferences = '[]'
  DATABASE system                                  | ALTER DATABASE system CONFIGURE ZONE USING
                                                   |     range_min_bytes = 134217728,
                                                   |     range_max_bytes = 536870912,
                                                   |     gc.ttlseconds = 90000,
                                                   |     num_replicas = 5,
                                                   |     constraints = '[]',
                                                   |     lease_preferences = '[]'
  RANGE meta                                       | ALTER RANGE meta CONFIGURE ZONE USING
                                                   |     range_min_bytes = 134217728,
                                                   |     range_max_bytes = 536870912,
                                                   |     gc.ttlseconds = 3600,
                                                   |     num_replicas = 5,
                                                   |     constraints = '[]',
                                                   |     lease_preferences = '[]'
  RANGE system                                     | ALTER RANGE system CONFIGURE ZONE USING
                                                   |     range_min_bytes = 134217728,
                                                   |     range_max_bytes = 536870912,
                                                   |     gc.ttlseconds = 90000,
                                                   |     num_replicas = 5,
                                                   |     constraints = '[]',
                                                   |     lease_preferences = '[]'
  RANGE liveness                                   | ALTER RANGE liveness CONFIGURE ZONE USING
                                                   |     range_min_bytes = 134217728,
                                                   |     range_max_bytes = 536870912,
                                                   |     gc.ttlseconds = 600,
                                                   |     num_replicas = 5,
                                                   |     constraints = '[]',
                                                   |     lease_preferences = '[]'
  TABLE system.public.replication_constraint_stats | ALTER TABLE system.public.replication_constraint_stats CONFIGURE ZONE USING
                                                   |     gc.ttlseconds = 600,
                                                   |     constraints = '[]',
                                                   |     lease_preferences = '[]'
  TABLE system.public.replication_stats            | ALTER TABLE system.public.replication_stats CONFIGURE ZONE USING
                                                   |     gc.ttlseconds = 600,
                                                   |     constraints = '[]',
                                                   |     lease_preferences = '[]'
(7 rows)

Time: 16.715016ms

```

Given that, it's expected that there would be five replicas, so it makes sense that with two nodes down, these ranges are under-replicated.

> n4 and n5 were “suspect” by 17:36Z and “dead” by 17:41Z. Why did CockroachDB decide at 17:51Z to rebalance ranges from n3 onto these dead nodes? Does it not take into account that a node is dead before rebalancing?

These timestamps were from the day before, when the nodes were probably up.

> How is it possible that the replication apparently succeeded for n5 when that node was offline?

Again, these timestamps were from the day before, when the nodes were probably up.

> Why is that that the latest range descriptor in the log has all five nodes in it, but we only see three columns in the range report? Are there really five replicas and we don’t see those columns because the other two nodes are down?

I don't have more information about this, but I suspect my guess is right there, that it just doesn't show columns from nodes that are down.  When I brought the nodes back up, the columns showed up.  When I temporarily bring down n4 again, the column disappears again.  When I bring it back, the column comes back.

> In other words, maybe this is under-replicated not because there aren’t 3 (the replication factor), but because there are five, but two of them are on dead nodes? If that’s true, is there operationally a way to distinguish between replicas that are under-replicated because they’re under the replication factor vs. under-replicated because there are some dead replicas?

The premise for this question is no longer valid -- the replication factor _is_ 5.

> Relatedly, is there a way to know operationally how many under-replicated ranges are not making forward progress (e.g., because they require another node to be up)?

I think this is a valid question.

New questions:

* Is there a way to determine what zone a range is part of?  The best way I know now is to figure out what database/table it's part of (for which I think there may be a reverse index, in the Admin UI, if you browse the databases/tables?), and then use `show zone configurations`.  You can also do the first part with `SHOW RANGES FROM ...`

I wrote a new post on the CockroachDB forum.

I ran out of time today -- got distracted with other things.

== 2020-09-18

Plan: basically same as 9/16, but try to do it right this time.

* check on the https://forum.cockroachlabs.com/t/understanding-under-replicated-ranges/3982[CockroachDB forum thread I created].
* drop and recreate previous sysbench database
* disable db3, db4 to get back to a 3-node cluster
* ramp up workload:
** start one load generator for each db node, about 10 minutes apart
* expand cluster, one node at a time, about 10 minutes apart

Details:

15:26Z: cluster started
15:34Z: shut down nodes db3 and db4.  Both timed out and where forcibly killed by SMF.  As we'd expect, we have 33 under-replicated ranges -- presumably all system ranges.
15:45Z: started sysbench workload from one client:

[source,text]
----
sysbench --threads=1 --time=0 --pgsql-host=192.168.1.227 --pgsql-port=26257 --pgsql-user=root --pgsql-db=sbtest oltp_insert prepare
...
sysbench --threads=1 --time=0 --pgsql-host=192.168.1.227 --pgsql-port=26257 --pgsql-user=root --pgsql-db=sbtest oltp_insert run
----

15:55Z: started sysbench workload from second client (now two workloads running):

[source,text]
----
sysbench --threads=1 --time=0 --pgsql-host=192.168.1.66 --pgsql-port=26257 --pgsql-user=root --pgsql-db=sbtest oltp_insert run
----

16:05Z: started sysbench workload from third client (now three workloads running):

[source,text]
----
sysbench --threads=1 --time=0 --pgsql-host=192.168.1.214 --pgsql-port=26257 --pgsql-user=root --pgsql-db=sbtest oltp_insert run
----

I let this run for a few hours and ran into a few issues.  In the second and third load generators, sysbench reported:

[source,text]
----
client_loop: send disconnect: Broken pipe
----

This appeared to happen at 18:05Z and 18:15Z, based on the throughput graphs.

From about 16:10 to about 18:05Z, throughput was well-distributed across the nodes at about 280 total inserts per second.  There was plenty of CPU headroom and disks topped out around 75% busy.  p95 was steady around 16ms, p99 ranged from 18-40ms.  It's not at all clear what caused the client issues.  There was a tiny blip in "exec_error" around 18:05, but none around 18:15.  p99 heatbeat latency did spike a few times to almost 4s, including two spikes to 2.7s around 18:12:30 and 18:15.  Both of those were on .214, which is db2 (n3).  That's what the third load generator was talking to.

Checked that CockroachDB has not restarted on either of those nodes.

[source,text]
----
192.168.1.66  db1 == n3 workload failed at 18:05
192.168.1.214 db2 == n2 workload failed at 18:15
----

Interesting that they started 10 minutes apart and failed 10 minutes apart,
just about 2h10m after starting.


Next step: check logs.  On db1, this would be around 18:05.  This looks surprising from the log:

[source,text]
----
W200918 18:04:53.861971 1762946 vendor/google.golang.org/grpc/internal/channelz/logging.go:73  grpc: addrConn.createTransport failed to connect to {192.168.1.103:26257  <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 192.168.1.103:26257: connect: connection refused". Reconnecting...
W200918 18:04:53.862255 1762949 vendor/google.golang.org/grpc/internal/channelz/logging.go:73  grpc: addrConn.createTransport failed to connect to {192.168.1.152:26257  <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 192.168.1.152:26257: connect: connection refused". Reconnecting...
W200918 18:04:54.862449 1762949 vendor/google.golang.org/grpc/internal/channelz/logging.go:73  grpc: addrConn.createTransport failed to connect to {192.168.1.152:26257  <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing cannot reuse client connection". Reconnecting...
W200918 18:04:54.862545 1762946 vendor/google.golang.org/grpc/internal/channelz/logging.go:73  grpc: addrConn.createTransport failed to connect to {192.168.1.103:26257  <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing cannot reuse client connection". Reconnecting...
----

Note that .103 and .152 are other cockroachdb server nodes: n5 and n4, respectively.  Those should both be offline -- and they are, and have been since 15:35Z.  Why those messages then?  Actually, these seem to be spat out every 30 seconds ro so.  So this is probably a red herring.

I just realized that the `client_loop` error may have come from `ssh` -- around 11:05am PT and 11:15am PT.  I'm not sure why it would have affected those two and not the other load generator, and at different times, too.  I don't see anything too correlated in the system log on the load generator:

[source,text]
----
Sep 18 18:08:16 loadgen0 sshd[2335]: [ID 800047 auth.error] error: kex_exchange_identification: Connection closed by remote host
Sep 18 18:24:10 loadgen0 sshd[2610]: [ID 800047 auth.error] error: kex_exchange_identification: Connection closed by remote host
----

Obvious sources would be:

- NAT state drop in my home router.  (No log message in the remote side?)
- NAT state drop inside AWS?

but why not the first ssh session?  Maybe I should use ServerAliveInterval to keep these open.

Anyway, there's no indication of a CockroachDB problem here.  I'm going to restart the two workloads.

20:44Z: restarted those two workloads
20:59:24: brought up cockroachdb on db3.
22:15Z: brought up cockroachdb on db4
23:12Z: brought up cockroachdb on db5
23:35Z: start load generators pointed at db3, db4, db5

Summary of results:

* When I brought up db3 at 20:59: there was a spike in p95 latency to over 40ms (from about 18ms) and a brief dip in throughput from about 280 qps to about 70 (but the period was so short that that time average is probably not meaningful).  After that, p95 latency was slightly lower than before, and throughput was slightly better (about 320qps).
* When I brought up db4 at 22:15Z: throughput shot up to about 460qps.  p95 latency improved accordingly.
* When I brought up db5 at 23:12Z, there was another spike in latency and brief dip in throughput, after which they were both about the same as before.  However, three nodes were still processing 0 requests.
* When I brought up the extra three load generators at 23:35Z, request throughput evened out across all six nodes.  Overall throughput increased a fair bit, but it's ranged from 620-750qps -- not super consistent yet (as of 23:53Z).

Through all of this:

* CPU utilization on all CPUs has been below 80%.
* There have been two disk %busy spikes over 75%, both very brief -- mostly they've been below 65%.
* Average disk I/O time has largely been under 1ms, with a few spikes as high as 15ms or so.  (Interestingly, that 15ms outlier correlates with 5ms spent queued in the device driver, which is very rare -- that generally doesn't exceed a few tens of microseconds.)
* p99 CockroachDB heartbeat latency has peaked at around 850ms, correlated with an 800ms spike in p99 round-trip latency around that time.
* The internal error rate has peaked at about 3 per second.  It's largely been zero, with spikes around 2100Z, 22:05Z, 22:15Z, 23:15Z.  The biggest spikes have been around when we bring up new nodes.  I'm not sure I'm looking at SQL errors though -- these might all be internal errors.

This has been good operational experience playing around with these operations, but feels a bit unfocused.

---

Stepping back, here are a few things I would like to better understand:

* What's the performance impact of making write requests to the leaseholder of a range vs. one of the other nodes with a replica vs. one of the other nodes in the cluster?
* How does that inform the best way to do basic load testing?
* When is splitting better for load?  When is merging better?  How can you tell from the metrics?

Is it worth trying to demonstrate horizontal scalability by starting up, say, 6 load generators pointing at a 3-node cluster, then expanding it to 6 nodes?  (Or, equivalently? starting 6 load generators, each pointing to a node in the 6-node cluster I already have, and then gradually repointing a load generator at one of the first three as I remove the last three nodes?)

It might be worth stepping back to better understand what we'd really like to know, which is probably something like:

* how is a moderately heavy read-write workload affected when one node disappears
** for just a minute?
** for a while (at least long enough for the cluster to rebalance)?
** immediately after that node comes back?
** for an extended period after that node comes back (long enough for the cluster to rebalance)
 In particular, I think we want to know the change in throughput, tail latency, and errors.
* how does a moderately heavy read-write workload run for an extended period (at least 24h, maybe a week)
* similar questions for a few different actions:
** kill -9 a node
** OS panic a node
** hardware reset a node
** decommission a node
** drain a node
** introduce a partition around a node
*** for just a little while
*** for an extended period

Next steps:

* Make sure I know how to monitor SQL errors.
* Figure out tests to answer my questions above.


== 2020-09-21

Summary of the day:

* Reviewed how reads and writes work in CockroachDB.  Tested this out to see how being a leaseholder affects throughput on that node.
* Dug into load-based splitting and merging to understand the impact.
* Dug a bit deeper into the metrics.  I know how to look for SQL failures now, as well as how to see splits and merges.
* Revised README in this repo to reflect the work done so far and where to go from here.

Details:

The https://www.cockroachlabs.com/docs/v20.1/architecture/reads-and-writes-overview[architectural overview of reads and writes] explains that issuing reads or writes to the leaseholder of a range should avoid one internal network round-trips.  From my read, it wouldn't matter if you issued the request to a different node holding a replica for the range or not, as long as it wasn't the leaseholder.  One idea would be to try to empirically measure this, but I'm not sure how useful that is.

Found and skimmed the https://wiki.crdb.io/wiki/spaces/CRDB/overview?homepageId=56197319[CRDB wiki] (got here looking for suggestions for performance testing).

Plan for today:

* Make sure I know how to monitor SQL errors.
* Restart cluster.  Drop sbtest database and recreate it.
* Start the workload and run it for like 10 minutes.  Figure out what range it's working on.  Figure out where the leaseholder for that range is.  Run the query for about 5 minutes each a few times on the leaseholder, another replica, and another cluster node.
* Dig into when merging vs. splitting is better for load.

Details:

* Restarted cluster.  All nodes came up fine.
* Dropped and recreated database "sbtest".  Looked at ranges:

[source,text]
----
> show ranges from database sbtest;
  table_name | start_key | end_key | range_id | range_size_mb | lease_holder | lease_holder_locality | replicas | replica_localities
-------------+-----------+---------+----------+---------------+--------------+-----------------------+----------+---------------------
  sbtest1    | NULL      | NULL    |      115 |      2.151715 |            6 |                       | {4,5,6}  | {"","",""}
  sbtest1    | NULL      | NULL    |      116 |      0.329636 |            6 |                       | {4,5,6}  | {"","",""}
(2 rows)

Time: 17.045268ms
----

There are two ranges (one for the table and one for the index?), both on leaseholder 6 (presumably n6, which is 192.168.1.190) and with replicas on nodes 4 and 5 (presumably n4 and n5, which are 192.168.1.152 and 192.168.1.103, respectively).

For good measure, let's confirm our understanding of those ranges:

[source,text]
----
root@192.168.1.227:26257/sbtest> SHOW TABLES;
  schema_name | table_name | type
--------------+------------+--------
  public      | sbtest1    | table
(1 row)

Time: 8.173483ms

root@192.168.1.227:26257/sbtest> SHOW INDEXES FROM sbtest1;
  table_name | index_name | non_unique | seq_in_index | column_name | direction | storing | implicit
-------------+------------+------------+--------------+-------------+-----------+---------+-----------
  sbtest1    | primary    |   false    |            1 | id          | ASC       |  false  |  false
  sbtest1    | k_1        |    true    |            1 | k           | ASC       |  false  |  false
  sbtest1    | k_1        |    true    |            2 | id          | ASC       |  false  |   true
(3 rows)

Time: 8.326325ms

root@192.168.1.227:26257/sbtest> SHOW RANGES FROM TABLE sbtest1;
  start_key | end_key | range_id | range_size_mb | lease_holder | lease_holder_locality | replicas | replica_localities
------------+---------+----------+---------------+--------------+-----------------------+----------+---------------------
  NULL      | NULL    |      115 |      2.151715 |            6 |                       | {4,5,6}  | {"","",""}
(1 row)

Time: 12.654598ms

root@192.168.1.227:26257/sbtest> SHOW RANGES FROM INDEX sbtest1 @ k_1;
  start_key | end_key | range_id | range_size_mb | lease_holder | lease_holder_locality | replicas | replica_localities
------------+---------+----------+---------------+--------------+-----------------------+----------+---------------------
  NULL      | NULL    |      116 |      0.329636 |            6 |                       | {4,5,6}  | {"","",""}
(1 row)

Time: 13.490146ms
----

Good.

Now, let's run workloads in the following sequence:

- n6: 5 minutes (leaseholder)
- n5: 5 minutes (non-leaseholder replica)
- n3: 5 minutes (non-replica)
- n2: 5 minutes (non-replica)
- n4: 5 minutes (non-leaseholder replica)
- n6: 5 minutes (leaseholder)

Basic command:

[source,text]
----
sysbench --threads=1 --time=0 --pgsql-host=192.168.1.214 --pgsql-port=26257 --pgsql-user=root --pgsql-db=sbtest oltp_insert run
----

So that suggests:

[source,bash]
----
n2=192.168.1.214
n3=192.168.1.66
n4=192.168.1.152
n5=192.168.1.103
n6=192.168.1.190

set -o errexit

for host in $n6 $n5 $n3 $n2 $n4 $n6; do
	echo "host: $host"
	sysbench --threads=1 --time=300 --pgsql-port=26257 --pgsql-user=root --pgsql-db=sbtest --pgsql-host=$n6 oltp_insert run
	sleep 60
done
----

Result:

[source,text]
----
$ bash load.sh
2020-09-21T18:03:57Z: host: 192.168.1.190
sysbench 1.1.0-bbee5d5 (using bundled LuaJIT 2.1.0-beta3)

Running the test with following options:
Number of threads: 1
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            0
        write:                           37102
        other:                           0
        total:                           37102
    transactions:                        37102  (123.67 per sec.)
    queries:                             37102  (123.67 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

Throughput:
    events/s (eps):                      123.6707
    time elapsed:                        300.0063s
    total number of events:              37102

Latency (ms):
         min:                                    3.19
         avg:                                    8.08
         max:                                  124.42
         95th percentile:                       10.65
         sum:                               299930.37

Threads fairness:
    events (avg/stddev):           37102.0000/0.00
    execution time (avg/stddev):   299.9304/0.00

2020-09-21T18:09:57Z: host: 192.168.1.103
sysbench 1.1.0-bbee5d5 (using bundled LuaJIT 2.1.0-beta3)

Running the test with following options:
Number of threads: 1
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            0
        write:                           37414
        other:                           0
        total:                           37414
    transactions:                        37414  (124.71 per sec.)
    queries:                             37414  (124.71 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

Throughput:
    events/s (eps):                      124.7104
    time elapsed:                        300.0071s
    total number of events:              37414

Latency (ms):
         min:                                    3.86
         avg:                                    8.02
         max:                                   78.27
         95th percentile:                       10.84
         sum:                               299932.03

Threads fairness:
    events (avg/stddev):           37414.0000/0.00
    execution time (avg/stddev):   299.9320/0.00

2020-09-21T18:15:57Z: host: 192.168.1.66
sysbench 1.1.0-bbee5d5 (using bundled LuaJIT 2.1.0-beta3)

Running the test with following options:
Number of threads: 1
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            0
        write:                           37399
        other:                           0
        total:                           37399
    transactions:                        37399  (124.66 per sec.)
    queries:                             37399  (124.66 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

Throughput:
    events/s (eps):                      124.6603
    time elapsed:                        300.0074s
    total number of events:              37399

Latency (ms):
         min:                                    3.53
         avg:                                    8.02
         max:                                   68.68
         95th percentile:                       10.46
         sum:                               299931.79

Threads fairness:
    events (avg/stddev):           37399.0000/0.00
    execution time (avg/stddev):   299.9318/0.00

2020-09-21T18:21:57Z: host: 192.168.1.214
sysbench 1.1.0-bbee5d5 (using bundled LuaJIT 2.1.0-beta3)

Running the test with following options:
Number of threads: 1
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            0
        write:                           37366
        other:                           0
        total:                           37366
    transactions:                        37366  (124.55 per sec.)
    queries:                             37366  (124.55 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

Throughput:
    events/s (eps):                      124.5527
    time elapsed:                        300.0014s
    total number of events:              37366

Latency (ms):
         min:                                    3.88
         avg:                                    8.03
         max:                                   71.34
         95th percentile:                       11.04
         sum:                               299926.44

Threads fairness:
    events (avg/stddev):           37366.0000/0.00
    execution time (avg/stddev):   299.9264/0.00

2020-09-21T18:27:57Z: host: 192.168.1.152
sysbench 1.1.0-bbee5d5 (using bundled LuaJIT 2.1.0-beta3)

Running the test with following options:
Number of threads: 1
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            0
        write:                           37428
        other:                           0
        total:                           37428
    transactions:                        37428  (124.76 per sec.)
    queries:                             37428  (124.76 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

Throughput:
    events/s (eps):                      124.7568
    time elapsed:                        300.0076s
    total number of events:              37428

Latency (ms):
         min:                                    4.07
         avg:                                    8.01
         max:                                   75.42
         95th percentile:                       11.04
         sum:                               299932.50

Threads fairness:
    events (avg/stddev):           37428.0000/0.00
    execution time (avg/stddev):   299.9325/0.00

2020-09-21T18:33:57Z: host: 192.168.1.190
sysbench 1.1.0-bbee5d5 (using bundled LuaJIT 2.1.0-beta3)

Running the test with following options:
Number of threads: 1
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            0
        write:                           37273
        other:                           0
        total:                           37273
    transactions:                        37273  (124.24 per sec.)
    queries:                             37273  (124.24 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

Throughput:
    events/s (eps):                      124.2423
    time elapsed:                        300.0024s
    total number of events:              37273

Latency (ms):
         min:                                    3.31
         avg:                                    8.05
         max:                                   58.86
         95th percentile:                       10.27
         sum:                               299926.12

Threads fairness:
    events (avg/stddev):           37273.0000/0.00
    execution time (avg/stddev):   299.9261/0.00

----

After running this (which took about 40 minutes):

* the requests were indeed handled by different nodes
* the throughput was reasonably constant (125 inserts / second) regardless of which node we were using
* It appeared that there was a merge at some point between ranges 115 and 116, because there's only one range associated with "sbtest" now and it's range 115.  It's still held on n6 with replicas on 4, 5, and 6.  From the metrics, the merge happened at about 18:45:30Z, which is after the last experiment.
* Disk utilization was basically the same through all runs, and about 2x higher on db5 (n6) than all other nodes.  That's largely true for CPU utilization as well, though it bounced around a bit between nodes in different runs.
* db3, db4, and db5 were consistently doing about 1K write IOPS, while the other nodes were doing basically none.
* There was less network traffic when n6 was being hit.

It seems like maybe the network round-trip was not the blocker.  That makes sense, as the p50 network RTT is about 800us.  p99 spiked up to at most 15ms.  By comparison, p95 SQL latency was about 10ms, with p99 about 18ms.

Let's try the same thing with concurrency=4 to minimize the impact of waiting on the client.  I'm not going to wipe the database before this run.

Results from this sequence:

[source,text]
----
cockroachdb@loadgen0:~$ bash load.sh
2020-09-21T20:02:11Z: host: 192.168.1.190
sysbench 1.1.0-bbee5d5 (using bundled LuaJIT 2.1.0-beta3)

Running the test with following options:
Number of threads: 4
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            0
        write:                           255401
        other:                           0
        total:                           255401
    transactions:                        255401 (851.33 per sec.)
    queries:                             255401 (851.33 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

Throughput:
    events/s (eps):                      851.3275
    time elapsed:                        300.0032s
    total number of events:              255401

Latency (ms):
         min:                                    2.16
         avg:                                    4.70
         max:                                   56.52
         95th percentile:                        7.84
         sum:                              1199501.13

Threads fairness:
    events (avg/stddev):           63850.2500/25.78
    execution time (avg/stddev):   299.8753/0.00

2020-09-21T20:08:11Z: host: 192.168.1.103
sysbench 1.1.0-bbee5d5 (using bundled LuaJIT 2.1.0-beta3)

Running the test with following options:
Number of threads: 4
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            0
        write:                           210668
        other:                           0
        total:                           210668
    transactions:                        210668 (702.22 per sec.)
    queries:                             210668 (702.22 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

Throughput:
    events/s (eps):                      702.2183
    time elapsed:                        300.0036s
    total number of events:              210668

Latency (ms):
         min:                                    2.81
         avg:                                    5.69
         max:                                   63.35
         95th percentile:                        8.43
         sum:                              1199577.67

Threads fairness:
    events (avg/stddev):           52667.0000/214.72
    execution time (avg/stddev):   299.8944/0.00

2020-09-21T20:14:11Z: host: 192.168.1.66
sysbench 1.1.0-bbee5d5 (using bundled LuaJIT 2.1.0-beta3)

Running the test with following options:
Number of threads: 4
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            0
        write:                           223659
        other:                           0
        total:                           223659
    transactions:                        223659 (745.52 per sec.)
    queries:                             223659 (745.52 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

Throughput:
    events/s (eps):                      745.5164
    time elapsed:                        300.0055s
    total number of events:              223659

Latency (ms):
         min:                                    2.69
         avg:                                    5.36
         max:                                   66.33
         95th percentile:                        7.98
         sum:                              1199545.61

Threads fairness:
    events (avg/stddev):           55914.7500/84.28
    execution time (avg/stddev):   299.8864/0.00

2020-09-21T20:20:11Z: host: 192.168.1.214
sysbench 1.1.0-bbee5d5 (using bundled LuaJIT 2.1.0-beta3)

Running the test with following options:
Number of threads: 4
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            0
        write:                           218966
        other:                           0
        total:                           218966
    transactions:                        218966 (729.88 per sec.)
    queries:                             218966 (729.88 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

Throughput:
    events/s (eps):                      729.8776
    time elapsed:                        300.0037s
    total number of events:              218966

Latency (ms):
         min:                                    2.82
         avg:                                    5.48
         max:                                   71.20
         95th percentile:                        7.98
         sum:                              1199569.89

Threads fairness:
    events (avg/stddev):           54741.5000/63.66
    execution time (avg/stddev):   299.8925/0.00

2020-09-21T20:26:11Z: host: 192.168.1.152
sysbench 1.1.0-bbee5d5 (using bundled LuaJIT 2.1.0-beta3)

Running the test with following options:
Number of threads: 4
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            0
        write:                           212824
        other:                           0
        total:                           212824
    transactions:                        212824 (709.40 per sec.)
    queries:                             212824 (709.40 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

Throughput:
    events/s (eps):                      709.4044
    time elapsed:                        300.0038s
    total number of events:              212824

Latency (ms):
         min:                                    2.72
         avg:                                    5.64
         max:                                   72.86
         95th percentile:                        8.28
         sum:                              1199565.97

Threads fairness:
    events (avg/stddev):           53206.0000/86.02
    execution time (avg/stddev):   299.8915/0.00

2020-09-21T20:32:11Z: host: 192.168.1.190
sysbench 1.1.0-bbee5d5 (using bundled LuaJIT 2.1.0-beta3)

Running the test with following options:
Number of threads: 4
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            0
        write:                           258800
        other:                           0
        total:                           258800
    transactions:                        258800 (862.65 per sec.)
    queries:                             258800 (862.65 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

Throughput:
    events/s (eps):                      862.6541
    time elapsed:                        300.0044s
    total number of events:              258800

Latency (ms):
         min:                                    2.21
         avg:                                    4.63
         max:                                  115.68
         95th percentile:                        7.70
         sum:                              1199494.31

Threads fairness:
    events (avg/stddev):           64700.0000/72.58
    execution time (avg/stddev):   299.8736/0.00

----

In this run, we did see a clear difference between writes to the leaseholder vs. writes to other nodes, with no real difference among the other nodes.  Summary:

[source,text]
----
RAW RESULTS (5-minute average TPS, from raw output above):

    192.168.1.190 851.3275
    192.168.1.103 702.2183
    192.168.1.66  745.5164
    192.168.1.214 729.8776
    192.168.1.152 709.4044
    192.168.1.190 862.65

BY NODE:

    IP             NODE  ROLE        TPS (multiple runs)
    192.168.1.190  n6    leaseholder 863 851
    192.168.1.103  n5    replica     702
    192.168.1.66   n3    general     746
    192.168.1.214  n2    general     730
    192.168.1.152  n4    replica     709

BY ROLE ONLY:

    ROLE        TPS (multiple runs)
    leaseholder 863 851
    replica     702 709
    general     746 730

----

Indeed, the leaseholder performs at least 14% - 23% better than the other nodes.  Surprisingly, replicas do a bit worse than non-replicas.

Other notes from this experiment:

* The throughput-per-node graph confirms that we hit different nodes.
* Disk %busy remained at or below 60% on all nodes, with n6 by far being the busiest (as before).  Plenty of headroom in that sense.
* CPU utilization remained at or below about 60% on all CPUs, so plenty of headroom there.
* As before, the three replica nodes were pegged at about 1K write IOPS, with no read IOPS going on and no activity on the other three nodes.
* We were much better at keeping the pipeline full: the average active queries remained above 2 for the duration, with few exceptions.

Finally, check on the ranges to make sure they didn't change during the test:

[source,text]
----
root@192.168.1.227:26257/sbtest> show ranges from table sbtest1;
  start_key | end_key | range_id | range_size_mb | lease_holder | lease_holder_locality | replicas | replica_localities
------------+---------+----------+---------------+--------------+-----------------------+----------+---------------------
  NULL      | NULL    |      115 |    400.569721 |            6 |                       | {4,5,6}  | {"","",""}
(1 row)

Time: 11.999547ms

root@192.168.1.227:26257/sbtest> show ranges from index sbtest1 @ primary;
  start_key | end_key | range_id | range_size_mb | lease_holder | lease_holder_locality | replicas | replica_localities
------------+---------+----------+---------------+--------------+-----------------------+----------+---------------------
  NULL      | NULL    |      115 |    400.569721 |            6 |                       | {4,5,6}  | {"","",""}
(1 row)

Time: 12.867224ms

root@192.168.1.227:26257/sbtest> show ranges from index sbtest1 @ k_1;
  start_key | end_key | range_id | range_size_mb | lease_holder | lease_holder_locality | replicas | replica_localities
------------+---------+----------+---------------+--------------+-----------------------+----------+---------------------
  NULL      | NULL    |      115 |    400.569721 |            6 |                       | {4,5,6}  | {"","",""}
(1 row)

Time: 12.278072ms

root@192.168.1.227:26257/sbtest> show ranges from database sbtest;
  table_name | start_key | end_key | range_id | range_size_mb | lease_holder | lease_holder_locality | replicas | replica_localities
-------------+-----------+---------+----------+---------------+--------------+-----------------------+----------+---------------------
  sbtest1    | NULL      | NULL    |      115 |    400.569721 |            6 |                       | {4,5,6}  | {"","",""}
(1 row)

Time: 10.485938ms
----

That looks good -- same state as before.

=== Read performance against leaseholder vs. replica vs. other

Now, I want to try the same experiment again for a read-heavy workload.  I've modified the script to run `oltp_point_select` instead of `oltp_insert`:

[source,bash]
----
#!/bin/bash

n2=192.168.1.214
n3=192.168.1.66
n4=192.168.1.152
n5=192.168.1.103
n6=192.168.1.190

set -o errexit

for host in $n6 $n5 $n3 $n2 $n4 $n6; do
        echo "$(date +%FT%TZ): host: $host"
        sysbench --threads=4 --time=300 --pgsql-port=26257 --pgsql-user=root --pgsql-db=sbtest --pgsql-host=$host oltp_point_select run
        sleep 60
done
----

To run this, I'm going to drop the database first and recreate it, which means re-checking the ranges:

[source,text]
----
$ sysbench --threads=1 --time=0 --pgsql-host=192.168.1.227 --pgsql-port=26257 --pgsql-user=root --pgsql-db=sbtest oltp_point_select prepare
WARNING: Both event and time limits are disabled, running an endless test
sysbench 1.1.0-bbee5d5 (using bundled LuaJIT 2.1.0-beta3)

Creating table 'sbtest1'...
Inserting 10000 records into 'sbtest1'
Creating a secondary index on 'sbtest1'...
cockroachdb@loadgen0:~$ 
----

Here are the resulting schemas and ranges:

[source,text]
----
root@192.168.1.227:26257/defaultdb> SHOW RANGES FROM DATABASE sbtest;
  table_name | start_key | end_key | range_id | range_size_mb | lease_holder | lease_holder_locality | replicas | replica_localities
-------------+-----------+---------+----------+---------------+--------------+-----------------------+----------+---------------------
  sbtest1    | NULL      | NULL    |      117 |      2.151715 |            6 |                       | {4,5,6}  | {"","",""}
  sbtest1    | NULL      | NULL    |      118 |      0.329636 |            6 |                       | {4,5,6}  | {"","",""}
(2 rows)

Time: 12.807122ms

root@192.168.1.227:26257/defaultdb> use sbtest;
SET

Time: 3.329146ms

root@192.168.1.227:26257/sbtest> show ranges from table sbtest1;
  start_key | end_key | range_id | range_size_mb | lease_holder | lease_holder_locality | replicas | replica_localities
------------+---------+----------+---------------+--------------+-----------------------+----------+---------------------
  NULL      | NULL    |      117 |      2.151715 |            6 |                       | {4,5,6}  | {"","",""}
(1 row)

Time: 13.086509ms

root@192.168.1.227:26257/sbtest> show ranges from index sbtest1 @ primary;
  start_key | end_key | range_id | range_size_mb | lease_holder | lease_holder_locality | replicas | replica_localities
------------+---------+----------+---------------+--------------+-----------------------+----------+---------------------
  NULL      | NULL    |      117 |      2.151715 |            6 |                       | {4,5,6}  | {"","",""}
(1 row)

Time: 12.772464ms

root@192.168.1.227:26257/sbtest> show ranges from index sbtest1 @ k_1;
  start_key | end_key | range_id | range_size_mb | lease_holder | lease_holder_locality | replicas | replica_localities
------------+---------+----------+---------------+--------------+-----------------------+----------+---------------------
  NULL      | NULL    |      118 |      0.329636 |            6 |                       | {4,5,6}  | {"","",""}
(1 row)

Time: 12.971536ms

root@192.168.1.227:26257/sbtest> SHOW TABLES;
  schema_name | table_name | type
--------------+------------+--------
  public      | sbtest1    | table
(1 row)

Time: 8.281401ms

root@192.168.1.227:26257/sbtest> show index from sbtest1;
  table_name | index_name | non_unique | seq_in_index | column_name | direction | storing | implicit
-------------+------------+------------+--------------+-------------+-----------+---------+-----------
  sbtest1    | primary    |   false    |            1 | id          | ASC       |  false  |  false
  sbtest1    | k_1        |    true    |            1 | k           | ASC       |  false  |  false
  sbtest1    | k_1        |    true    |            2 | id          | ASC       |  false  |   true
(3 rows)

Time: 8.101588ms

----

We got new ranges, but they're on the same nodes, with the same leaseholder, so we can use the same load.sh script (changing the workload).

Here are the detailed results:

[source,text]
----
$ time bash load.sh
2020-09-21T20:59:58Z: host: 192.168.1.190
sysbench 1.1.0-bbee5d5 (using bundled LuaJIT 2.1.0-beta3)

Running the test with following options:
Number of threads: 4
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            1383060
        write:                           0
        other:                           0
        total:                           1383060
    transactions:                        1383060 (4610.17 per sec.)
    queries:                             1383060 (4610.17 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

Throughput:
    events/s (eps):                      4610.1706
    time elapsed:                        300.0019s
    total number of events:              1383060

Latency (ms):
         min:                                    0.48
         avg:                                    0.87
         max:                                   41.97
         95th percentile:                        1.39
         sum:                              1199127.44

Threads fairness:
    events (avg/stddev):           345765.0000/447.71
    execution time (avg/stddev):   299.7819/0.00

2020-09-21T21:05:58Z: host: 192.168.1.103
sysbench 1.1.0-bbee5d5 (using bundled LuaJIT 2.1.0-beta3)

Running the test with following options:
Number of threads: 4
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            689759
        write:                           0
        other:                           0
        total:                           689759
    transactions:                        689759 (2299.18 per sec.)
    queries:                             689759 (2299.18 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

Throughput:
    events/s (eps):                      2299.1810
    time elapsed:                        300.0020s
    total number of events:              689759

Latency (ms):
         min:                                    0.94
         avg:                                    1.74
         max:                                   32.18
         95th percentile:                        2.39
         sum:                              1199594.98

Threads fairness:
    events (avg/stddev):           172439.7500/269.51
    execution time (avg/stddev):   299.8987/0.00

2020-09-21T21:11:58Z: host: 192.168.1.66
sysbench 1.1.0-bbee5d5 (using bundled LuaJIT 2.1.0-beta3)

Running the test with following options:
Number of threads: 4
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            1380248
        write:                           0
        other:                           0
        total:                           1380248
    transactions:                        1380248 (4600.80 per sec.)
    queries:                             1380248 (4600.80 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

Throughput:
    events/s (eps):                      4600.8011
    time elapsed:                        300.0017s
    total number of events:              1380248

Latency (ms):
         min:                                    0.48
         avg:                                    0.87
         max:                                   31.00
         95th percentile:                        1.39
         sum:                              1199145.44

Threads fairness:
    events (avg/stddev):           345062.0000/813.18
    execution time (avg/stddev):   299.7864/0.00

2020-09-21T21:17:58Z: host: 192.168.1.214
sysbench 1.1.0-bbee5d5 (using bundled LuaJIT 2.1.0-beta3)

Running the test with following options:
Number of threads: 4
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            649948
        write:                           0
        other:                           0
        total:                           649948
    transactions:                        649948 (2166.48 per sec.)
    queries:                             649948 (2166.48 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

Throughput:
    events/s (eps):                      2166.4816
    time elapsed:                        300.0016s
    total number of events:              649948

Latency (ms):
         min:                                    0.96
         avg:                                    1.85
         max:                                   37.94
         95th percentile:                        2.57
         sum:                              1199618.76

Threads fairness:
    events (avg/stddev):           162487.0000/99.93
    execution time (avg/stddev):   299.9047/0.00

2020-09-21T21:23:58Z: host: 192.168.1.152
sysbench 1.1.0-bbee5d5 (using bundled LuaJIT 2.1.0-beta3)

Running the test with following options:
Number of threads: 4
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            692496
        write:                           0
        other:                           0
        total:                           692496
    transactions:                        692496 (2308.30 per sec.)
    queries:                             692496 (2308.30 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

Throughput:
    events/s (eps):                      2308.3036
    time elapsed:                        300.0021s
    total number of events:              692496

Latency (ms):
         min:                                    0.94
         avg:                                    1.73
         max:                                  114.17
         95th percentile:                        2.39
         sum:                              1199590.33

Threads fairness:
    events (avg/stddev):           173124.0000/235.70
    execution time (avg/stddev):   299.8976/0.00

2020-09-21T21:29:58Z: host: 192.168.1.190
sysbench 1.1.0-bbee5d5 (using bundled LuaJIT 2.1.0-beta3)

Running the test with following options:
Number of threads: 4
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            724305
        write:                           0
        other:                           0
        total:                           724305
    transactions:                        724305 (2414.32 per sec.)
    queries:                             724305 (2414.32 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

Throughput:
    events/s (eps):                      2414.3216
    time elapsed:                        300.0035s
    total number of events:              724305

Latency (ms):
         min:                                    0.93
         avg:                                    1.66
         max:                                   35.97
         95th percentile:                        2.22
         sum:                              1199568.27

Threads fairness:
    events (avg/stddev):           181076.2500/116.19
    execution time (avg/stddev):   299.8921/0.00


real    36m0.207s
user    0m48.721s
sys     1m44.241s
----

Check range status:

[source,text]
----
root@192.168.1.227:26257/sbtest> show ranges from table sbtest1;
  start_key | end_key | range_id | range_size_mb | lease_holder | lease_holder_locality | replicas | replica_localities
------------+---------+----------+---------------+--------------+-----------------------+----------+---------------------
  NULL      | NULL    |      117 |      2.151715 |            3 |                       | {3,5,6}  | {"","",""}
(1 row)

Time: 121.712751ms

root@192.168.1.227:26257/sbtest>  show ranges from index sbtest1 @ primary;
  start_key | end_key | range_id | range_size_mb | lease_holder | lease_holder_locality | replicas | replica_localities
------------+---------+----------+---------------+--------------+-----------------------+----------+---------------------
  NULL      | NULL    |      117 |      2.151715 |            3 |                       | {3,5,6}  | {"","",""}
(1 row)

Time: 13.173433ms

root@192.168.1.227:26257/sbtest> show ranges from index sbtest1 @ k_1;
  start_key | end_key | range_id | range_size_mb | lease_holder | lease_holder_locality | replicas | replica_localities
------------+---------+----------+---------------+--------------+-----------------------+----------+---------------------
  NULL      | NULL    |      118 |      0.329636 |            4 |                       | {4,5,6}  | {"","",""}
(1 row)

Time: 14.280008ms

root@192.168.1.227:26257/sbtest> SHOW ranges from database sbtest;
  table_name | start_key | end_key | range_id | range_size_mb | lease_holder | lease_holder_locality | replicas | replica_localities
-------------+-----------+---------+----------+---------------+--------------+-----------------------+----------+---------------------
  sbtest1    | NULL      | NULL    |      117 |      2.151715 |            3 |                       | {3,5,6}  | {"","",""}
  sbtest1    | NULL      | NULL    |      118 |      0.329636 |            4 |                       | {4,5,6}  | {"","",""}
(2 rows)

Time: 13.843042ms
----

Unfortunately, this output means that range 117's lease was moved to n3 -- which explains some of the results I got:

[source,text]
----
NODE               QPS   STARTED AT
n6   192.168.1.190 4610  20:59:58Z
n5   192.168.1.103 2299  21:05:58Z
n3   192.168.1.66  4601  21:11:58Z
n2   192.168.1.214 2166  21:17:58Z
n4   192.168.1.152 2308  21:23:58Z
n6   192.168.1.190 2414  21:29:58Z 
----

(That's pulled out of the output above by hand.)

Based on the metrics in the Admin UI, I believe we have:

[source,text]
----
21:00:00 split, add, remove, load-based lease transfer
21:05:30 split, add, remove, lease transfer (non-load-based)
21:12:00 split + load-based lease transfer
21:17:30 range merge
----

It's a little hard to know what happened there.  Let's take a closer look at ranges 117 and 118.

Range 117:

[source,text]
----
20:57:17Z split 115 -> 117
20:57:20Z split 117 -> 118
21:00:00Z split 117 -> 119
21:00:21Z add n3, remove n4 (reason: rebalance)
21:05:52Z merge r117 with r119 (this is the first event labeled for store "s3" instead of "s6")
21:12:09Z split r117 into r135
21:17:27Z merge r117 with r135
21:57:24Z merge r117 with 4118
----

If we look at the lease history there, we see it on n6 from 17:42:08 to 21:01:23.  Reading between the lines, it seems like we split the range around 21:00 and moved the lease to n3.  My guess is that we split the range due to load and then moved one of the two halves to another node, and this was the half that got moved.

If we merge this timeline with our results above, we have:

[source,text]
----
NODE               QPS   STARTED AT  ROLE
n6   192.168.1.190 4610  20:59:58Z   initially leaseholder, changed to replica while running
n5   192.168.1.103 2299  21:05:58Z   replica
n3   192.168.1.66  4601  21:11:58Z   leaseholder
n2   192.168.1.214 2166  21:17:58Z   general
n4   192.168.1.152 2308  21:23:58Z   replica
n6   192.168.1.190 2414  21:29:58Z   replica
----

The trend remains fairly clear that reads from the leaseholder are quite a lot faster than from other nodes in the cluster.  This experiment is as controlled as we'd like, and we could potentially improve it by disabling load-based splitting or rebalancing, but this is consistent enough with the documentation that it's probably not worth exploring further.

=== Notes on performance, splits, and merges

Meanwhile: looking at https://www.cockroachlabs.com/docs/stable/load-based-splitting.html[load-based splitting] again.  It seems the main purpose is take a range that's hot, but where the load is distributed across at least two keys, and split it into two ranges that can then be https://www.cockroachlabs.com/docs/v20.1/architecture/replication-layer#load-based-replica-rebalancing[rebalanced] onto other nodes so that the load is split between two nodes.  According to that first source, CockroachDB won't split a range on which queries would have to hit both ranges after the split, since that's more expensive.

What about https://www.cockroachlabs.com/docs/stable/range-merges.html[merges]?  It looks like Cockroach only merges ranges based on size, not load.  This behavior does improve performance because the fewer ranges a query has to deal with, the fewer network round-trip times and other per-range overhead costs are needed, plus it's less likely to hit a non-functional or overloaded node.

It looks to me that if a workload is heavily concentrated on one key, splitting can't help.  You'll be limited to the capacity of a single node -- but in principle that entire node can be dedicated to that key, if it's really that hot.

I asked above how one can tell of splitting or merging will be good for performance.  I'm not sure there's an easy way to tell.  I do not see a great way to look at queries per range.  You _can_ look at range size, which will tell you when something might be a candidate for merging.

I decided to take a closer look at the metrics available -- there's lots of good stuff here, including pending merges and splits, etc.  New question, though: what's the difference between sql_exec_latency (SQL "statement" execution) and sql_service_latency (SQL "request" execution)?  I don't see any metrics with "range" as a label, so I don't think you can get queries per range out of Prometheus.

=== Next steps

Tactical steps:

* update dashboards
** show sql_exec_failures
** add graph of range splits, merges, etc.
* go through my notes up to this point and see if there are unexplained issues that need to be noted for further investigation (or just marked caveats)

The experiments from the last few days have helped me come to ground on some of my questions about testing from 9/18:

* Performance _does_ differ depending on whether the gateway node (that receives the request) is the leaseholder or not.  This means that performance can easily change during an experiment as CockroachDB splits ranges, merges them, and moves them as part of rebalancing.
* I don't fully understand haproxy's load balancing behavior well enough to rely on it during testing.  As a result, I'm pointing each load generator at individual CockroachDB nodes.  This too complicates testing things like horizontal scalability (increased throughput as the cluster expands), since we have to add or redirect load generators in order to use new capacity.

I revised the README to reflect the bigger directions we want to think about.

== 2020-09-22

Summary of the day:

* updated dashboard.
** show sql_exec_failures.
** add graph of range splits, merges, etc.
** add graph of queries by store.
* went through my notes up to this point and collected outstanding issues.  Added these to the README list.
* Completed an "online expansion" experiment that went well, but was limited.  See Conclusions below.
* Completed an "online contraction" experiment that went well, but was limited.  See Conclusions below.
* Started creating a much bigger database.

=== Summary of online expansion experiment

[source,text]
----
17:06Z Deployed a fresh cluster: 6 VMs, only started CockroachDB on 3 of them.
17:26Z Started 50% read / 50% write "kv" workload against db1 (n1)
17:34Z Started 50% read / 50% write "kv" workload against db2 (n2)
17:39Z Started 50% read / 50% write "kv" workload against db3 (n3)
18:05Z Bring up db4.  Rebalancing visible until about 18:06Z.
18:19Z Bring up db5.  Rebalancing visible until about 18:21Z.
19:54Z Bring up db6.  Rebalancing visible until about 19:56Z.
20:25Z End of experiment -- writing up summary.
----

Summary of impact:

* Throughput:
** Aggregate throughput started around 2500 qps (evenly split between "select" and "insert") and declined over about 90 minutes to about 2000 qps.
** Throughput on db1 is consistently about 15% higher than the other two nodes, which are about equal.  This is consistent with db1 being the leaseholder for the ranges being queried.  I confirmed via Admin UI that db1 is seeing nearly all of the queries.
** For the entire period, across all three load generators, throughput reported is 441+336+344 reads per second and 442+336+344 writes per second, which is 1121 total reads per second and 1122 writes per second, or 2243 queries per second.  Grafana reports 2,176 queries per second (from the server) for the same period (1,088 reads and 1,099 writes) -- it's good that these are consistent.
* Latency:
** Matches what you'd expect given the throughput graph.
** p95 latency for db1 is slightly better than the other two, which are about the same.  p95 ranges from about 8ms-11ms.  p99 has one major outlier from db1 up to 1.3s around 18:47Z.  There are two other excursions to 60ms and 160ms.  Ignoring these three, p99 ranged from about 14ms - 18ms, quite consistently, with db1 being slightly better than the other two.
* Errors:
** CockroachDB reported zero SQL failures.
** All three client load generators reported zero errors.
* Utilization and saturation
** CPU: the busiest CPUs were the two CPUs on db1, which are about 80-88% busy.  The other nodes are around 40% or less busy.
** Disk: average I/O time saw one excursion up to 4.7ms and a few up to 2ms, but averaged under 600us.  Disks got less busy over time, starting around 65% busy on db1 at the beginning and falling to about 45-50% busy.  Average write throughput is only about 2-3 MBps per node -- it's not a heavy workload.  We're hitting the max write IOPS of 1K most of the time on db1 and db2.  Other nodes are less busy.
** Network: throughput is fairly minimal (averaging under 1 MBps on all nodes).
* Other notes:
** The workload is pretty small.  The nodes are still 98.8% free in terms of disk space and the database is only 880 MiB.
** p99 CockroachDB heartbeat latency peaked below 60ms.
** Mean RPC clock offset peaked around 550us.
** We can clearly see the rebalancing happen in the change in replicas per node.
** We do see some miscellaneous range operations from time to time (splits, adds, removes, merges).
** There's a jump in under-replicated ranges around 18:20Z.  I suspect what happened is that we brought up the fifth node, which caused CockroachDB to flip its default replication factor for system ranges from 3 to 5, and then immediately replicate those ranges.

Overall this feels very successful.  I believe this workload is mainly 1-byte reads and writes, so it's heavy on operations, but not heavy on data throughput.  The load is clearly not well distributed -- it's nearly all being handled by db1.

There was that big latency outlier at 18:47Z.  That's correlated with a spike in both splits and merges at that time.

=== Details

Note: I've updated the naming of the "db" hosts so that they're 1-based like the CockroachDB naming.  Hopefully, this means db3 == n3, etc.

I've tainted all six database servers and redeployed them so that I can start with a fresh cluster of only three nodes.  I've initialized the cluster and set it up to run the "kv" workload with:

[source,text]
----
cockroach workload init kv postgresql://root@192.168.1.223:26257/kv?sslmode=disable
----

Now running the workload with:

[source,text]
----
cockroach workload run kv --histograms kv-histograms-$(date +%FT%TZ).out --concurrency 4 --display-every=60s --read-percent 50 --tolerate-errors postgresql://root@192.168.1.223:26257/kv?sslmode=disable
----

Started that around 17:26Z.

Will shortly start two more, redirected at the other two nodes: 192.168.1.14 and 192.168.1.121.

17:34Z: started second one
17:36Z: started third one

I ran that third one against the wrong host.  Rerunning it at 17:39Z.

Want to wait until at least 17:50Z before doing anything real.  Next step: enable cockroachdb on db4.

18:05Z: bring up db4
18:19Z: bring up db5
19:54Z: bring up db6

=== Online contraction

Next, I'm going to try an online scale-down (contraction).  What's the order that I want to do this?  Obvious would be to remove db6, db5, and db4 -- there are no load generators running on them.  However, all the load is still pointed at db1.  Maybe it's worth removing that one.  How about: db6, db5, then verify the load is still on db1, then remove that one.  In order to do this experiment, though, I'll want to move the load generator that's currently pointed at db1 over to db4, so that it can be kept running the entire time.  (Otherwise, I can't really tell if latency or errors happened during the operation.)

[source,text]
----
20:54Z Shut down load generator against db1.
20:55Z Restart same load generator against db4 (192.168.1.85)
20:10Z Begin decommission n6.  Took about 40 seconds.
19:20Z Begin decommission n5.  Took 37 seconds.
19:30Z Begin decommission n1, after verifying its store is still taking all the load.  Took 91 seconds.
21:45Z Actually shut down nodes n6, n5, n1.
----

In terms of impact: there was little impact on throughput, latency, error rate (0), etc. from decommissioning nodes n6 and n5.  There was a small spike in p95 and p99 latency on all nodes around 21:11 (p95 spiked to 18ms from about 12ms, p99 spiked to about 35ms from about 18ms).  There was almost nothing at 21:20Z.  We do see the replicas and leaseholders moving, and some internal errors, but that's it.  p99 heartbeat latency peaked below 125ms.

There was a much more significant impact for decommissioning n1: p95 latency spiked to about 33ms on all nodes, with p99 spiking up to 60ms.  Throughput saw a similar (brief) crash.  Queries per second for store 1 crashed t0 zero, while QPS for stores 2 and 4 rose up.

As expected, there was no impact for the actual shutdown at 21:45Z.

Broadly: there was some brief, notable performance impact, but no errors, and performance never got that bad. (60ms p99 is still pretty decent.)

Final output from db1 load generator, for reference:

[source,text]
----
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
12060.2s        0          413.8          438.7      1.7      3.4      4.7     26.2 read
12060.2s        0          413.2          439.2      7.6     12.1     17.8     39.8 write
12120.2s        0          400.0          438.5      1.7      3.7      6.0     41.9 read
12120.2s        0          402.3          439.0      7.6     13.1     18.9    109.1 write
12180.2s        0          413.1          438.4      1.6      3.4      4.7     22.0 read
12180.2s        0          415.8          438.9      7.6     12.1     18.9     44.0 write
12240.2s        0          414.4          438.3      1.7      3.4      5.0     28.3 read
12240.2s        0          409.9          438.8      7.6     12.6     17.8     44.0 write
12300.2s        0          393.6          438.1      1.7      3.8      6.0     31.5 read
12300.2s        0          394.6          438.6      7.9     14.2     21.0     71.3 write
12360.2s        0          363.4          437.7      1.9      4.7      7.6     39.8 read
12360.2s        0          363.5          438.2      8.1     15.7     24.1     65.0 write
12420.2s        0          393.5          437.5      1.8      3.8      6.0     39.8 read
12420.2s        0          397.0          438.0      7.6     13.6     21.0     65.0 write
^CNumber of reads that didn't return any results: 3.
Highest sequence written: 5444392. Can be passed as --write-seq=R5444392 to the next run.

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
12430.4s        0        5437954          437.5      1.6      1.4      3.1      4.7   1543.5  read

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
12430.4s        0        5444392          438.0      7.6      7.3     12.6     18.9   1543.5  write

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__result
12430.4s        0       10882346          875.5      4.6      3.8     10.5     16.3   1543.5
----

Conclusions:

* These experiments both went well.  However, there are two caveats: the database was tiny (peaked just over 1 GiB, just 2 ranges for most of the day).  That means most of the load was concentrated in one machine at a time and replication of each range was pretty fast.  It's worth redoing these with a much larger database size.

Also today: compared my sysbench numbers to the ones posted in the https://www.cockroachlabs.com/docs/stable/performance.html#latency[CockroachDB docs].  They tested on a 3-node AWS cluster of `c5d.9xlarge` instances with much higher concurrency, so this isn't an apples-to-apples comparison.  They got an average latency of 4.3ms for oltp_insert and 0.7ms for oltp_point_select.  I tested on 3 c4 instances, much smaller, and got (looking at data from yesterday) 5.6ms for oltp_insert and 0.9ms for oltp_point_select.  That's in the right ballpark, at least.

=== Making a bigger database

As mentioned above, I want to rerun these tests with a bigger database -- ideally a few tens of GiB.  That'll ensure that load is well distributed across the nodes.  It will also make sure there's a bunch of stuff to replicate when adding or removing nodes.

To do this, I'm going to increase the "kv" workload batch size and record size.

Plan: 

* done: drop the "kv" database
* done: mark db1, db5, db6 as tainted and then `terraform apply` to redeploy them.
* done: For my own sanity, enable CockroachDB on db1, decommission db4, mark it tainted, and redeploy it too.  Now I'll be back to a 3-node cluster with db1, db2, and db3.  But this didn't do what I expected: of course, db1 came back as n7.  Oh well.

22:50Z: I'm now attempting to fill the database using three copies of this command, one running against each node:

[source,text]
----
cockroach workload run kv --concurrency 4 --display-every=60s --batch 1000 --max-block-bytes 1024 --min-block-bytes 1024 postgresql://root@192.168.1.121:26257/kv?sslmode=disable
----

This is write-only.  I'm going to let this run for a while to see how quickly it goes.  Ideally, once the database is reasonably large, I'd drop the batch size back to 1 and make it 50% reads.  This will simulate a basic read-write workload on a reasonably large database.

Immediately, the CPU utilization of both CPUs on db1 are maxed out.  (Note that it's called db1 here because that's what terraform calls it.  It's actually n7 in CockroachDB.)  I expect to see some splitting happen.  This probably would have happened automatically if I'd used to the right flags above, but this will be interesting to see happen.

Also interesting is that the p95 and p99 are both close to 15s, with throughput only 1-5 per second.  That's presumably because the batch size and data size are so large.  We did see a split early on, and that seems to have reduced disk %busy, but not CPU utilization on db1.

22:55Z: I'm going to change these to batch size 100 to make it a bit more manageable -- both observable, and probably better for CRDB.  (It's hard to make guesses about load when queries are that big.)

On the plus side, the database is already at 2.7 GiB.  This shouldn't take all that long.

23:13Z: I can see we're still maxed out on CPU, but we're still only doing up to 10 queries per second.  I wonder if CockroachDB just can't tell that.  I'm going to change the batch size to 10 and see what happens to the rate.  (At this rate, it'll take over 8 hours to reach 50% full.)  I do observe that the rate of decrease in capacity did go down (i.e., we were filling it more slowly) when I reduced the batch size from 1000 to 100, so it's possible this too will make it worse.

It finally split the range around 23:25Z and the CPU became much more balanced.

2020-09-23T03:43Z: I finally stopped the workload.  The table is now 35 GiB with 32 ranges, so that seems pretty good -- a few multiples of DRAM.  Last data points from load generators:

[source,text]
----
16080.3s        0           16.6           21.1    226.5    453.0    570.4    738.2 write
16140.3s        0           17.7           21.1    209.7    469.8    604.0    738.2 write

^CHighest sequence written: 3408050. Can be passed as --write-seq=R3408050 to the next run.

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
16169.1s        0         340805           21.1    189.7    167.8    402.7    570.4  11274.3  write

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__result
16169.1s        0         340805           21.1    189.7    167.8    402.7    570.4  11274.3

----


[source,text]
----
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
15660.3s        0           17.6           21.3    184.5    520.1    671.1    805.3 write
15720.3s        0           17.5           21.2    201.3    453.0    704.6   1040.2 write
15780.3s        0           16.9           21.2    226.5    486.5    637.5    838.9 write
15840.3s        0           16.9           21.2    209.7    503.3    704.6   1006.6 write
15900.3s        0            9.1           21.2    419.4    805.3   1040.2   1140.9 write
15960.3s        0           12.1           21.1    302.0    704.6    973.1   1476.4 write
16020.3s        0           18.6           21.1    201.3    419.4    604.0    704.6 write
16080.3s        0           16.8           21.1    226.5    453.0    604.0    805.3 write
16140.3s        0           17.8           21.1    201.3    486.5    604.0    805.3 write
^CHighest sequence written: 3409280. Can be passed as --write-seq=R3409280 to the next run.

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
16168.3s        0         340927           21.1    189.7    167.8    419.4    637.5  12884.9  write

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__result
16168.3s        0         340927           21.1    189.7    167.8    419.4    637.5  12884.9

----


[source,text]
----
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
15660.3s        0           17.7           21.3    192.9    520.1    671.1    906.0 write
15720.3s        0           17.2           21.2    209.7    469.8    671.1    872.4 write
15780.3s        0           17.4           21.2    218.1    469.8    738.2   1040.2 write
15840.3s        0           16.6           21.2    218.1    503.3    738.2   1342.2 write
15900.3s        0            8.5           21.2    436.2    906.0   1140.9   1342.2 write
15960.3s        0           13.8           21.1    260.0    604.0    872.4   1342.2 write
16020.3s        0           18.2           21.1    209.7    419.4    604.0    872.4 write
16080.3s        0           16.5           21.1    226.5    486.5    671.1    973.1 write
16140.3s        0           17.6           21.1    192.9    503.3    604.0   1208.0 write
^CHighest sequence written: 3409410. Can be passed as --write-seq=R3409410 to the next run.

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
16164.9s        0         340941           21.1    189.6    159.4    436.2    637.5  12348.0  write

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__result
16164.9s        0         340941           21.1    189.6    159.4    436.2    637.5  12348.0
----

Here are the ranges and the leaseholder distribution of ranges:

[source,text]
----
root@192.168.1.121:26257/defaultdb> show ranges from database kv;
  table_name |       start_key       |        end_key        | range_id | range_size_mb | lease_holder | lease_holder_locality | replicas | replica_localities
-------------+-----------------------+-----------------------+----------+---------------+--------------+-----------------------+----------+---------------------
  kv         | NULL                  | /-8644906336239945703 |       55 |    390.805626 |            7 |                       | {2,3,7}  | {"","",""}
  kv         | /-8644906336239945703 | /-8069435259144101732 |       66 |    389.691597 |            7 |                       | {2,3,7}  | {"","",""}
  kv         | /-8069435259144101732 | /-7496722784489744891 |       63 |    387.804012 |            7 |                       | {2,3,7}  | {"","",""}
  kv         | /-7496722784489744891 | /-6917067466064761547 |       67 |    389.692066 |            7 |                       | {2,3,7}  | {"","",""}
  kv         | /-6917067466064761547 | /-6345214731536894017 |       61 |    387.901001 |            3 |                       | {2,3,7}  | {"","",""}
  kv         | /-6345214731536894017 | /-5768973510993151024 |       88 |    388.597722 |            3 |                       | {2,3,7}  | {"","",""}
  kv         | /-5768973510993151024 | /-5193738011353484949 |       85 |    388.533233 |            3 |                       | {2,3,7}  | {"","",""}
  kv         | /-5193738011353484949 | /-4617490555988151481 |       89 |    388.672154 |            3 |                       | {2,3,7}  | {"","",""}
  kv         | /-4617490555988151481 | /-4036745315811621515 |       58 |    391.854331 |            2 |                       | {2,3,7}  | {"","",""}
  kv         | /-4036745315811621515 | /-3452917408621695570 |       80 |    393.718129 |            2 |                       | {2,3,7}  | {"","",""}
  kv         | /-3452917408621695570 | /-2872759373245409765 |       77 |    391.686346 |            2 |                       | {2,3,7}  | {"","",""}
  kv         | /-2872759373245409765 | /-2289453959667452470 |       82 |    392.913568 |            2 |                       | {2,3,7}  | {"","",""}
  kv         | /-2289453959667452470 | /-1714384884657519217 |       75 |    387.967262 |            2 |                       | {2,3,7}  | {"","",""}
  kv         | /-1714384884657519217 | /-1139083832953030361 |       83 |    388.191021 |            2 |                       | {2,3,7}  | {"","",""}
  kv         | /-1139083832953030361 | /-566197846551287554  |       78 |    388.813476 |            2 |                       | {2,3,7}  | {"","",""}
  kv         | /-566197846551287554  | /12078714989006353    |       84 |    390.292272 |            2 |                       | {2,3,7}  | {"","",""}
  kv         | /12078714989006353    | /586892179000115549   |       56 |    387.762182 |            7 |                       | {2,3,7}  | {"","",""}
  kv         | /586892179000115549   | /1161385910102187330  |       68 |    389.766379 |            7 |                       | {2,3,7}  | {"","",""}
  kv         | /1161385910102187330  | /1734319184862864901  |       64 |    387.568861 |            7 |                       | {2,3,7}  | {"","",""}
  kv         | /1734319184862864901  | /2307300159150767034  |       69 |    387.097826 |            7 |                       | {2,3,7}  | {"","",""}
  kv         | /2307300159150767034  | /2876845921679339393  |       59 |    387.859396 |            7 |                       | {2,3,7}  | {"","",""}
  kv         | /2876845921679339393  | /3448510611752689972  |       70 |    389.075896 |            7 |                       | {2,3,7}  | {"","",""}
  kv         | /3448510611752689972  | /4030373006148413508  |       62 |    391.879301 |            7 |                       | {2,3,7}  | {"","",""}
  kv         | /4030373006148413508  | /4607975129262295186  |       65 |    390.736928 |            7 |                       | {2,3,7}  | {"","",""}
  kv         | /4607975129262295186  | /5191185650921251275  |       57 |    392.320298 |            2 |                       | {2,3,7}  | {"","",""}
  kv         | /5191185650921251275  | /5769855355138244213  |       79 |    391.159562 |            2 |                       | {2,3,7}  | {"","",""}
  kv         | /5769855355138244213  | /6349221757903677613  |       76 |    391.089277 |            2 |                       | {2,3,7}  | {"","",""}
  kv         | /6349221757903677613  | /6926818746854337714  |       81 |    390.233135 |            2 |                       | {2,3,7}  | {"","",""}
  kv         | /6926818746854337714  | /7495563559043730703  |       60 |     385.85205 |            3 |                       | {2,3,7}  | {"","",""}
  kv         | /7495563559043730703  | /8067630312680358853  |       90 |    387.064929 |            3 |                       | {2,3,7}  | {"","",""}
  kv         | /8067630312680358853  | /8645022156636441903  |       86 |    390.134479 |            3 |                       | {2,3,7}  | {"","",""}
  kv         | /8645022156636441903  | NULL                  |       87 |    390.868304 |            3 |                       | {2,3,7}  | {"","",""}
(32 rows)

Time: 107.598008ms

root@192.168.1.121:26257/defaultdb> select lease_holder,count(*) from [show ranges from database kv] group by lease_holder;
  lease_holder | count
---------------+--------
             7 |    12
             3 |     8
             2 |    12
(3 rows)
----

That's not bad.  Hopefully that'll still be true after a cold start overnight.

== 2020-09-23

Summary of the day:

* Shut down CRDB and took ZFS snapshots so that I could rollback the database state.
* Ran an extended read-write workload to settle into a new steady state (make any splits/merges happen).  Dug into various behavior (e.g., compactions).
* Run expansion experiment with new database.  This demonstrated a lot of pathologies that I didn't understand and got lost trying to dig into them.

Details:

* booted the three db VMs
* in all three db VMs: shutdown cockroachdb (stop method timed out)
* in all three db VMs: zfs snapshot
* in all three db VMs: start up cockroachdb again

Now, I want to start a reasonably heavy workload against all three.  I don't want to do so many writes that we're as swamped as when we built up the database.  I'll do this:

[source,text]
----
cockroach workload run kv --histograms kv-histograms-$(date +%FT%TZ).out --concurrency 4 --display-every=60s --read-percent 80 --tolerate-errors postgresql://root@192.168.1.223:26257/kv?sslmode=disable
----

This started around 14:10Z.  By 15:13Z, it's been fairly stable at about 800 reads / second and 200 writes / second, with p95 35ms and p99 just over 50ms.  The qps per store is reasonably well distributed, as is the qps by node.  Latencies match up between all three nodes.  There have been zero errors.  CPU utilization has been about 35-40% (and about the same for all six CPUs on the 3 db nodes).  Average I/O time ranges from about 3ms to 8ms on average.

That's all except for one excursion at 15:05 while I was debugging the system -- this may have been correlated with DTrace stopping one cockroach process to resolve stack traces?  That's the only thing I did that would cause a blip, but the blip affected all three nodes.  On the other hand, there was a big spike in average I/O time up to about 15ms on all nodes and a big spike in time queued in device driver on all nodes (up to about 5-6ms).  During this period, CPU utilization dipped on all nodes, but we did a bunch more total read throughput (spike from 40 MBps to 55 MBps.  Read IOPS actually went down.  Another thing I had done briefly around this time was `find /usr` -- I wonder if that could cause this, but only on one node.

I also note that there are small latency spikes every 5 minutes, and these appear correlated with big spikes in disk bytes written per second.  I wonder if ZFS is sync'ing fairly infrequently.

It's not that:

[source,text]
----
root@db1:~# dtrace -q -n 'BEGIN{ printf("tracing\n"); } spa_sync:entry,spa_sync:return{ printf("%Y %s:%s\n", walltimestamp, probefunc, probename); }'
tracing
2020 Sep 23 15:29:01 spa_sync:entry
2020 Sep 23 15:29:02 spa_sync:return
2020 Sep 23 15:29:06 spa_sync:entry
2020 Sep 23 15:29:07 spa_sync:return
2020 Sep 23 15:29:11 spa_sync:entry
2020 Sep 23 15:29:12 spa_sync:return
2020 Sep 23 15:29:16 spa_sync:entry
2020 Sep 23 15:29:17 spa_sync:return
2020 Sep 23 15:29:21 spa_sync:entry
2020 Sep 23 15:29:22 spa_sync:return
^C
----

I did spend some time digging into the system's performance, as the disks are nearly 100% busy.  Mostly I found that:

* Most of all disk I/Os are 128 KiB reads
* Most of the rest are small writes: 4096, then 8192, and other small multiples of 4096.
* Nearly all of the writes are coming from the taskq thread so it's a bit hard to see where they're really coming from.
* Cockroach is doing a fair number of `fdsync` calls to a file that I suspect is its WAL ("/cockroachdb/data/037460.log").
* The writes to that file vary in size from about 64 bytes to 4K, but they cluster around 512 bytes to 2K.

All of this seems pretty expected given:

* a workload whose write component involves small writes (<2 KiB writes)
* a system with no slog configured

I realized while writing this up that I did not specify the 1 KiB write size that I intended with the current workload.  However, that's kind of okay: I'm not doing perf testing here -- I just want there to be a substantial read/write load while I do these other experiments.

At 15:37Z there was another big event like the one mentioned above: crash in just about all metrics, spike in latency, and spike in bytes read from disk per second.  I was definitely not doing anything when this happened.  How can we capture this, plus the every-5-minute spikes?  Kind of want to trace volume of I/O by userland stack trace on a per-minute basis?

I'm going to take a break to get ready for the day.  When I resume:

* verify preconditions for the experiment
** qps per node distributed across all three nodes
** qps per store distributed across all three nodes
** %cpu, %busy, and disk IOPS and throughput are distributed across all three nodes
** throughput and latency are reasonably consistent

=== Every-5-minute latency spikes

See above for context.  These appear correlated with large increases in bytes written to disk (but not necessarily write IOPS).  It also seems correlated with compactions / flushes in Admin UI.

My tracing:

[source,text]
----
root@db1:~# dtrace -q -n 'BEGIN{ pcount = 0; }' -n 'fsinfo:::write/args[0]->fi_fs == "zfs"/{ @[execname, pid, probename, args[0]->fi_name, args[0]->fi_dirname] = sum(arg1); }' -n 'tick-1s/pcount++ % 60 == 0/{ printf("%Y\n", walltimestamp); printf("    %9s  %20s %-6s %-5s %s %s\n", "BYTES", "EXECNAME", "PID", "R/W", "FILE", "DIRECTORY"); trunc(@, 15); printa("    %@9d  %20s %-6d %-5s %s %s\n", @); trunc(@); }'
2020 Sep 23 17:58:17
        BYTES              EXECNAME PID    R/W   FILE DIRECTORY
          266             cockroach 552    write MANIFEST-037248 /cockroachdb/data
          754             cockroach 552    write cockroach.db1.cockroachdb.2020-09-23T14_05_45Z.000552.log /cockroachdb/data/logs
         1192             cockroach 552    write cockroach-pebble.db1.cockroachdb.2020-09-23T14_05_45Z.000552.log /cockroachdb/data/logs
        96616             cockroach 552    write 038260.log /cockroachdb/data
       471517             cockroach 552    write 038262.sst /cockroachdb/data
       476456             cockroach 552    write 038263.sst /cockroachdb/data
       518326             cockroach 552    write 038266.sst /cockroachdb/data
      1630429             cockroach 552    write 038261.sst /cockroachdb/data
      4248787             cockroach 552    write 038265.sst /cockroachdb/data
      4442674             cockroach 552    write 038264.sst /cockroachdb/data
2020 Sep 23 17:59:17
        BYTES              EXECNAME PID    R/W   FILE DIRECTORY
      3822300             cockroach 552    write 038278.sst /cockroachdb/data
      4229497             cockroach 552    write 038277.sst /cockroachdb/data
      4230425             cockroach 552    write 038280.sst /cockroachdb/data
      4231111             cockroach 552    write 038283.sst /cockroachdb/data
      4234233             cockroach 552    write 038274.sst /cockroachdb/data
      4235074             cockroach 552    write 038279.sst /cockroachdb/data
      4241833             cockroach 552    write 038282.sst /cockroachdb/data
      4245448             cockroach 552    write 038268.sst /cockroachdb/data
      4246357             cockroach 552    write 038276.sst /cockroachdb/data
      4248364             cockroach 552    write 038267.sst /cockroachdb/data
      4252136             cockroach 552    write 038275.sst /cockroachdb/data
      6412542             cockroach 552    write 038271.sst /cockroachdb/data
      8399522             cockroach 552    write 038272.sst /cockroachdb/data
      8410169             cockroach 552    write 038270.sst /cockroachdb/data
      8960741             cockroach 552    write 038260.log /cockroachdb/data
2020 Sep 23 18:00:17
        BYTES              EXECNAME PID    R/W   FILE DIRECTORY
          822      illumos-exporter 426    write application-illumos-exporter:default.log /var/svc/log
         5634             cockroach 552    write cockroach.db1.cockroachdb.2020-09-23T14_05_45Z.000552.log /cockroachdb/data/logs
      9144370             cockroach 552    write 038260.log /cockroachdb/data
2020 Sep 23 18:01:17
        BYTES              EXECNAME PID    R/W   FILE DIRECTORY
          822      illumos-exporter 426    write application-illumos-exporter:default.log /var/svc/log
         5902             cockroach 552    write cockroach.db1.cockroachdb.2020-09-23T14_05_45Z.000552.log /cockroachdb/data/logs
      9187226             cockroach 552    write 038260.log /cockroachdb/data
2020 Sep 23 18:02:17
        BYTES              EXECNAME PID    R/W   FILE DIRECTORY
          822      illumos-exporter 426    write application-illumos-exporter:default.log /var/svc/log
         5634             cockroach 552    write cockroach.db1.cockroachdb.2020-09-23T14_05_45Z.000552.log /cockroachdb/data/logs
       229825             cockroach 552    write memprof.000000000185769104_2020-09-23T18_02_16.235 /cockroachdb/data/logs/heap_profiler
      9012342             cockroach 552    write 038260.log /cockroachdb/data
2020 Sep 23 18:03:17
        BYTES              EXECNAME PID    R/W   FILE DIRECTORY
          130               syslogd 454    write messages /var/adm
          822      illumos-exporter 426    write application-illumos-exporter:default.log /var/svc/log
         5635             cockroach 552    write cockroach.db1.cockroachdb.2020-09-23T14_05_45Z.000552.log /cockroachdb/data/logs
      9236271             cockroach 552    write 038260.log /cockroachdb/data
2020 Sep 23 18:04:17
        BYTES              EXECNAME PID    R/W   FILE DIRECTORY
          395             cockroach 552    write cockroach-pebble.db1.cockroachdb.2020-09-23T14_05_45Z.000552.log /cockroachdb/data/logs
          822      illumos-exporter 426    write application-illumos-exporter:default.log /var/svc/log
         5904             cockroach 552    write cockroach.db1.cockroachdb.2020-09-23T14_05_45Z.000552.log /cockroachdb/data/logs
        35236             cockroach 552    write 038285.log /cockroachdb/data
      1357749             cockroach 552    write 038286.sst /cockroachdb/data
      9006463             cockroach 552    write 038260.log /cockroachdb/data
2020 Sep 23 18:05:17
        BYTES              EXECNAME PID    R/W   FILE DIRECTORY
      4232742             cockroach 552    write 038305.sst /cockroachdb/data
      4232877             cockroach 552    write 038295.sst /cockroachdb/data
      4234729             cockroach 552    write 038299.sst /cockroachdb/data
      4235717             cockroach 552    write 038301.sst /cockroachdb/data
      4241627             cockroach 552    write 038291.sst /cockroachdb/data
      4241918             cockroach 552    write 038304.sst /cockroachdb/data
      4244596             cockroach 552    write 038297.sst /cockroachdb/data
      4249726             cockroach 552    write 038296.sst /cockroachdb/data
      4250267             cockroach 552    write 038292.sst /cockroachdb/data
      4250663             cockroach 552    write 038290.sst /cockroachdb/data
      4251812             cockroach 552    write 038293.sst /cockroachdb/data
      4545094             cockroach 552    write 038289.sst /cockroachdb/data
      8406244             cockroach 552    write 038307.sst /cockroachdb/data
      8407968             cockroach 552    write 038308.sst /cockroachdb/data
      9071976             cockroach 552    write 038285.log /cockroachdb/data
2020 Sep 23 18:06:17
        BYTES              EXECNAME PID    R/W   FILE DIRECTORY
          822      illumos-exporter 426    write application-illumos-exporter:default.log /var/svc/log
         9147             cockroach 552    write cockroach.db1.cockroachdb.2020-09-23T14_05_45Z.000552.log /cockroachdb/data/logs
      9111146             cockroach 552    write 038285.log /cockroachdb/data
2020 Sep 23 18:07:17
        BYTES              EXECNAME PID    R/W   FILE DIRECTORY
          822      illumos-exporter 426    write application-illumos-exporter:default.log /var/svc/log
         5633             cockroach 552    write cockroach.db1.cockroachdb.2020-09-23T14_05_45Z.000552.log /cockroachdb/data/logs
      9131271             cockroach 552    write 038285.log /cockroachdb/data
2020 Sep 23 18:08:17
        BYTES              EXECNAME PID    R/W   FILE DIRECTORY
          822      illumos-exporter 426    write application-illumos-exporter:default.log /var/svc/log
         5633             cockroach 552    write cockroach.db1.cockroachdb.2020-09-23T14_05_45Z.000552.log /cockroachdb/data/logs
      9109181             cockroach 552    write 038285.log /cockroachdb/data
2020 Sep 23 18:09:17
        BYTES              EXECNAME PID    R/W   FILE DIRECTORY
          822      illumos-exporter 426    write application-illumos-exporter:default.log /var/svc/log
         5902             cockroach 552    write cockroach.db1.cockroachdb.2020-09-23T14_05_45Z.000552.log /cockroachdb/data/logs
      9174892             cockroach 552    write 038285.log /cockroachdb/data
2020 Sep 23 18:10:17
        BYTES              EXECNAME PID    R/W   FILE DIRECTORY
          822      illumos-exporter 426    write application-illumos-exporter:default.log /var/svc/log
         5634             cockroach 552    write cockroach.db1.cockroachdb.2020-09-23T14_05_45Z.000552.log /cockroachdb/data/logs
      9092377             cockroach 552    write 038285.log /cockroachdb/data
2020 Sep 23 18:11:17
        BYTES              EXECNAME PID    R/W   FILE DIRECTORY
      3735505             cockroach 552    write 038327.sst /cockroachdb/data
      4230352             cockroach 552    write 038329.sst /cockroachdb/data
      4230802             cockroach 552    write 038326.sst /cockroachdb/data
      4232601             cockroach 552    write 038319.sst /cockroachdb/data
      4235221             cockroach 552    write 038315.sst /cockroachdb/data
      4235270             cockroach 552    write 038323.sst /cockroachdb/data
      4240725             cockroach 552    write 038324.sst /cockroachdb/data
      4242037             cockroach 552    write 038320.sst /cockroachdb/data
      4242169             cockroach 552    write 038328.sst /cockroachdb/data
      4245301             cockroach 552    write 038321.sst /cockroachdb/data
      4246048             cockroach 552    write 038317.sst /cockroachdb/data
      4250534             cockroach 552    write 038314.sst /cockroachdb/data
      4258931             cockroach 552    write 038316.sst /cockroachdb/data
      4638051             cockroach 552    write 038313.sst /cockroachdb/data
      9205484             cockroach 552    write 038309.log /cockroachdb/data
2020 Sep 23 18:12:17
        BYTES              EXECNAME PID    R/W   FILE DIRECTORY
          822      illumos-exporter 426    write application-illumos-exporter:default.log /var/svc/log
         5634             cockroach 552    write cockroach.db1.cockroachdb.2020-09-23T14_05_45Z.000552.log /cockroachdb/data/logs
      9191817             cockroach 552    write 038309.log /cockroachdb/data
2020 Sep 23 18:13:17
        BYTES              EXECNAME PID    R/W   FILE DIRECTORY
          822      illumos-exporter 426    write application-illumos-exporter:default.log /var/svc/log
         5632             cockroach 552    write cockroach.db1.cockroachdb.2020-09-23T14_05_45Z.000552.log /cockroachdb/data/logs
      9156597             cockroach 552    write 038309.log /cockroachdb/data
2020 Sep 23 18:14:17
        BYTES              EXECNAME PID    R/W   FILE DIRECTORY
          822      illumos-exporter 426    write application-illumos-exporter:default.log /var/svc/log
         5903             cockroach 552    write cockroach.db1.cockroachdb.2020-09-23T14_05_45Z.000552.log /cockroachdb/data/logs
      8968980             cockroach 552    write 038309.log /cockroachdb/data
2020 Sep 23 18:15:17
        BYTES              EXECNAME PID    R/W   FILE DIRECTORY
          822      illumos-exporter 426    write application-illumos-exporter:default.log /var/svc/log
         5634             cockroach 552    write cockroach.db1.cockroachdb.2020-09-23T14_05_45Z.000552.log /cockroachdb/data/logs
      9117409             cockroach 552    write 038309.log /cockroachdb/data
2020 Sep 23 18:16:17
        BYTES              EXECNAME PID    R/W   FILE DIRECTORY
          129             cockroach 552    write cockroach-pebble.db1.cockroachdb.2020-09-23T14_05_45Z.000552.log /cockroachdb/data/logs
          822      illumos-exporter 426    write application-illumos-exporter:default.log /var/svc/log
         7957             cockroach 552    write cockroach.db1.cockroachdb.2020-09-23T14_05_45Z.000552.log /cockroachdb/data/logs
        24868             cockroach 552    write 038331.log /cockroachdb/data
      1240865             cockroach 552    write 038332.sst /cockroachdb/data
      9130242             cockroach 552    write 038309.log /cockroachdb/data
2020 Sep 23 18:17:17
        BYTES              EXECNAME PID    R/W   FILE DIRECTORY
      4240976             cockroach 552    write 038338.sst /cockroachdb/data
      4241661             cockroach 552    write 038345.sst /cockroachdb/data
      4241681             cockroach 552    write 038350.sst /cockroachdb/data
      4242437             cockroach 552    write 038353.sst /cockroachdb/data
      4245858             cockroach 552    write 038340.sst /cockroachdb/data
      4250058             cockroach 552    write 038337.sst /cockroachdb/data
      4252294             cockroach 552    write 038339.sst /cockroachdb/data
      4256064             cockroach 552    write 038344.sst /cockroachdb/data
      4625014             cockroach 552    write 038359.sst /cockroachdb/data
      4712037             cockroach 552    write 038336.sst /cockroachdb/data
      8758259             cockroach 552    write 038331.log /cockroachdb/data
      8954597             cockroach 552    write 038348.sst /cockroachdb/data
     16784652             cockroach 552    write 038358.sst /cockroachdb/data
     16789817             cockroach 552    write 038357.sst /cockroachdb/data
     20172971             cockroach 552    write 038360.sst /cockroachdb/data
2020 Sep 23 18:18:17
        BYTES              EXECNAME PID    R/W   FILE DIRECTORY
          822      illumos-exporter 426    write application-illumos-exporter:default.log /var/svc/log
         5905             cockroach 552    write cockroach.db1.cockroachdb.2020-09-23T14_05_45Z.000552.log /cockroachdb/data/logs
      8998121             cockroach 552    write 038331.log /cockroachdb/data

----

Summary of spikes in write I/O from my tracing:

[source,text]
----
17:58 - 17:59
18:05
18:11
18:17
----

Summary of spikes in compactions **on n7** (db1):

[source,text]
----
17:58:30
18:04:30
18:10:30
18:16:30
----

Summary of spikes in disk bytes written:

[source,text]
----
17:58:30
18:04:30
18:10:30
18:16:30
----

My tracing shows spikes in bytes written to the sst files at the same time as spikes in bytes written to disk, at the same time as compactions/flushes.  Tracing where these are coming from:

[source,text]
----
root@db1:~# dtrace -q -n 'BEGIN{ pcount = 0; }' -n 'fsinfo:::write/args[0]->fi_fs == "zfs" && execname == "cockroach" && args[0]->fi_dirname == "/cockroachdb/data"/{ @[args[0]->fi_name, ustack()] = sum(arg1); }' -n 'tick-1s/pcount++ % 10 == 0/{ printf("%Y\n", walltimestamp); printa(@); trunc(@); }'
2020 Sep 23 18:30:00

  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushBlock+0x77
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x9d
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
             1300
  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x215
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
            98598
2020 Sep 23 18:30:10

  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushBlock+0x77
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x9d
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           466606
  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x215
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
          1001777
2020 Sep 23 18:30:20

  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushBlock+0x77
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x9d
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           490221
  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x215
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
          1085940
2020 Sep 23 18:30:30

  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushBlock+0x77
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x9d
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           545091
  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x215
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
          1011152
2020 Sep 23 18:30:40

  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushBlock+0x77
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x9d
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           499791
  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x215
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           958154
2020 Sep 23 18:30:50

  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushBlock+0x77
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x9d
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           510696
  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x215
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
          1068227
2020 Sep 23 18:31:00

  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushBlock+0x77
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x9d
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           505482
  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x215
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
          1045419
2020 Sep 23 18:31:10

  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushBlock+0x77
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x9d
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           523152
  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x215
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           963423
2020 Sep 23 18:31:20

  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushBlock+0x77
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x9d
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           570946
  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x215
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           981128
2020 Sep 23 18:31:30

  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushBlock+0x77
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x9d
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           543966
  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x215
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           970835
2020 Sep 23 18:31:40

  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushBlock+0x77
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x9d
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           504532
  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x215
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
          1031607
2020 Sep 23 18:31:50

  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushBlock+0x77
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x9d
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           541405
  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x215
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
          1005244
2020 Sep 23 18:32:00

  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushBlock+0x77
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x9d
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           484510
  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x215
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
          1019241
2020 Sep 23 18:32:10

  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushBlock+0x77
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x9d
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           465532
  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x215
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           928871
2020 Sep 23 18:32:20

  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushBlock+0x77
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x9d
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           540366
  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x215
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           865638
2020 Sep 23 18:32:30

  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushBlock+0x77
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x9d
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           506049
  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x215
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           776038
2020 Sep 23 18:32:40

  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushBlock+0x77
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x9d
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           549427
  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x215
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           975391
2020 Sep 23 18:32:50

  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushBlock+0x77
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x9d
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           490999
  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x215
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
          1034167
2020 Sep 23 18:33:00

  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushBlock+0x77
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x9d
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           525451
  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x215
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
          1039681
2020 Sep 23 18:33:10

  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushBlock+0x77
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x9d
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           489763
  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x215
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
          1026542
2020 Sep 23 18:33:20

  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushBlock+0x77
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x9d
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           509978
  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x215
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           969318
2020 Sep 23 18:33:30

  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushBlock+0x77
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x9d
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           585123
  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x215
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           954583
2020 Sep 23 18:33:40

  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushBlock+0x77
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x9d
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           570240
  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x215
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
          1041078
2020 Sep 23 18:33:50

  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushBlock+0x77
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x9d
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           522892
  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x215
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           978804
2020 Sep 23 18:34:00

  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushBlock+0x77
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x9d
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           502441
  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x215
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
          1091489
2020 Sep 23 18:34:10

  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushBlock+0x77
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x9d
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           504292
  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x215
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
          1041930
2020 Sep 23 18:34:20

  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushBlock+0x77
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x9d
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           544372
  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x215
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           980280
2020 Sep 23 18:34:30

  038415.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xa64
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
              162
  038407.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xa64
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).flush1+0x2af
              cockroach`github.com/cockroachdb/pebble.(*DB).flush.func1+0x77
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).flush+0x89
              cockroach`runtime.goexit+0x1
              248
  MANIFEST-037248
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/internal/record.(*Writer).writePending+0x98
              cockroach`github.com/cockroachdb/pebble/internal/record.(*Writer).Flush+0x33
              cockroach`github.com/cockroachdb/pebble.(*versionSet).logAndApply.func1+0x2dc
              cockroach`github.com/cockroachdb/pebble.(*versionSet).logAndApply+0x19f
              cockroach`github.com/cockroachdb/pebble.(*DB).flush1+0x745
              cockroach`github.com/cockroachdb/pebble.(*DB).flush.func1+0x77
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).flush+0x89
              cockroach`runtime.goexit+0x1
              265
  038425.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xa64
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
              885
  038416.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xa64
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
              889
  038424.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xa64
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
              933
  038431.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xa64
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
              941
  038432.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xa64
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
              941
  038433.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xa64
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
              941
  038428.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xa64
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
              942
  038429.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xa64
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
              942
  038413.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xa64
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
              967
  038411.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xa64
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             1026
  038419.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xa64
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             1129
  038420.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xa64
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             1137
  038418.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xa64
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             1144
  038426.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xa64
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             1151
  038421.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xa64
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             1152
  038417.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xa64
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             1189
  038414.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xa64
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             1198
  038412.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xa64
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             1279
  038423.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xa64
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             1317
  038409.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xa64
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).flush1+0x2af
              cockroach`github.com/cockroachdb/pebble.(*DB).flush.func1+0x77
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).flush+0x89
              cockroach`runtime.goexit+0x1
             1372
  038408.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xa64
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).flush1+0x2af
              cockroach`github.com/cockroachdb/pebble.(*DB).flush.func1+0x77
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).flush+0x89
              cockroach`runtime.goexit+0x1
             1567
  038427.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xa64
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             2034
  MANIFEST-037248
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/internal/record.(*Writer).writePending+0x98
              cockroach`github.com/cockroachdb/pebble/internal/record.(*Writer).Flush+0x33
              cockroach`github.com/cockroachdb/pebble.(*versionSet).logAndApply.func1+0x2dc
              cockroach`github.com/cockroachdb/pebble.(*versionSet).logAndApply+0x19f
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x6aa
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             2278
  038410.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xa64
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             2516
  038430.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xa64
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             3359
  038422.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xa64
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             3703
  038407.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x50d
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).flush1+0x2af
              cockroach`github.com/cockroachdb/pebble.(*DB).flush.func1+0x77
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).flush+0x89
              cockroach`runtime.goexit+0x1
             4096
  038407.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).flush1+0x2af
              cockroach`github.com/cockroachdb/pebble.(*DB).flush.func1+0x77
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).flush+0x89
              cockroach`runtime.goexit+0x1
             4096
  038407.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).flush1+0x2af
              cockroach`github.com/cockroachdb/pebble.(*DB).flush.func1+0x77
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).flush+0x89
              cockroach`runtime.goexit+0x1
             4096
  038408.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).flush1+0x2af
              cockroach`github.com/cockroachdb/pebble.(*DB).flush.func1+0x77
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).flush+0x89
              cockroach`runtime.goexit+0x1
             4096
  038408.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).flush1+0x2af
              cockroach`github.com/cockroachdb/pebble.(*DB).flush.func1+0x77
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).flush+0x89
              cockroach`runtime.goexit+0x1
             4096
  038409.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).flush1+0x2af
              cockroach`github.com/cockroachdb/pebble.(*DB).flush.func1+0x77
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).flush+0x89
              cockroach`runtime.goexit+0x1
             4096
  038410.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038410.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038410.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xee3
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038411.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038411.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038411.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xee3
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038412.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038412.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038412.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xee3
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038413.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038413.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038413.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xee3
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038414.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038414.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xee3
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038415.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x50d
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038415.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038415.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038416.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x50d
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038416.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038416.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038417.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038417.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038417.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xee3
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038418.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038418.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038418.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xee3
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038419.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038419.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038419.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xee3
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038420.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038420.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038420.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xee3
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038421.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038421.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xee3
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038422.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038423.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038423.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038423.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xee3
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038424.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x50d
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038424.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038424.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038425.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x50d
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038425.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038425.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038426.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038426.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038426.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xee3
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038428.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038428.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038428.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xee3
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038429.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038429.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038429.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xee3
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038430.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038430.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038430.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xee3
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038431.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038431.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038431.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xee3
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038432.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038432.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038432.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xee3
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038433.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038433.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038433.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xee3
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4096
  038429.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xee3
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4370
  038428.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xee3
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4416
  038431.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             4582
  038431.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             5322
  038408.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).flush1+0x2af
              cockroach`github.com/cockroachdb/pebble.(*DB).flush.func1+0x77
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).flush+0x89
              cockroach`runtime.goexit+0x1
             5450
  038409.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).flush1+0x2af
              cockroach`github.com/cockroachdb/pebble.(*DB).flush.func1+0x77
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).flush+0x89
              cockroach`runtime.goexit+0x1
             5852
  038411.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             5886
  038423.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             6027
  038420.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             6664
  038408.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).flush1+0x2af
              cockroach`github.com/cockroachdb/pebble.(*DB).flush.func1+0x77
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).flush+0x89
              cockroach`runtime.goexit+0x1
             8243
  038432.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             8394
  038433.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             8615
  038418.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             9751
  038430.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             9930
  038413.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
             9995
  038426.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
            10318
  038424.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
            13032
  038432.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xee3
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
            13086
  038431.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xee3
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
            13658
  038419.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
            13660
  038433.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xee3
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
            15058
  038410.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
            16509
  038430.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
            18124
  038422.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
            20480
  038417.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
            20711
  038416.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
            21191
  038432.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
            21645
  038425.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
            21829
  038407.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).flush1+0x2af
              cockroach`github.com/cockroachdb/pebble.(*DB).flush.func1+0x77
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).flush+0x89
              cockroach`runtime.goexit+0x1
            22258
  038428.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
            22413
  038415.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
            23269
  038429.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
            25031
  038415.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
            26826
  038422.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0xdb
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
            28639
  038425.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
            30666
  038419.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
            37066
  038421.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
            37530
  038416.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
            40522
  038411.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
            42570
  038424.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
            43466
  038426.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
            45258
  038423.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
            45898
  038418.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
            49098
  038409.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).flush1+0x2af
              cockroach`github.com/cockroachdb/pebble.(*DB).flush.func1+0x77
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).flush+0x89
              cockroach`runtime.goexit+0x1
            49152
  038413.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
            50634
  038420.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
            50890
  038417.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
            55370
  038412.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
            56114
  038414.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
            56236
  038408.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).flush1+0x2af
              cockroach`github.com/cockroachdb/pebble.(*DB).flush.func1+0x77
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).flush+0x89
              cockroach`runtime.goexit+0x1
            77824
  038407.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).flush1+0x2af
              cockroach`github.com/cockroachdb/pebble.(*DB).flush.func1+0x77
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).flush+0x89
              cockroach`runtime.goexit+0x1
           125002
  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushBlock+0x77
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x9d
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           154665
  038410.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Close+0x102a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction.func4+0x28a
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x1543
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
           157386
  038422.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
           174681
  038382.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x215
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           179186
  038434.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
           196608
  038409.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).flush1+0x2af
              cockroach`github.com/cockroachdb/pebble.(*DB).flush.func1+0x77
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).flush+0x89
              cockroach`runtime.goexit+0x1
           309693
  038406.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushBlock+0x77
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x9d
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           316394
  038415.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
           405504
  038407.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).flush1+0x2af
              cockroach`github.com/cockroachdb/pebble.(*DB).flush.func1+0x77
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).flush+0x89
              cockroach`runtime.goexit+0x1
           454656
  038408.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).flush1+0x2af
              cockroach`github.com/cockroachdb/pebble.(*DB).flush.func1+0x77
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).flush+0x89
              cockroach`runtime.goexit+0x1
           474650
  038425.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
           520192
  038416.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
           524288
  038419.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
           532480
  038421.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
           532480
  038424.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
           532480
  038411.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
           544768
  038420.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
           548864
  038426.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
           548864
  038423.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
           552960
  038413.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
           557056
  038414.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
           561152
  038417.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
           561152
  038418.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
           561152
  038412.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
           573440
  038406.log
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushPending+0x215
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop+0x2a1
              cockroach`github.com/cockroachdb/pebble/internal/record.(*LogWriter).flushLoop-fm+0x3e
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble/internal/record.NewLogWriter.func2+0x89
              cockroach`runtime.goexit+0x1
           701532
  038410.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
           765952
  038430.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
           815104
  038428.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
          1056768
  038429.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
          1056768
  038434.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
          1404625
  038407.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).flush1+0x2af
              cockroach`github.com/cockroachdb/pebble.(*DB).flush.func1+0x77
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).flush+0x89
              cockroach`runtime.goexit+0x1
          1412090
  038432.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
          2117632
  038431.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
          2134016
  038433.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Flush+0x75
              cockroach`bufio.(*Writer).Write+0xfa
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
          2150400
  038415.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
          2844576
  038410.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
          3396339
  038417.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
          3602903
  038412.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
          3610970
  038418.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
          3611794
  038413.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
          3617925
  038414.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
          3624025
  038426.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
          3625666
  038423.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
          3626298
  038420.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
          3629305
  038411.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
          3634334
  038419.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
          3636229
  038424.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
          3639580
  038416.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
          3639688
  038425.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
          3644066
  038421.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
          3654822
  038430.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
          5632396
  038429.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
          7315395
  038428.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
          7315856
  038433.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
         14586547
  038432.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
         14612799
  038431.sst
              libc.so.1`__write+0xa
              cockroach`runtime.asmsysvicall6+0x5a
              cockroach`syscall.write+0x90
              cockroach`internal/poll.(*FD).Write+0x179
              cockroach`os.(*File).Write+0x77
              cockroach`github.com/cockroachdb/pebble/vfs.(*syncingFile).Write+0x64
              cockroach`bufio.(*Writer).Write+0x13b
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).writeBlock+0x144
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).maybeFlush+0xde
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).addPoint+0xa8
              cockroach`github.com/cockroachdb/pebble/sstable.(*Writer).Add+0x114
              cockroach`github.com/cockroachdb/pebble.(*DB).runCompaction+0x13ed
              cockroach`github.com/cockroachdb/pebble.(*DB).compact1+0x1ad
              cockroach`github.com/cockroachdb/pebble.(*DB).compact.func1+0x8b
              cockroach`runtime/pprof.Do+0xcc
              cockroach`github.com/cockroachdb/pebble.(*DB).compact+0xa2
              cockroach`runtime.goexit+0x1
         14613595

----

I kept all the data for context, but we can ignore the writes to the log files for now.  The writes to all the sst files at "2020 Sep 23 18:34:00" correlate with a compaction+flush, and they're coming from `flushLoop`.  I think that's as far as I want to go with this right now.

=== Online expansion experiment

Verifying preconditions:

* qps per node distributed across all three nodes
* qps per store distributed across all three nodes
* %cpu, %busy, and disk IOPS and throughput are distributed across all three nodes
* throughput and latency are reasonably consistent

[source,text]
----
19:55Z boot the remaining three database VMs
20:02Z verified above preconditions.  Of note, db3's %busy is quite high and its average I/O latency is like 3x higher than the others.  It's reading more bytes per second, though.  Maybe that's why?  I will check ranges.  They're pretty balanced:

    root@loadgen0:~# cockroach sql --host 192.168.1.24 --execute 'select lease_holder, count(*) from [show ranges from database kv] group by lease_holder'
      lease_holder | count
    ---------------+--------
                 7 |    11
                 3 |    11
                 2 |    10
    (3 rows)
    
    Time: 112.547288ms

       Of course it's possible that one of those is particularly busy right now for some reason.  This started around 18:34Z.  I'm not sure what else that correlates with.  I'm going to proceed anyway.
20:15Z start cockroachdb on db4 (became n8).  This didn't go that well.
20:25Z ranges aren't what I expect

    root@loadgen0:~# cockroach sql --host 192.168.1.24 --execute 'select lease_holder, count(*) from [show ranges from database kv] group by lease_holder'
      lease_holder | count
    ---------------+--------
                 2 |    14
                 3 |     9
                 8 |     9
    (3 rows)
    
    Time: 1.951390717s

      Where's node 7?  Ironically, store 7 is getting all the load right now.  Did it go suspect?  No -- the live node count never went down.  Also, the queries per second by store is supposed to be a moving average -- how does it have such a sharp increase?

20:29Z ranges more reasonable now

      root@loadgen0:~# cockroach sql --host 192.168.1.24 --execute 'select lease_holder, count(*) from [show ranges from database kv] group by lease_holder'
        lease_holder | count
      ---------------+--------
                   8 |    12
                   2 |     9
                   7 |     6
                   3 |     5
      (4 rows)
      
      Time: 396.930654ms

However, p95/p99 and throughput have not even come close to what they were before.  Previously about 925 total qps, now about 450.  CPU utilization fluctuated a lot, and is now quite unsteady, but plenty of headroom there.  A number of hosts are now frequently hitting 100% busy on the disks.  Average I/O time is also a lot higher.  They're all doing a lot more bytes written than they were before this.  Bytes read -- not so much, and actually fewer write IOPS.  The p99 RTT hit 3.6s on some hosts.

20:33Z Performance remains very uneven.  On the plus side, I haven't seen any actual errors up to this point.

20:33Z The erratic behavior appears to stabilize.  Performance is still poor, but getting steadily better.  That's about the point where replicas per node evened out across the cluster and leaseholders per node stopped changing too.  This is when we stopped with range operations.

20:48Z After just over half an hour, we've reached about the same performance we had before expanding the cluster.  Note that queries served per node has been about the same (across nodes, not over time) the whole time.  By 21:04Z, the cluster is doing about 1400 qps -- quite a bit more than the 950 it was doing before.  It's starting to level off.

----

To summarize the impact, I would say that there were a few periods of note:

[source,text]
----
20:15Z to 20:20Z: throughput dropped all the way to zero
20:20Z to 20:33Z: throughput poor and very erratic
20:33Z to 20:48Z: throughput climbing steadily, but less than before
20:48Z to 21:02Z: throughput climbing steadily, and better than before
21:02Z and after: throughput mostly steady, except for some big crashes at 21:08Z, 21:12, 21:14, 21:24, 21:34, and 21:52.
----

I'm very confused about some of the performance during this period.  Let's take a look at about 20:36Z.  At this point, db3's disk is seeing an average latency of 24ms and declining to about 10ms over the next 20 minutes or so.  That's way more than the https://aws.amazon.com/ebs/faqs/["single-digit millisecond latencies"] that Amazon says.  Is it a problem on our side?  I don't think so: the EC2 graphs of average read and write latency show similar shapes during this period, and also peaking around 20-25ms and declining to 10-15ms.  Why is it higher than before?  The queue length has also gone up -- in fact, the latency graph basically tracks the queue length graph pretty well.

Another thing that's strange about this: read throughput was steadily about 1,000 IOPS for a while, with write throughput about 100 IOPS.  But these volumes are provisioned with 500 IOPS.  What gives?

EBS/CloudWatch references:
* https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using_cloudwatch_ebs.html#ebs-volume-metrics[metric descriptions]
* https://aws.amazon.com/ebs/faqs/[EBS performance expectations]

Regarding the big crashes in throughput: these correlate with spikes of disk activity to 100% busy on db2 and db1, plus spikes in average I/O time on those hosts at those times.  These correlate with _crashes_ in write IOPS and spikes in read IOPS.  There are drops in CPU usage and network throughput.  I don't see any range operations during this time, nor internal CockroachDB errors, nor significant heartbeat latency.  These _are_ correlated with active flows for distributed SQL queries.  I did not find much else in the Cockroach Admin UI to explain this.  There were no additional SQL connections to the nodes, so it's not clear why some queries would be suddenly so expensive.

This all feels like a big mess.  Here are a bunch of open questions.  At the top level:

* Why the crash in performance for 5 full minutes?
* Why the erratic performance while rebalancing?
* Why the slow ramp-up in performance afterwards?
* Why did performance wind up better than it was before?
* Why the occasional crashes after the fact?

Lower level questions related to these:

* Why the sudden skyrocket in queries per second served by store 7 right after 8 came online?  How is that even possible -- it's way more queries than we were doing?  And there were no ranges at all on 7 for a few minutes there?  Why was that?
* Why is the AWS disk latency so high sometimes?  (Seems to track with queue depth, so maybe that's enough explanation?)
* Why are I/Os queued in the driver for so long?  Is that because we'd reached at max at the disk?
* Why was db3 so busy before the expansion at 20:15Z?
* Why did the heartbeat latency spike so much during that period?  Ditto RTT outliers (but not p50 RTT)?  Did things have trouble getting on-CPU?

Some things to check:

* Recommended hardware configurations
** wrong: 4 GiB DRAM per vCPU (we only have <4 GiB total for 2 vCPUs)
** wrong: 60 GiB storage per vCPU (but we're not using all that)
** wrong: 500 IOPS per vCPU (we have 500 IOPS for both vCPUs)
** okay: 30 MBps disk I/O per vCPU (we have plenty)
* Note: we're running a "master" build, using Pebble, on an unsupported OS

23:09Z: updated instance types of db nodes to "c4.xlarge" and bumped up to 1000 provisioned IOPS.  That's not right though because we bumped the vCPU count as well.  Sigh.  We really want: 2 vCPUs, 8 GiB of memory, and 1000 IOPS.  We just want more memory than we had.  Maybe I want m4.large?

23:23Z: updated instance types of db nodes to "m4.large".

NOTE: when changing the instance type, the Instances are rebooted!

23:29Z: finally shut down the load generator programs.  I learned from this that the "errors" column is cumulative, and we do see errors when nodes are offline.  There had been no errors up to that point.

Next steps:

* note: I might well be running into EBS bandwidth limits: it was 500 Mbps or 62.5 MBps.  I don't know if that number is in each direction or what.  If it's combined in + out, then I was totally hitting that.  If not, I could still be hitting that since the limit depends on using 128 KiB I/Os.  There's https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-optimized.html[more info here].  On the other hand, the disks weren't 100% busy all the time...although nor was performance terrible all the time?  You know, though, the slow increase starting around 22:35 does look a bit like a token bucket being replenished.
* Retry the experiment with bigger hardware?  Make sure next time we're not starting at 100% busy disks?
* Add some sort of tracing to identify cases where a Goroutine (or thread) has been off-CPU for a lot longer than we'd expect?  Maybe trace wakeup stacks when a thread has been off-CPU for over 1s?
* Try to articulate why I have such a hard time understanding I/O problems and what data would help explain it better

== 2020-09-24

Plan for the day:

* Rerun online expansion experiment now that I'm on larger instance type
* Also add tracing for long off-CPU times

Tracing for long off-CPU times:

[source,text]
----
dtrace -q -n 'BEGIN{ pcount = 0; printf("tracing\n"); }' -n 'sched:::off-cpu/args[1]->pr_fname == "cockroach"/{ timeoff[args[1]->pr_pid, args[0]->pr_lwpid] = timestamp; }' -n 'sched:::on-cpu/timeoff[curpsinfo->pr_pid, curlwpsinfo->pr_lwpid] && timestamp - timeoff[curpsinfo->pr_pid, curlwpsinfo->pr_lwpid] > 1000000000/{ @[ustack()] = quantize(timestamp - timeoff[curpsinfo->pr_pid, curlwpsinfo->pr_lwpid]); timeoff[curpsinfo->pr_pid, curlwpsinfo->pr_lwpid] = 0; }' -n 'tick-1s/pcount++ % 60 == 0/{ printf("%Y\n", walltimestamp); printa(@); trunc(@); }'
----

16:13Z: Running this workload x3 (nodes db1, db2, db3):

[source,text]
----
cockroach workload run kv --histograms kv-histograms-$(date +%FT%TZ).out --concurrency 4 --display-every=60s --read-percent 80 --tolerate-errors postgresql://root@192.168.1.14:26257/kv?sslmode=disable
----

18:09Z: Checking preconditions:

* disks %busy: reasonably balanced around 25-30% on the four active nodes.
* CPU: rather higher than expected: 90% on db2, 87% on db3, 80% on db1, 60% on db4.  This may be worth looking at a bit.
* Throughput per node is close to the same among the three nodes, but further apart than I've seen before.  Busiest: db3, then db1, then db2.  Total throughput is about 2000 selects / second and 500 inserts / second.
* Queries per second by store isn't exactly balanced (ranges from about 500 to 750 per store), but there's a good amount on each at least.  I/O is reliably under 1ms on all nodes.
* Replicas per node: pretty balanced.
* Leaseholders per node: okay, not great: ranges from 12-22.  Most on db2, then db4 and db3, then db1.  There was a bunch of range activity around 16:00 - 16:10Z.  The cluster looks healthy in terms of internal errors, heartbeat latency, bad ranges, round trip latency, etc.

Digging into db2 CPU utilization:

* It's serving the fewer number of queries right now.
* Its store is tied for serving the most queries.
* Its disks are among the least busy.  It's in the middle of the road for write IOPS, read IOPS, bytes written.  Not much outstanding I/O.  It is the one with 22 leaseholders, compared with 16, 16, and 12 on the other nodes.

I discovered while doing this that there's an Advanced Debug screen for "hot ranges", where you can get them by node.  It'll even show you qps.  The hottest 1-2 ranges on node 2 are ranges 55 and 66.  I may be able to move these by "manually enqueue[ing] range in a replica queue", but I can't find much information on what this does so I'm going to skip it for now.

I did run:

[source,text]
----
cockroach sql --host 192.168.1.24 --execute 'SET CLUSTER SETTING server.remote_debugging.mode = "any"'
----

to access the allocator debug table for these ranges.  For range 55, Cockroach says "consider rebalance" but "no suitable rebalance target".  I'm not sure why n7 isn't a good target.  That node is in the middle of the pack for replica count and lower than most for lease count.

There's an open question of why Cockroach isn't rebalancing here, but I don't think there's much I can do as a standard user.  Let's see what happens if we bring up a new node.  That said, there's definitely saturation on db2: lots of LAT for cockroachdb and headroom is single-digit percentages.

Okay, I did try to manually enqueue it after all.  It failed:

[source,text]
----
2020-09-24 18:34:24

kv/kvserver/store.go:2679 [n2,s2,r55/3:/Table/55{-/1/-8644…}] running replicate.shouldQueue

2020-09-24 18:34:24

kv/kvserver/replicate_queue.go:241 [n2,s2,r55/3:/Table/55{-/1/-8644…}] no rebalance target found, not enqueuing

2020-09-24 18:34:24

kv/kvserver/allocator.go:945 [n2,s2,r55/3:/Table/55{-/1/-8644…}] ShouldTransferLease (lease-holder=2):
  candidate: avg-ranges=49.5 avg-leases=16.5 avg-disk-usage=9.1 GiB avg-queries-per-second=639.5519724859548
  8: ranges=50 leases=16 disk-usage=10 GiB queries-per-second=638.80
  7: ranges=50 leases=12 disk-usage=8.4 GiB queries-per-second=484.52
  2: ranges=51 leases=22 disk-usage=9.1 GiB queries-per-second=718.15
  3: ranges=47 leases=16 disk-usage=8.7 GiB queries-per-second=716.74

2020-09-24 18:34:24

kv/kvserver/allocator.go:968 [n2,s2,r55/3:/Table/55{-/1/-8644…}] ShouldTransferLease decision (lease-holder=2): false

2020-09-24 18:34:24

kv/kvserver/store.go:2681 [n2,s2,r55/3:/Table/55{-/1/-8644…}] shouldQueue=false, priority=0.000000
----

I do have misgivings about doing this with so little headroom on this box, but I don't see a way to repair that.

18:38Z: started cockroachdb on db5

We had a similar problem as yesterday, but not so bad.  Throughput plummeted and we immediately saw p95+p99 outliers exceeding 10 seconds.  Throughput was basically from 18:40 to 18:43.  It wasn't until about 19:00 that we were back to the level of throughput we were at before the change.  However, p95 was back to about 50-80ms by about 18:43.

There were apparently two errors around 19:23 and 19:24.

QPS per node stayed about balanced among the three active nodes the whole time.  QPS per store looks pretty balanced, including the new node, after the update.

During the heavy rebalancing period from 18:38 through about 18:55Z, CPU utilization per node fluctuated wildly but largely had some headroom -- even on db2.  CPU utilization went close to where it was before after that.

Disks were definitely a lot busier during the rebalance period and up through a little after 19:00Z for db2.  db5 (the new node) got very busy for most of the rebalance period.  db2 got busy after db5 settled down.  The others had headroom most of that time.

During that heavy period, about 18:38 to 18:50, average I/O time was high on db5 and higher than normal on the others.  This correlates with spikes in disk bytes written and read per second, a _decrease_ in write IOPS, a big increase in read IOPS, more I/Os outstanding to the device, and more time queued in the device driver, and lots of network traffic.  Most of this is as we'd expect.  Leaseholders per node became more balanced after all this, ranging from 11 to 16 (previously 12 to 22).

p99 heartbeat latency did see some spikes in the several-second range, up to 4.5 seconds.  GCs got as high as 150ms, which is as high as I've ever seen them.  Round-trip latency spike: p95 to 3s, p99 to over 5s.  According to my tracing, no CockroachDB thread had been off-CPU for as much as a full second at a time at any point on any node.

=== Another round

I'm going to try this experiment one more time, and I'll change up my tracing methodology: this time I'm going to watch for any syscall that takes over 1s (regardless of whether we were off-CPU for that time).

This is a little underbaked, but here goes:

[source,text]
----
dtrace -q -n 'BEGIN{ pcount = 0; printf("tracing\n"); }' -n 'syscall:::entry/execname == "cockroach"/{ self->start = timestamp; }' -n 'syscall:::return/self->start && timestamp - self->start > 1000000000/{ @[ustack()] = quantize(timestamp - self->start); @all[ustack()] = quantize(timestamp - self->start); @avgs[probefunc] = avg(timestamp - self->start); }' -n 'syscall:::return{ self->start = 0; }' -n 'tick-60s/pcount++ % 1 == 0/{ printf("%Y\n", walltimestamp); printa(@); trunc(@); }' -n 'END{ printf("ALL\n"); printa(@all); printa(@avgs); }'
----

19:43Z: starting cockroachdb on db6.  Pretty similar story -- took almost 20 minutes to get back to full throughput.  One big difference though is that the p95 only spiked to 500ms, but the p99 was still over 8s.

My tracing didn't show a whole lot.  Nearly everything that blocked for that long was in lwp_park.  There was also fdsync on db4, with an average time of just over 1s, and write on db1, with an average time of 2.6s (these are averages for calls taking over 1s).

Things have been pretty stable since about 20:00Z.  I'll wait until about 21:00 and then start shrinking the cluster again.

22:00Z: the cluster has been fairly stable, albeit with some latency spikes at 21:21, 21:31, and 21:48 or so (and associated throughput dips).  %busy is steady at 20-32% on all nodes.  Average I/O time is well under 1ms.  The cluster looks fairly balanced, although leaseholders vary as 7-13 per node.  Heartbeat latency is consistently under 80ms.  GC under 1ms.  Throughput is about 2700 selects per second and 670 inserts per second.  p95 is about 6ms, p99 is about 10ms.  Queries per store fall into two modes: ~620qps (four nodes) and ~460ps (two nodes).  CPU utilization ranges across the cluster, but there's mostly headroom everywhere.  (Not a ton on a few nodes -- several are about 75%+.)

I'm going to start decommissioning nodes.  I'll decommission n10 (.248), n9 (.9), and n8 (.13) -- the reverse order that I just added them.  I will not be changing the load generators.

22:05Z begin decommissioning n10.  This officially took 5 minutes.
22:10Z decommissioning n10 completes (it's still running and reports as "decommissioning" until I actually disable it and it's been gone for 5 minutes.)

This looked a lot like adding a new node. Throughput crashed, although not quite to zero.  After about 20 minutes, it was close to recovered and more stable.  The bump in p95 and p99 was much smaller -- p99 peaked at around 350ms.  Disks got quite busy on a lot of systems, and there were lots of bytes read and written.

22:28Z: disable cockroachdb on n10.  As we'd expect, this had no effect.
22:39Z: start decommissioning n9.  Took 4m47s.


Impact: very similar to the last one, with slower recovery.

23:02Z: disable cockroachdb on n9.  No impact, as we'd expect.
23:12Z: begin decommissioning n8.  This one took 7m41s and had a similar impact as the others.
23:25Z: disable cockroachdb on n8.

I let this run until about 2020-09-25T03:55Z (about 4.5 more hours) and took Grafana screenshots for the whole day.  Of note, we got a total of three errors from the `cockroach workload` runs.  The one pointed at .24 (db1) reported:

[source,text]
----
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
38457.9s        0          702.2          670.5      3.4      7.6     12.6     92.3 read
38457.9s        0          173.0          167.6      7.3     13.1     19.9     33.6 write
38517.9s        0          627.6          670.5      3.7      9.4     15.7     88.1 read
38517.9s        0          159.0          167.6      7.9     15.7     25.2    113.2 write
...
E200925 02:55:44.465932 1 workload/cli/run.go:445  ERROR: result is ambiguous (error=unable to dial n2: breaker open [exhausted]) (SQLSTATE 40003)
38577.9s        1          469.4          670.2      3.5      8.1     13.1  11811.2 read
38577.9s        1          116.2          167.5      7.6     14.7     21.0    973.1 write
38637.9s        1          638.1          670.1      3.5      8.1     13.1  11811.2 read
38637.9s        1          159.5          167.5      7.3     13.6     21.0  11811.2 write
38697.8s        1          654.4          670.1      3.5      8.4     14.2     88.1 read
38697.8s        1          165.2          167.5      7.6     14.7     23.1     88.1 write
38757.8s        1          684.9          670.1      3.4      7.9     13.1     54.5 read
38757.8s        1          173.1          167.5      7.3     14.7     22.0    109.1 write
----

The one pointed at .14 (db2) reported:

[source,text]
----
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
31258.3s        0          497.2          638.6      5.0     12.1     17.8     88.1 read
31258.3s        0          126.4          159.7      9.4     17.8     25.2     96.5 write
31318.3s        0          645.5          638.6      3.9      7.9     11.5     37.7 read
31318.3s        0          165.2          159.7      7.9     13.1     18.9     54.5 write
E200925 00:55:42.465421 1 workload/cli/run.go:445  ERROR: result is ambiguous (error=unable to dial n3: breaker open [exhausted]) (SQLSTATE 40003)
31378.3s        1          453.7          638.2      4.2     10.0     18.9   1677.7 read
31378.3s        1          115.3          159.6      7.9     16.3     27.3  15569.3 write
31438.3s        1          512.2          638.0      3.9      9.4     15.7   8589.9 read
31438.3s        1          131.6          159.6      7.9     15.7     25.2   8589.9 write
31498.3s        1          648.9          638.0      3.7      8.9     14.2     44.0 read
31498.3s        1          163.6          159.6      7.6     14.2     22.0     44.0 write

...
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
38457.9s        1          658.2          637.8      3.7      7.9     13.1     65.0 read
38457.9s        1          166.6          159.5      7.9     14.2     22.0     79.7 write
38517.9s        1          624.8          637.8      3.8      8.9     15.2     79.7 read
38517.9s        1          156.6          159.5      7.9     15.7     25.2    109.1 write
E200925 02:56:19.557115 1 workload/cli/run.go:445  ERROR: result is ambiguous (error=unable to dial n7: breaker open [exhausted]) (SQLSTATE 40003)
38577.9s        2          417.9          637.4      3.7      7.9     12.6  10737.4 read
38577.9s        2          103.6          159.4      7.9     14.2     22.0  10737.4 write
38637.9s        2          651.4          637.5      3.8      7.6     12.6     71.3 read
38637.9s        2          164.3          159.4      7.9     13.6     21.0     88.1 write
38697.8s        2          650.7          637.5      3.7      8.1     13.6     71.3 read
38697.8s        2          163.8          159.4      7.9     15.2     23.1    109.1 write
----

All of these were right around sudden, brief periods of a few requests having 10-15s latencies.  Note these weren't caught even by the p99 -- they're only in pMax.

Final output:

[source,text]
----
^CNumber of reads that didn't return any results: 4.
Highest sequence written: 7105384. Can be passed as --write-seq=R7105384 to the next run.

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
42519.2s        1       28426499          668.6      3.9      3.1      8.1     17.8 103079.2  read

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
42519.2s        1        7105384          167.1      8.2      6.8     14.2     29.4  94489.3  write

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__result
42519.2s        1       35531883          835.7      4.8      3.7     10.5     21.0 103079.2  
----


[source,text]
----
^CNumber of reads that didn't return any results: 1.
Highest sequence written: 6754550. Can be passed as --write-seq=R6754550 to the next run.

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
42508.5s        2       27007396          635.3      4.2      3.4      8.4     17.8 103079.2  read

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
42508.5s        2        6754550          158.9      8.5      7.1     14.7     29.4 103079.2  write

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__result
42508.5s        2       33761946          794.2      5.0      3.9     10.5     21.0 103079.2  
----


[source,text]
----
^CNumber of reads that didn't return any results: 4.
Highest sequence written: 7336781. Can be passed as --write-seq=R7336781 to the next run.

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
42501.7s        0       29352493          690.6      3.8      3.1      7.9     16.8  98784.2  read

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
42501.7s        0        7336781          172.6      8.0      6.6     14.2     28.3 103079.2  write

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__result
42501.7s        0       36689274          863.2      4.6      3.7     10.0     19.9 103079.2  
----

== 2020-09-25

Today I'll do some basic fault testing.  I want to get back to a five-node cluster.  To do that, I need to reinitialize nodes 4, 5, and 6.  To do this, I'm using `terraform taint` on those three instances and then `terraform apply`.

I've brought up the last two nodes and have a five-node cluster with reasonably balanced replias.  Starting the "kv" workload once against each node.

[source,text]
----
cockroach workload run kv --histograms kv-histograms-$(date +%FT%TZ).out --concurrency 4 --display-every=60s --read-percent 80 --tolerate-errors postgresql://root@192.168.1.14:26257/kv?sslmode=disable
----

That was around 15:13Z.  I will leave this running for several hours.

Performance was a little erratic, and this is a higher load (cluster-wide) than previously, so I decided to bring the concurrency back down to see if it would be stable at a lower level instead.  Reran above commands but with concurrency 2.  That was at 15:20Z.

At 15:46Z I restarted these in order to run them under nohup.

19:52Z: These have now been running for about 4 hours reasonably steadily.  Throughput looks fairly consistent, though it's definitely jagged (small spike sin latency all the time).  Disks %busy is largely around 35%.  CPU utilization ranges a lot, but the busiest nodes are largely under 80%.  There have been 0 errors.

I'm going to try an easy test first: hit CockroachDB on one node with `kill -9`.  I'll do this several times in a row, a few minutes apart.  I'm picking db1 (n7) fairly arbitrarily.

The client talking to that node immediately reported 35 errors:

[source,text]
----
14999.2s        0          436.5          465.8      2.6      5.8     10.0    113.2 read
14999.2s        0          110.1          116.6      5.8     11.0     19.9    159.4 write
E200925 19:55:45.182200 1 workload/cli/run.go:445  EOF
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
15059.1s       35          434.5          465.6      2.6      5.2      8.9   1811.9 read
15059.1s       35          108.2          116.5      5.8     10.5     16.8   1744.8 write
----

I did this two more times, at 20:02 and 20:08.  There were a bunch of errors each time, and a reduction in throughput, but no real impact on latency.  Three of the four load generators aimed at different nodes also saw a few errors that all looked like this:

[source,text]
----
E200925 20:02:20.514932 1 workload/cli/run.go:445  ERROR: result is ambiguous (error=unable to dial n7: breaker open [exhausted]) (SQLSTATE 40003)
...
15719.1s        1          472.9          482.1      2.5      5.0      8.9     67.1 read
15719.1s        1          118.7          120.6      5.5     10.0     16.3     92.3 write
E200925 20:08:15.107262 1 workload/cli/run.go:445  ERROR: result is ambiguous (error=unable to dial n7: breaker open [exhausted]) (SQLSTATE 40003)
15779.1s        3          461.9          482.1      2.5      5.0      8.1    184.5 read
15779.1s        3          114.3          120.5      5.5     10.0     16.3   2818.6 write
----

Just for completeness I'm going to do this one more time on a different node, db4 (n11).  Done at 20:19:30 or so.  This had almost no visible impact on any of the graphs.  I did see similar errors on one client, but that was it.

**Reboot OS:** Next I'm going to try a hard reset of the OS on db4 using `uadmin 2 1`.  Did that around 20:44:50.  cockroachdb was back up by 20:45:55Z.

To the client running against that load balancer, this looked similar to `kill -9`:

[source,text]
----
17939.0s       83          497.9          510.8      2.4      5.0      8.9     50.3 read
17939.0s       83          126.3          127.7      5.5     10.5     16.3     75.5 write
E200925 20:45:00.996989 1 workload/cli/run.go:445  EOF
17999.0s      155          196.3          509.8      2.6      6.3     11.0     35.7 read
17999.0s      155           48.8          127.4      6.0     12.6     22.0     71.3 write
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
18059.0s      262          247.6          508.9      2.6      6.3     11.0   5100.3 read
18059.0s      262           61.3          127.2      5.8     11.5     16.8     35.7 write
----

Two other clients also saw similar errors as before.  Here's one:

[source,text]
----
17999.0s        0          419.5          461.1      2.8      6.3     11.5     41.9 read
17999.0s        0          105.1          115.4      6.0     12.1     18.9     71.3 write
E200925 20:45:08.161418 1 workload/cli/run.go:445  ERROR: result is ambiguous (error=unable to dial n11: breaker open [exhausted]) (SQLSTATE 40003)
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
18059.0s        1          362.5          460.8      2.8      6.8     10.5   7247.8 read
18059.0s        1           89.9          115.3      6.0     12.1     17.8   7247.8 write
18119.0s        1          435.3          460.7      2.6      6.0     10.0     71.3 read
18119.0s        1          108.6          115.3      5.8     11.0     15.7     62.9 write
----

There was a general bump in latency (p95 from 6ms to 8ms, p99 from 8ms to 14ms) and commensurate drop in throughput on all nodes.  The node I rebooted was back to full strength within 90 seconds.

Next, I'm going to try a real **OS panic**.  Did this at 20:51:20 or so.  Again, some clients saw some errors:

[source,text]
----
18299.0s        3          509.5          505.8      2.4      4.7      7.6     56.6 read
18299.0s        3          126.5          126.4      5.5     10.0     15.7     46.1 write
E200925 20:51:38.605596 1 workload/cli/run.go:445  ERROR: result is ambiguous (error=rpc error: code = Unavailable desc = transport is closing [propagate]) (SQLSTATE 40003)
18359.0s        4          323.6          505.2      2.5      5.0      8.4  20401.1 read
18359.0s        4           80.7          126.2      5.8     10.0     15.7  20401.1 write
18419.0s        4          485.1          505.2      2.5      5.0      7.9     41.9 read
18419.0s        4          121.7          126.2      5.8     10.0     15.2     35.7 write
----

The client connected to that host reported:

[source,text]
----
18359.0s      262          381.2          508.4      2.4      4.7      8.1     56.6 read
18359.0s      262           94.2          127.1      5.5      9.4     14.2    109.1 write
E200925 20:52:09.494424 1 workload/cli/run.go:445  read tcp 192.168.1.219:55958->192.168.1.252:26257: read: connection reset by peer
18419.0s      398            0.0          506.7      0.0      0.0      0.0      0.0 read
18419.0s      398            0.5          126.7      0.5      0.9  51539.6  51539.6 write
E200925 20:53:09.901031 1 workload/cli/run.go:445  dial tcp 192.168.1.252:26257: connect: connection refused
18479.0s      783          156.2          505.6      2.6      5.5     10.5   2281.7 read
18479.0s      783           39.3          126.4      5.8     10.5     16.3   1342.2 write
18539.0s      783          485.5          505.5      2.4      5.2     10.0     75.5 read
18539.0s      783          122.0          126.4      5.5     10.5     18.9     88.1 write
18599.0s      783          501.6          505.5      2.4      4.7      7.9    121.6 read
18599.0s      783          123.2          126.4      5.5     10.0     17.8    100.7 write
----

CockroachDB was back up by 20:53:09Z (estimated 2m or so of downtime).  Total impacted time for that one node, according to Prometheus, was about 3 minutes (from full strength to full strength).  The cluster latency and throughput were affected similarly to the reboot before it.

So those tests went pretty well.  Note in these cases, there was no rebalancing needed (or done).

23:18Z: going to start partitioning tests.  Will partition off db4.  Note that this is kind of tricky because my ssh session is coming in over the network.

I found that I don't have `ipdadm` available here, and I also wouldn't have access to the box while I did.  I may try doing this with ipf.

Notes:

[source,text]
----
# To enable IPF in the kernel (once per boot)
ipf -E

# To load/reload rules
ipf -F a -f filename

# To debug rules:
ipfstat -ionhl
----

I think the rule I want should be something like:

[source,text]
----
pass in all
pass out all
block in from any to any port = 26257
block out from any to any port = 26257
----

23:46Z: enabled IPF on db4 using `ipf -E`.  Did _not_ create any rules.  Want to double check that performance is okay.  I did not see any immediate result in the graphs, which makes sense, since the network is not a bottleneck here.

23:51Z: enable IPF rules above.  I confirmed that my ssh session still works.  I can't tell how Cockroach has been affected.  I see these errors from a few clients:

[source,text]
----
29158.4s      103          454.5          460.2      2.6      5.2      8.9     67.1 read
29158.4s      103          114.4          115.1      5.8     10.5     16.8     48.2 write
E200925 23:51:09.590144 1 workload/cli/run.go:445  ERROR: result is ambiguous (error=rpc error: code = Unavailable desc = transport is closing [exhausted]) (SQLSTATE 40003)
29218.4s      104          415.8          460.1      2.6      5.0      8.4   7247.8 read
29218.4s      104          103.6          115.1      5.8      9.4     15.7   7247.8 write
29278.4s      104          479.0          460.1      2.6      4.7      7.3     35.7 read
29278.4s      104          118.3          115.1      5.8      9.4     14.7     48.2 write
----

Admin UI does say n11 (.252) is suspect (which is the right node).  This is what I see in the load generator for the instance that's assigned to the node that's partitioned:

[source,text]
----
29038.4s      783          501.8          503.0      2.5      4.7      8.4     54.5 read
29038.4s      783          124.8          125.8      5.5     10.0     15.7     50.3 write
29098.4s      783          504.1          503.0      2.5      4.7      7.9     54.5 read
29098.4s      783          127.1          125.8      5.2      9.4     15.2     50.3 write
29158.4s      783          233.9          502.5      2.5      5.0     10.0     67.1 read
29158.4s      783           57.9          125.6      5.5     10.5     18.9    113.2 write
29218.4s      783            0.0          501.5      0.0      0.0      0.0      0.0 read
29218.4s      783            0.0          125.4      0.0      0.0      0.0      0.0 write
29278.4s      783            0.0          500.4      0.0      0.0      0.0      0.0 read
29278.4s      783            0.0          125.1      0.0      0.0      0.0      0.0 write
----

At 23:55Z, I flushed the rules with `ipf -F a`, re-enabling access.  The node is immediately no longer suspect.  The other load generators don't report anything interesting here.  The one pointed at the partitioned node:

[source,text]
----
29338.4s      783            0.0          499.4      0.0      0.0      0.0      0.0 read
29338.4s      783            0.0          124.9      0.0      0.0      0.0      0.0 write
29398.3s      783            0.0          498.4      0.0      0.0      0.0      0.0 read
29398.3s      783            0.0          124.6      0.0      0.0      0.0      0.0 write
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
29458.3s      783          406.7          498.2      2.5      4.7      7.6 103079.2 read
29458.3s      783          104.5          124.6      5.5      9.4     14.7 103079.2 write
29518.3s      783          499.4          498.2      2.5      4.7      7.9    125.8 read
29518.3s      783          124.3          124.6      5.5      9.4     16.3     79.7 write
----

It never reported any errors during that interval, and the max latency it reported was 103s.  That seems wrong.  (Added it to the README list of questions.)

That said, the overall impact here was pretty impressive.  CockroachDB reported no errors and (for whatever reason?) p95/p99 only rose a little bit -- less than ambient fluctuation for 30 minutes prior to the test.  It did have an effect -- queries per second dropped from about 2200 to about 1900 (across the cluster) during the partition.  Throughput on all nodes went down.  .252 plummeted to 0, of course.  QPS by store showed that store 11 did no queries for a little while.  Everybody's CPU usage and disk throughput went down a bit (probably because one load generator was off, not because one node was down).  p99 heartbeat latency shot up to 10s as well for that node.  I'd call this pretty successful, but the 103s peak is worth digging into.

Next, I'm going to try partitioning for a longer time.

2020-09-25T00:03Z: create partition again (using `ipf -F a -f block_crdb.ipf`)

Note: while digging into this, I found https://forum.cockroachlabs.com/t/high-service-latency/1105/6[a forum post] that shows two interesting things:

* high latencies of exactly 10.2 seconds (which is exactly what I see, when I feel it should be more).  Found https://github.com/cockroachdb/cockroach/issues/26529[another reference to 10.2 seconds].
* "The " slow heartbeat took" message usually means that the disk is slow (or has bizarre performance characteristics). We’ve seen this on some cloud vms."  I've seen this as well during bad tests, and wondered whether slow I/O could cause it somehow.  I'm not sure why it would.
* This has also led me to https://github.com/cockroachdb/cockroach/issues/19699[this GitHub issue].

00:12Z: The extended partition has so far proceeded as we'd expect:

[source,text]
----
29758.3s      783          124.8          124.6      5.5     10.0     15.7     44.0 write
29818.3s      783          437.9          498.1      2.4      5.0      9.4    134.2 read
29818.3s      783          111.8          124.5      5.5     12.6     96.5    352.3 write
29878.3s      783          276.7          497.6      2.5      5.0      8.4     33.6 read
29878.3s      783           71.1          124.4      5.5      9.4     14.2     75.5 write
29938.3s      783            0.0          496.6      0.0      0.0      0.0      0.0 read
29938.3s      783            0.0          124.2      0.0      0.0      0.0      0.0 write
29998.3s      783            0.0          495.6      0.0      0.0      0.0      0.0 read
29998.3s      783            0.0          123.9      0.0      0.0      0.0      0.0 write
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
30058.3s      783            0.0          494.6      0.0      0.0      0.0      0.0 read
30058.3s      783            0.0          123.7      0.0      0.0      0.0      0.0 write
30118.3s      783            0.0          493.7      0.0      0.0      0.0      0.0 read
30118.3s      783            0.0          123.4      0.0      0.0      0.0      0.0 write
30178.3s      783            0.0          492.7      0.0      0.0      0.0      0.0 read
30178.3s      783            0.0          123.2      0.0      0.0      0.0      0.0 write
E200926 00:08:49.992643 1 workload/cli/run.go:445  read tcp 192.168.1.219:44348->192.168.1.252:26257: read: connection timed out
30238.3s      787            0.0          491.7 103079.2 103079.2 103079.2 103079.2 read
30238.3s      787            0.0          122.9 103079.2 103079.2 103079.2 103079.2 write
30298.3s      787            0.0          490.7      0.0      0.0      0.0      0.0 read
30298.3s      787            0.0          122.7      0.0      0.0      0.0      0.0 write
30358.3s      787            0.0          489.7      0.0      0.0      0.0      0.0 read
30358.3s      787            0.0          122.5      0.0      0.0      0.0      0.0 write
E200926 00:12:32.150330 1 workload/cli/run.go:445  dial tcp 192.168.1.252:26257: connect: connection timed out
30418.3s      789            0.0          488.8      0.0      0.0      0.0      0.0 read
30418.3s      789            0.0          122.2 103079.2 103079.2 103079.2 103079.2 write
----

That's the load generator whose node is partitioned.  It's completing nothing and having queries take either 0 or the apparent maximum latency.  The other nodes have reported a few of the same error as above and a few multi-second latency outliers.  Here's an example from the load generator aimed at db1:

[source,text]
----
29818.3s      104          115.3          115.1      5.8     10.0     16.3     62.9 write
29878.3s      104          402.2          460.0      2.6      5.8     10.0    285.2 read
29878.3s      104           99.2          115.1      5.8     13.1    104.9    402.7 write
E200926 00:03:13.840994 1 workload/cli/run.go:445  ERROR: result is ambiguous (error=rpc error: code = Unavailable desc = transport is closing [exhausted]) (SQLSTATE 40003)
29938.3s      105          415.9          459.9      2.6      5.0      7.6   6174.0 read
29938.3s      105          103.9          115.0      5.8     10.0     15.7   6174.0 write
29998.3s      105          465.7          459.9      2.6      5.0      7.9     52.4 read
29998.3s      105          118.8          115.1      5.8      9.4     15.2     48.2 write
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
30058.3s      105          474.7          459.9      2.6      4.7      7.3     27.3 read
30058.3s      105          119.4          115.1      5.5      9.4     13.6     28.3 write
30118.3s      105          465.4          459.9      2.6      5.0      8.9    109.1 read
30118.3s      105          115.6          115.1      5.8     10.0     17.8     60.8 write
30178.3s      105          284.7          459.6      2.9     11.5     22.0   2684.4 read
30178.3s      105           71.3          115.0      6.0     18.9     46.1   2550.1 write
30238.3s      105          226.6          459.1      4.1     13.1     25.2   1275.1 read
30238.3s      105           56.2          114.9      8.1     27.3     88.1    570.4 write
30298.3s      105          409.1          459.0      3.0      6.3     11.5    100.7 read
30298.3s      105           99.5          114.8      6.0     11.0     16.8     96.5 write
30358.3s      105          447.3          459.0      2.8      5.2      8.1     50.3 read
30358.3s      105          111.1          114.8      5.8      9.4     14.2     32.5 write
----

It's a little surprising that things got better, then we still saw some pMax outliers in the multi-second range, but they weren't even caught by p99.

00:20: I'm surprised that now 17 minutes after the partition was introduced, there are still a lot of under-replicated ranges.  I wonder if those are all the system ranges, which have replication-factor 5?  At 03:30 Grafana reports 40 ranges being under-replicated.  At 07:30, the number is going down, and at 00:08 it's settled at 33.  Let's look at the ranges from our "kv" database:

[source,text]
----
root@192.168.1.24:26257/defaultdb> select (range_id, lease_holder, replicas) from [show ranges from database kv];
       ?column?
----------------------
  (55,3,"{2,3,7}")
  (67,3,"{2,3,7}")
  (85,7,"{2,3,7}")
  (77,12,"{2,7,12}")
  (83,2,"{2,3,7}")
  (84,2,"{2,3,7}")
  (69,7,"{2,7,12}")
  (62,7,"{2,3,7}")
  (79,3,"{2,3,12}")
  (76,12,"{3,7,12}")
  (90,12,"{2,3,12}")
(11 rows)
----

The node we're messing with is db4, which is n11.  Sure enough: it has no ranges for this database.  We did see a bunch of read activity and disk utilization on db1 starting around 00:08Z.  The read activity is going down slowly, but interestingly: it did not replace activity that was happening before on db4.  db4's reads were idle before that.  I suspect that previous to this, everything was in db4's ARC, and we're slowly filling db1's ARC.  Disk %busy on db1 has had a few spikes to saturation, but they've been very short-lived -- it's mostly at about 40%.  Other nodes' utilization was basically unaffected (except db4, which went to 0).  CPU utilization went up a bit on some nodes and down a bit on others after this happened.

Cluster thoughput had a big, brief crash at 00:08 corresponding to a spike in latency and CPU utilization on a few nodes (but a crash in utilization on other nodes0).  There was also a spike in network throughput, and there were a bunch of range adds and removes around this time.  I'm coming to think these are _replica_ adds and removes.

This all seems to have gone pretty well.

00:28Z: I'm going to restore network connectivity and see what happens.

This had a bigger impact than we'd like to see, but it looks like the cluster is recovering.  p95 bumped up from about 6ms to about 25ms.  p99 bumped up from about 10ms to about 90ms.  Still no errors reported.  The load generator picked up:

[source,text]
----
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
31258.2s      795            0.0          475.6      0.0      0.0      0.0      0.0 read
31258.2s      795            0.0          118.9      0.0      0.0      0.0      0.0 write
E200926 00:27:20.780072 1 workload/cli/run.go:445  dial tcp 192.168.1.252:26257: connect: connection timed out
31318.2s      797            0.0          474.7      0.0      0.0      0.0      0.0 read
31318.2s      797            0.0          118.7 103079.2 103079.2 103079.2 103079.2 write
31378.2s      797            0.0          473.8      0.0      0.0      0.0      0.0 read
31378.2s      797            0.0          118.5      0.0      0.0      0.0      0.0 write

31438.2s      797          148.4          473.2      2.9      7.9     19.9 103079.2 read
31438.2s      797           36.8          118.3      6.3     14.2     75.5 103079.2 write
31498.2s      797          191.8          472.7      3.7     17.8     35.7   2415.9 read
31498.2s      797           49.1          118.2      7.6     30.4    113.2   2013.3 write
31558.2s      797          192.4          472.1      2.8     18.9     48.2   1208.0 read
31558.2s      797           48.7          118.1      6.8     56.6    201.3   1006.6 write
31618.2s      797          465.2          472.1      2.5      5.5      8.9     60.8 read
31618.2s      797          113.7          118.0      5.5     10.0     17.8   2080.4 write
31678.2s      797          506.8          472.2      2.5      4.7      7.1     39.8 read
31678.2s      797          126.2          118.1      5.5      8.9     13.6     50.3 write
31738.2s      797          455.7          472.2      2.5      6.0     10.0     44.0 read
31738.2s      797          112.0          118.0      5.8     11.0     17.8     48.2 write
----

As we've seen with cluster expansion, CPU utilization went all over the place for a little while.  Disk %busy seemed to even out across the cluster, with only a few spikes.  The reads on db1 stopped.

I hadn't noticed before, but: although the partitioned node maintained its system replicas, it lost all of its leases -- that makes sense.  It got them back when it came back.  Further, it shows up (remember, it's n11) in the list of replicas for ranges from the "kv" database:

[source,text]
----
root@192.168.1.24:26257/defaultdb> select (range_id, lease_holder, replicas) from [show ranges from database kv];
       ?column?
-----------------------
  (55,2,"{2,3,11}")
  (67,7,"{7,11,12}")
  (85,2,"{2,3,7}")
  (77,12,"{2,11,12}")
  (83,3,"{2,3,7}")
  (84,7,"{3,7,11}")
  (69,12,"{2,7,12}")
  (62,11,"{2,7,11}")
  (79,3,"{2,3,12}")
  (76,3,"{3,11,12}")
  (90,11,"{2,11,12}")
(11 rows)

Time: 122.942376ms
----

During all of this, p99 RTT latency has risen as high as 1.75s and p99 heartbeat latency has hit 10s -- but those 10s data points are all for the partitioned node.  Again, 10s seems to be some hardcoded max.

There are no more under-replicated ranges.

In summary: there was a big crash in cluster throughput when the node came back -- down to about 800 selects and 200 inserts (from 1900 selects and 470 inserts while at 4 nodes).  That lasted about 3 minutes.  After that, throughput recovered to the previous level of about 2500 selects and 615 inserts.  This has gone pretty well!

To summarize, I did a few experiments today:

* 19:52, 20:02, 20:08Z, 20:19:30: `kill -9` one db node
* 20:44: reboot OS
* 20:51: panic OS
* 23:51 - 23:55: transient partition of one node
* 00:03 - 00:28: longer partition of one node

These all went pretty well, though there was an impact when the long partition was removed similar to (but not as bad as) when a node is added to or removed from the cluster.

I'm going to let these workloads run through the weekend to see how they do.  We seem to have plenty of disk space in the cluster as well as the "mon" zone.

Plan for next time:

* Review how the workload ran over the weekend
* Review README tests -- what's left that's worth doing?
* Rerun expansion, contraction tests on systems with dedicated NVME devices?

== 2020-09-28

Plan for the day:

* Review the long-running workload over the weekend.
* If time:
** Begin running expansion, contraction tests on systems with dedicated NVME drives
** Review README to come up with concrete plan for any more testing, if any

=== Review of the weekend workload

Review of weekend workload, from period 2020-09-26T01:00Z to 2020-09-28T20:00Z (2d 19h = 67 hours):

* Overall throughput was reasonably stable, with lots of variation around a fairly flat trend line.  Average was 2475 selects/second and 618 inserts/second.  These were well-distributed across all nodes, and slightly less well distributed across all stores (as we've seen before).
* There were six spikes in latency (and drops in throughput) that were notably larger than the general variation.  Generally, p95 was around 6ms (biggest peak was up to 10ms) and p99 was around 9ms (biggest peak up to 18ms).
* There were no CockroachDB errors reported.
* CPU utilization was fairly constant for most of the time.  CPUs ranged in utilization from about 50% to about 75%, but each CPU was fairly stable the whole time.
* Average I/O time (to the disk) was reasonably consistently below 750us.  There were some outliers up to 2.5ms.
* Disks were largely under 30% busy the whole time.  There was a notable divergence in %busy starting around 2020-09-27T00:00Z and a sudden, bigger divergence around 2020-09-27T16:00Z.  This latter one correlated with a spike in disk bytes read per second.  The write IOPS graphs track the %busy graphs in this way -- big divergence around 2020-09-27T16:00Z.
* There was a relatively large number of compactions at 2020-09-27T15:25Z.

Interestingly, there were zero range ops at all reported by CockroachDB during this time.  Replicas per node and leaseholders per node did change a bit at various points.  There were no internal errors either.

p99 Heartbeat latency was largely under 150ms, with one notable spike up to 1.2s around 2020-09-27T15:25Z.

The count of SQL connections was steady the entire time.  Mean RPC clock offset peaked around 500us.

Overall it feels like this went pretty well.  It'd be nice to better understand the general variation in latency and particularly the larger spikes, but this probably doesn't make sense in the current configuration.  It would make more sense in something closer to a production configuration, potentially with at least a slog device.

=== Expansion/contraction with local NVME devices

AWS provides instance types with local NVME devices.

Per https://calculator.aws/#/estimate?id=efaf0e10e9cf496d4dfcc95a26dbbf4cecef56b7[my calculation above]: 6 c4.large instances, each with a 60 GiB "io1" volume with 500 provisioned IOPS costs $783 / month.  This no longer reflects my current testing, though; now I'm using 6 m4.large instances, each with a 60GiB "io1" volume with 1000 provisioned IOPS.  https://calculator.aws/#/estimate?id=b0e74a19f859eb291e70cd889c41193d6945a4db[This costs $1,083 / month.]

For attached drives, I think we'd want to use the "i3" series.  Take "i3.large": that includes a 475 GiB NVME SSD, 10 Gbe, 2 vCPUs, and 15 GiB of memory.  That's the smallest one, and strictly better than our "m4.large".  This would be a good one to go with.  What does it cost?  For 6 of them with the cheapest EBS volume (which I assume would be unused -- a 60 GiB gp2 device), https://calculator.aws/#/estimate?id=87ed0544f7e510e94b8f2546d79388bb327c7e8f[it costs $720].  That's cheaper than what I'm using now.  Note that I'd need to keep these up a lot longer (i.e., overnight), since storage is transient.

Testing this out: I've modified my Terraform config to be able to deploy i3.large systems in parallel and I've modified Prometheus to scrape them, too.  I think the Prometheus config needs some work though because right now it's hard to separate these from the other database nodes, while also looking at the load generators.  Maybe I can deal with this separately though (e.g., with regular expressions on the name label).  I either want to do that or (probably better) use Prometheus relabeling to adjust the cluster name and maybe another label.

Although it did work to provision the i3.large instances, this didn't do what we want: it put the root pool on a tiny disk (2 GiB, probably the min size of the AMI) and the NVME device is unused.  I need to modify vminit to create a new pool and put the cockroach filesystem on that pool if there's an NVME device present.

Next steps:

* modify vminit to use NVME device (and/or: modify the whole thing to _always_ use a separate device for this)
* modify Prometheus config to relabel the nodes from different clusters differently
* modify Prometheus config to relabel the other metrics too? and/or modify the Grafana dashboards so that the load generator is in a totally separate row and change the existing metrics to filter on db vs. nvmedb

== 2020-09-29

* Modified vminit to use second device for a new pool with our ZFS datasets.  This breaks deployment of the non-NVME nodes for now.  I'll fix that when I'mr eady to tear down the long-running experiment.
* Modified Prometheus config to relabel the CockroachDB metrics for the cluster based on the Role tag set by Terraform.
* Modified Grafana dashboards to look only at one set of database nodes at a time.  I put the utilization metrics for the load generator and monitoring instances in a separate row.
* Modified vminit script to wait up to a minute for DNS to work, since there seems to be a startup race where DNS isn't available by the time we go to run "fetcher"
* Deployed a new NVME cluster
* Checked Grafana dashboards are showing the right thing for each cluster
* Started filling up NVME cluster
* Summarized the long-running workload again

=== Filling up the cluster

Around 21:19Z, to fill up the new NVME cluster, I started workloads like this:

[source,text]
----
cockroach workload run kv --init --concurrency 4 --display-every=60s --batch 10 --max-block-bytes 1024 --min-block-bytes 1024 postgresql://root@192.168.1.53:26257/kv?sslmode=disable
----

based on my notes above.  I ran one instance of this from the load generator for each of the three database nodes.  I want these to get to at least, say, 48 GiB per node (3x DRAM), which will mean that it's still >1x DRAM when I double the size of the cluster.  Ideally I'd let this run until it reaches more like 64 GiB per node or even 72.  That would be 144 GiB, 192 GiB, or 216 GiB, respectively, in total usage.

After just a minute or two, latency has increased considerably.  The load is entirely hitting db1, whose CPUs are both tapped out.  Hopefully CockroachDB figures this out soon and makes some splits.

CockroachDB execute some range splits at 21:20, 21:25, 21:32, and 21:50 or so.  Queries per second per store has evened out over this time, though one store is still doing only 2/3 of the other two.  CPUs on all three are above 90% utilization, with disks under 20% utilized.  I think this suggests we'd get better throughput if we increased the node count, but I want to reserve the extra nodes to see what happens when we expand the cluster _after_ the database is larger than memory.

There have been a few errors, particularly 21:20 to 21:26 and again around 21:32.  These are correlated with those splits.  These were not reported by the load generator.

22:50Z I looked at the rate at which the disk usage is growing.  We wrote about 5.2 GiB in the last hour.  At that rate, if I leave this until the morning (say, 9am), that's 17 hours from now, so we'd expect to have written 88 GiB overnight.  That's well under the capacity of these boxes, but a bit more than I'd intended.  It seems better to do that than to try to stop now and pick it up in the morning.

While wrapping up, I went to "nohup" my load generators.  I accidentally clobbered 2.out and 3.out from the long-running workload (two of the five files).  That's a bummer.

==== CPU utilization

Realizing that the thing is CPU bound and disks are pretty idle, I considered trying to ramp up the batch size to see what happens, but I decided against it because this is going to be done soon anyway and this gives me a chance to look at the CPU utilization:

[source,text]
----
root@nvmedb1:~# prstat -mLc 1
Please wait...
   PID USERNAME USR SYS TRP TFL DFL LCK SLP LAT VCX ICX SCL SIG PROCESS/LWP
  1292 root      25  71 0.0 0.0 0.0 0.0 0.0 4.4   0  1K 75K   0 prstat/1
  1236 cockroachdb 5.2 2.9 0.1 0.0 0.0  83 1.7 7.3 725 236  2K   0 cockroach/26
  1236 cockroachdb 5.1 2.8 0.1 0.0 0.0  83 1.5 7.3 751 235  2K   0 cockroach/20
  1236 cockroachdb 5.0 2.7 0.1 0.0 0.0  83 1.6 7.0 705 228  1K   0 cockroach/33
  1236 cockroachdb 4.6 2.5 0.1 0.0 0.0  85 1.3 6.4 657 209  1K   0 cockroach/16
  1236 cockroachdb 4.5 2.5 0.1 0.0 0.0  85 1.4 6.3 621 205  1K   0 cockroach/31
  1236 cockroachdb 4.4 2.4 0.1 0.0 0.0  86 1.2 6.2 626 197  1K   0 cockroach/32
  1236 cockroachdb 4.3 2.4 0.1 0.0 0.0  86 1.1 6.1 624 195  1K   0 cockroach/15
  1236 cockroachdb 4.2 2.4 0.1 0.0 0.0  86 1.1 6.0 630 194  1K   0 cockroach/19
  1165 root     0.0 6.4 0.0 0.0 0.0 0.0  92 1.4   2  32   0   0 zpool-tank/89
  1236 cockroachdb 4.1 2.2 0.1 0.0 0.0  87 1.2 5.7 585 184  1K   0 cockroach/21
  1236 cockroachdb 4.0 2.2 0.1 0.0 0.0  87 1.1 5.8 591 186  1K   0 cockroach/29
  1236 cockroachdb 4.0 2.2 0.1 0.0 0.0  85 3.1 5.6 570 183  1K   0 cockroach/11
  1236 cockroachdb 4.0 2.2 0.1 0.0 0.0  87 1.2 5.5 542 178  1K   0 cockroach/22
  1236 cockroachdb 4.0 2.2 0.1 0.0 0.0  86 2.3 5.6 567 184  1K   0 cockroach/9
Total: 43 processes, 691 lwps, load averages: 3.48, 3.49, 3.43
   PID USERNAME USR SYS TRP TFL DFL LCK SLP LAT VCX ICX SCL SIG PROCESS/LWP
  1236 cockroachdb  22  16 0.2 0.0 0.1  36 1.1  25  3K 827 10K   0 cockroach/22
  1236 cockroachdb  21  11 0.2 0.0 0.1  37 1.1  30  3K 876  9K   0 cockroach/3
  1236 cockroachdb  18  13 0.1 0.0 0.0  46 1.5  22  3K 580  9K   5 cockroach/15
  1236 cockroachdb  18 9.7 0.2 0.0 0.1  47 1.1  24  3K 657  8K   0 cockroach/28
  1236 cockroachdb  16  11 0.2 0.0 0.1  50 0.9  22  3K 640  8K   0 cockroach/31
  1236 cockroachdb 2.8 2.1 0.0 0.0 0.0  88 0.2 7.1 608 237  1K   0 cockroach/14
  1236 cockroachdb 2.9 2.0 0.0 0.0 0.0  91 0.2 3.6 591 154  1K   0 cockroach/11
  1236 cockroachdb 2.9 1.7 0.0 0.0 0.0  92 0.1 3.1 527 136  1K   0 cockroach/26
  1236 cockroachdb 2.4 2.2 0.1 0.0 0.0  90 0.1 5.3 450 203  1K   0 cockroach/27
  1236 cockroachdb 1.7 1.6 0.0 0.0 0.0  94 0.1 3.0 341 145 883   0 cockroach/1
  1236 cockroachdb 1.3 1.4 0.0 0.0 0.0  95 0.1 2.4 289  83 761   0 cockroach/16
  1236 cockroachdb 1.4 1.2 0.0 0.0 0.0  96 0.1 1.5 266  68 685   0 cockroach/13
  1292 root     0.3 1.0 0.0 0.0 0.0 0.0  99 0.0  16  13  2K   0 prstat/1
  1236 cockroachdb 0.5 0.5 0.0 0.0 0.0  98 0.1 0.5 117  20 311   0 cockroach/21
  1236 cockroachdb 0.5 0.3 0.0 0.0 0.0  99 0.0 0.3  73   6 208   0 cockroach/20
Total: 43 processes, 691 lwps, load averages: 3.47, 3.49, 3.43
   PID USERNAME USR SYS TRP TFL DFL LCK SLP LAT VCX ICX SCL SIG PROCESS/LWP
  1236 cockroachdb  20  14 0.2 0.0 0.0  43 1.0  22  3K 670 10K   3 cockroach/22
  1236 cockroachdb  22  12 0.2 0.0 0.0  39 1.0  26  3K 838 10K   0 cockroach/31
  1236 cockroachdb  19  11 0.2 0.0 0.0  40 1.5  28  3K 818  9K   0 cockroach/28
  1236 cockroachdb  17 9.5 0.2 0.0 0.0  52 1.0  21  3K 616  8K   0 cockroach/3
  1236 cockroachdb  13 7.7 0.1 0.0 0.0  65 0.6  14  2K 394  6K   1 cockroach/14
  1236 cockroachdb  12 7.0 0.2 0.0 0.0  59 0.6  21  2K 721  5K   0 cockroach/1
  1236 cockroachdb 3.6 2.7 0.0 0.0 0.0  87 0.3 6.4 695 184  1K   0 cockroach/27
  1236 cockroachdb 2.5 1.9 0.0 0.0 0.0  93 0.1 2.9 468 106  1K   0 cockroach/11
  1236 cockroachdb 2.1 1.7 0.0 0.0 0.0  92 0.1 3.8 396 136  1K   0 cockroach/21
  1236 cockroachdb 1.0 0.9 0.0 0.0 0.0  96 0.0 2.1 254  87 663   0 cockroach/26
  1236 cockroachdb 1.0 0.9 0.0 0.0 0.0  96 0.0 1.7 190  46 532   0 cockroach/13
  1236 cockroachdb 0.9 0.8 0.0 0.0 0.0  97 0.0 1.6 217  82 557   0 cockroach/16
  1236 cockroachdb 0.8 0.8 0.0 0.0 0.0  97 0.2 1.6 230  74 572   0 cockroach/33
  1236 cockroachdb 0.6 0.5 0.0 0.0 0.0  98 0.0 0.6 118  16 332   0 cockroach/17
  1236 cockroachdb 0.5 0.6 0.0 0.0 0.0  97 0.0 1.5 126  49 319   0 cockroach/15
Total: 43 processes, 691 lwps, load averages: 3.46, 3.48, 3.43
   PID USERNAME USR SYS TRP TFL DFL LCK SLP LAT VCX ICX SCL SIG PROCESS/LWP
  1236 cockroachdb  20  13 0.2 0.0 0.0  47 1.6  19  2K 589  6K   0 cockroach/1
  1236 cockroachdb  21 9.9 0.2 0.0 0.0  43 1.3  24  2K 696  6K   5 cockroach/14
  1236 cockroachdb  21 9.6 0.3 0.0 0.0  38 1.4  30  2K 836  7K   0 cockroach/28
  1236 cockroachdb  18 9.0 0.2 0.0 0.0  51 3.3  19  2K 563  6K   1 cockroach/31
  1236 cockroachdb  17 8.0 0.2 0.0 0.0  54 1.5  17  1K 549  4K   3 cockroach/27
  1236 cockroachdb  11 5.4 0.1 0.0 0.0  69 0.7  14  1K 429  2K   0 cockroach/22
  1236 cockroachdb 5.1 2.8 0.1 0.0 0.0  86 0.4 5.2 652 224  1K   0 cockroach/11
  1236 cockroachdb 4.8 2.4 0.1 0.0 0.0  88 0.6 4.3 528 193  1K   0 cockroach/33
  1236 cockroachdb 1.8 1.1 0.0 0.0 0.0  95 0.2 1.8 276  77 738   0 cockroach/3
  1236 cockroachdb 1.0 0.6 0.0 0.0 0.0  98 0.0 0.8 173  25 492   0 cockroach/20
  1292 root     0.3 0.9 0.0 0.0 0.0 0.0  99 0.0  16   8  2K   0 prstat/1
  1236 cockroachdb 0.7 0.5 0.0 0.0 0.0  98 0.4 0.9 114  23 306   0 cockroach/21
  1165 root     0.0 0.8 0.0 0.0 0.0 0.0  99 0.5  83   0   0   0 zpool-tank/101
  1165 root     0.0 0.7 0.0 0.0 0.0 0.0  99 0.5  78   2   0   0 zpool-tank/93
  1236 cockroachdb 0.4 0.3 0.0 0.0 0.0  99 0.0 0.1  49   7 145   0 cockroach/32
Total: 43 processes, 691 lwps, load averages: 3.45, 3.48, 3.43
   PID USERNAME USR SYS TRP TFL DFL LCK SLP LAT VCX ICX SCL SIG PROCESS/LWP
  1236 cockroachdb  24  10 0.3 0.0 0.0  37 2.1  26  2K 798  6K   4 cockroach/14
  1236 cockroachdb  24 9.5 0.3 0.0 0.1  35 2.2  29  2K 842  6K   3 cockroach/28
  1236 cockroachdb  22 8.9 0.2 0.0 0.0  46 1.9  22  2K 660  5K   1 cockroach/22
  1236 cockroachdb  21 7.6 0.2 0.0 0.0  54 1.6  16  1K 599  5K   0 cockroach/33
  1236 cockroachdb  15 6.8 0.2 0.0 0.0  56 1.3  20  1K 633  4K   0 cockroach/31
  1236 cockroachdb 8.9 4.3 0.2 0.0 0.0  73 1.4  12  1K 456  2K   0 cockroach/1
  1236 cockroachdb 3.8 1.5 0.1 0.0 0.0  92 0.5 2.5 421 140  1K   0 cockroach/3
  1236 cockroachdb 3.4 1.7 0.0 0.0 0.0  90 0.8 3.9 372 131 985   0 cockroach/27
  1236 cockroachdb 2.5 1.1 0.0 0.0 0.0  95 0.4 1.0 217  62 560   0 cockroach/32
  1236 cockroachdb 1.5 0.9 0.0 0.0 0.0  96 0.7 1.3 215  73 596   0 cockroach/7
  1236 cockroachdb 1.4 0.5 0.0 0.0 0.0  97 0.2 0.4 122  22 328   0 cockroach/21
  1236 cockroachdb 1.2 0.7 0.0 0.0 0.0  97 0.3 1.0 168  52 444   0 cockroach/11
  1292 root     0.3 0.9 0.0 0.0 0.0 0.0  99 0.1  16  15  2K   0 prstat/1
  1236 cockroachdb 0.7 0.2 0.0 0.0 0.0  99 0.0 0.2  52  25 128   0 cockroach/18
  1165 root     0.0 0.8 0.0 0.0 0.0 0.0  99 0.4  81   1   0   0 zpool-tank/101
Total: 43 processes, 691 lwps, load averages: 3.45, 3.48, 3.43
...
root@nvmedb1:~# dtrace -n 'profile-97/pid == 1236 && arg0/{ @[stack()] = count(); }' -c 'sleep 60' > stacks-kernel.out
dtrace: description 'profile-97' matched 1 probe
dtrace: pid 1301 has exited
...
root@nvmedb1:~# prstat -mLc 1
Please wait...
   PID USERNAME USR SYS TRP TFL DFL LCK SLP LAT VCX ICX SCL SIG PROCESS/LWP
  1327 root      25  69 0.0 0.0 0.0 0.0 0.0 6.3   0 736 79K   0 prstat/1
  1236 cockroachdb 5.0 2.8 0.1 0.0 0.0  83 1.5 7.1 737 229  2K   0 cockroach/20
  1236 cockroachdb 5.0 2.8 0.1 0.0 0.0  83 1.7 7.0 708 228  1K   0 cockroach/33
  1236 cockroachdb 5.0 2.7 0.1 0.0 0.0  84 1.6 7.0 694 225  1K   0 cockroach/26
  1236 cockroachdb 4.5 2.5 0.1 0.0 0.0  85 1.3 6.3 644 205  1K   0 cockroach/16
  1236 cockroachdb 4.4 2.4 0.1 0.0 0.0  86 1.2 6.1 627 196  1K   0 cockroach/15
  1236 cockroachdb 4.4 2.4 0.1 0.0 0.0  86 1.3 6.1 599 196  1K   0 cockroach/31
  1236 cockroachdb 4.3 2.4 0.1 0.0 0.0  86 1.2 6.2 619 193  1K   0 cockroach/32
  1236 cockroachdb 4.2 2.4 0.1 0.0 0.0  86 1.1 6.0 627 190  1K   0 cockroach/19
  1236 cockroachdb 4.1 2.3 0.1 0.0 0.0  85 3.1 5.8 590 188  1K   0 cockroach/11
  1236 cockroachdb 4.1 2.2 0.1 0.0 0.0  87 1.2 5.6 554 181  1K   0 cockroach/22
  1236 cockroachdb 4.0 2.2 0.1 0.0 0.0  87 1.2 5.7 584 181  1K   0 cockroach/21
  1236 cockroachdb 4.1 2.2 0.1 0.0 0.0  86 2.3 5.7 578 185  1K   0 cockroach/9
  1165 root     0.0 6.2 0.0 0.0 0.0 0.0  92 1.4   2  31   0   0 zpool-tank/89
  1236 cockroachdb 3.9 2.2 0.1 0.0 0.0  87 1.2 5.4 566 178  1K   0 cockroach/14
Total: 43 processes, 691 lwps, load averages: 3.10, 3.36, 3.39
   PID USERNAME USR SYS TRP TFL DFL LCK SLP LAT VCX ICX SCL SIG PROCESS/LWP
  1165 root     0.0  35 0.0 0.0 0.0 0.0  61 4.8  10 147   0   0 zpool-tank/89
  1236 cockroachdb  10 5.0 0.3 0.0 0.0  42 9.3  34  1K 822  3K   2 cockroach/14
  1236 cockroachdb  10 4.9 0.3 0.0 0.0  51 7.3  27  1K 705  3K   0 cockroach/10
  1236 cockroachdb 9.3 4.5 0.2 0.0 0.0  54 6.5  25  1K 743  3K   0 cockroach/13
  1236 cockroachdb 8.2 4.1 0.2 0.0 0.0  60 6.6  21  1K 696  2K   0 cockroach/21
  1236 cockroachdb 8.4 3.9 0.2 0.0 0.0  59 5.9  23  1K 551  2K   0 cockroach/7
  1236 cockroachdb 5.6 2.3 0.1 0.0 0.0  78 5.6 8.6 653 297  1K   0 cockroach/15
  1236 cockroachdb 4.3 2.0 0.1 0.0 0.0  84 1.9 7.4 542 327  1K   0 cockroach/25
  1236 cockroachdb 2.7 1.3 0.1 0.0 0.0  91 0.6 4.6 277 134 737   2 cockroach/11
  1236 cockroachdb 2.3 1.0 0.1 0.0 0.0  88 4.0 4.7 366 118 948   0 cockroach/19
  1236 cockroachdb 2.0 0.9 0.0 0.0 0.0  86 7.9 2.8 279  66 743   0 cockroach/17
  1236 cockroachdb 1.2 0.7 0.0 0.0 0.0  96 0.9 1.0 125  48 352   0 cockroach/29
  1236 cockroachdb 1.1 0.3 0.0 0.0 0.0  95 0.6 2.6  90  54 249   0 cockroach/18
  1327 root     0.3 1.0 0.0 0.0 0.0 0.0  99 0.0  16  16  2K   0 prstat/1
  1236 cockroachdb 0.2 0.8 0.0 0.0 0.0  15  76 8.5 655  21 692   0 cockroach/2
Total: 43 processes, 691 lwps, load averages: 3.09, 3.36, 3.39
   PID USERNAME USR SYS TRP TFL DFL LCK SLP LAT VCX ICX SCL SIG PROCESS/LWP
  1236 cockroachdb 9.6 5.7 0.3 0.0 0.0  54  16  15  1K 821  4K   1 cockroach/25
  1236 cockroachdb 9.6 5.7 0.2 0.0 0.0  58  13  13  1K 711  4K   0 cockroach/13
  1236 cockroachdb 8.6 5.0 0.2 0.0 0.0  62  12  12  1K 699  3K   0 cockroach/14
  1236 cockroachdb 7.9 4.4 0.2 0.0 0.0  62  15  11  1K 487  3K   0 cockroach/10
  1236 cockroachdb 7.1 3.9 0.1 0.0 0.0  72  11 5.8  1K 411  3K   0 cockroach/11
  1236 cockroachdb 5.9 3.7 0.1 0.0 0.0  73 8.2 8.9  1K 417  2K   0 cockroach/21
  1236 cockroachdb 2.6 1.5 0.0 0.0 0.0  88 6.0 1.6 465 103  1K   0 cockroach/29
  1236 cockroachdb 2.6 1.3 0.1 0.0 0.0  92 3.1 1.3 422 127  1K   0 cockroach/18
  1236 cockroachdb 2.1 1.1 0.0 0.0 0.0  92 2.9 1.4 356 107 871   0 cockroach/17
  1236 cockroachdb 1.6 0.9 0.0 0.0 0.0  95 1.6 1.3 285  88 693   0 cockroach/15
  1236 cockroachdb 0.9 0.5 0.0 0.0 0.0  97 1.3 0.6 144  51 358   0 cockroach/7
  1236 cockroachdb 0.8 0.5 0.0 0.0 0.0  97 1.0 0.4 124  28 346   0 cockroach/19
  1236 cockroachdb 0.3 1.0 0.0 0.0 0.0  19  78 1.5 957  19 987   0 cockroach/2
  1327 root     0.3 0.9 0.0 0.0 0.0 0.0  99 0.0  16  18  2K   0 prstat/1
  1236 cockroachdb 0.6 0.3 0.0 0.0 0.0  99 0.3 0.2  55  15 149   0 cockroach/16
Total: 43 processes, 691 lwps, load averages: 3.09, 3.36, 3.39
...
root@nvmedb1:~# dtrace -n 'profile-97/pid == 1236 && arg1/{ @[ustack()] = count(); }' -c 'sleep 60' > stacks-user.out
dtrace: description 'profile-97' matched 1 probe
dtrace: pid 1330 has exited
root@nvmedb1:~# 
root@nvmedb1:~# prstat -mLc 1
Please wait...
   PID USERNAME USR SYS TRP TFL DFL LCK SLP LAT VCX ICX SCL SIG PROCESS/LWP   
  1332 root      28  70 0.8 0.0 0.0 0.0 0.0 1.0   0 774 .1M   0 prstat/1
  1236 cockroachdb 5.1 2.9 0.1 0.0 0.0  83 1.8 7.1 718 231  1K   0 cockroach/33
  1236 cockroachdb 5.0 2.8 0.1 0.0 0.0  83 1.6 7.1 730 227  2K   0 cockroach/20
  1236 cockroachdb 4.8 2.6 0.1 0.0 0.0  84 1.5 6.6 658 214  1K   0 cockroach/26
  1236 cockroachdb 4.5 2.5 0.1 0.0 0.0  85 1.4 6.2 636 203  1K   0 cockroach/16
  1236 cockroachdb 4.4 2.5 0.1 0.0 0.0  85 1.5 6.2 610 200  1K   0 cockroach/31
  1236 cockroachdb 4.3 2.4 0.1 0.0 0.0  86 1.2 6.1 619 192  1K   0 cockroach/32
  1236 cockroachdb 4.3 2.4 0.1 0.0 0.0  86 1.2 6.0 615 191  1K   0 cockroach/15
  1236 cockroachdb 4.2 2.5 0.1 0.0 0.0  86 1.2 6.0 634 191  1K   0 cockroach/19
  1236 cockroachdb 4.1 2.3 0.1 0.0 0.0  85 3.0 5.8 587 187  1K   0 cockroach/11
  1236 cockroachdb 4.0 2.2 0.1 0.0 0.0  87 1.3 5.6 548 179  1K   0 cockroach/22
  1236 cockroachdb 4.0 2.3 0.1 0.0 0.0  87 1.2 5.6 576 179  1K   0 cockroach/21
  1236 cockroachdb 4.0 2.2 0.1 0.0 0.0  86 2.2 5.6 571 182  1K   0 cockroach/9
  1236 cockroachdb 3.9 2.2 0.1 0.0 0.0  87 1.2 5.4 566 178  1K   0 cockroach/14
  1165 root     0.0 6.1 0.0 0.0 0.0 0.0  93 1.3   2  30   0   0 zpool-tank/89
Total: 43 processes, 693 lwps, load averages: 2.83, 3.11, 3.29
   PID USERNAME USR SYS TRP TFL DFL LCK SLP LAT VCX ICX SCL SIG PROCESS/LWP   
  1165 root     0.0  46 0.0 0.0 0.0 0.0  49 5.1  14 195   0   0 zpool-tank/89
  1236 cockroachdb 7.1 4.3 0.2 0.0 0.0  43  16  30  1K 819  3K   0 cockroach/20
  1236 cockroachdb 7.5 3.9 0.2 0.0 0.0  54  11  24  1K 691  3K   0 cockroach/32
  1236 cockroachdb 6.8 4.5 0.2 0.0 0.0  55  11  22  1K 740  3K   0 cockroach/7
  1236 cockroachdb 5.3 3.2 0.2 0.0 0.0  67  12  12  1K 546  2K   0 cockroach/31
  1236 cockroachdb 4.1 2.3 0.1 0.0 0.0  77  11 5.7 726 333  1K   0 cockroach/18
  1236 cockroachdb 4.3 1.9 0.1 0.0 0.0  77 4.2  13 668 371  1K   0 cockroach/8
  1236 cockroachdb 4.0 2.0 0.1 0.0 0.0  79 9.8 4.7 625 303  1K   0 cockroach/14
  1236 cockroachdb 1.9 1.1 0.0 0.0 0.0  89 4.1 3.6 365 150 898   0 cockroach/35
  1236 cockroachdb 2.0 1.0 0.0 0.0 0.0  92 1.6 3.0 260 116 671   0 cockroach/23
  1236 cockroachdb 0.3 1.0 0.0 0.0 0.0  25  68 5.5 868  19 898   0 cockroach/2
  1236 cockroachdb 0.8 0.3 0.0 0.0 0.0  97 0.7 0.9  72  58 185   0 cockroach/13
  1332 root     0.2 0.7 0.0 0.0 0.0 0.0  99 0.0  16  10  2K   0 prstat/1
  1165 root     0.0 0.7 0.0 0.0 0.0 0.0  97 2.7 484   0   0   0 zpool-tank/113
  1165 root     0.0 0.6 0.0 0.0 0.0 0.0  97 1.9 493   0   0   0 zpool-tank/115
Total: 43 processes, 693 lwps, load averages: 2.84, 3.11, 3.29
   PID USERNAME USR SYS TRP TFL DFL LCK SLP LAT VCX ICX SCL SIG PROCESS/LWP   
  1236 cockroachdb 8.2 5.4 0.3 0.0 0.0  52  18  16  1K 920  4K   0 cockroach/7
  1236 cockroachdb 7.7 4.9 0.2 0.0 0.0  58  18  12  1K 698  3K   1 cockroach/18
  1236 cockroachdb 6.9 4.4 0.2 0.0 0.0  63  16 9.3  1K 614  3K   0 cockroach/14
  1236 cockroachdb 6.2 3.8 0.1 0.0 0.0  71  12 7.1  1K 399  2K   0 cockroach/35
  1236 cockroachdb 5.5 2.7 0.1 0.0 0.0  80 4.1 7.1 744 423  1K   0 cockroach/32
  1236 cockroachdb 4.4 3.1 0.1 0.0 0.0  80 8.1 4.7 946 365  2K   0 cockroach/23
  1236 cockroachdb 2.9 2.0 0.0 0.0 0.0  88 4.9 1.8 511 139  1K   0 cockroach/20
  1236 cockroachdb 2.9 1.6 0.1 0.0 0.0  86 7.1 2.7 531 175  1K   0 cockroach/31
  1236 cockroachdb 1.8 1.0 0.0 0.0 0.0  93 3.1 1.1 289  67 756   0 cockroach/13
  1236 cockroachdb 1.5 1.0 0.0 0.0 0.0  91 4.9 1.1 298  63 748   0 cockroach/25
  1236 cockroachdb 0.2 0.9 0.0 0.0 0.0  29  69 1.1 844  18 868   0 cockroach/2
  1236 cockroachdb 0.7 0.4 0.0 0.0 0.0  97 1.8 0.3 122  21 298   0 cockroach/22
  1165 root     0.0 0.9 0.0 0.0 0.0 0.0  98 1.0 532   0   0   0 zpool-tank/105
  1332 root     0.2 0.6 0.0 0.0 0.0 0.0  99 0.0  16   4  2K   0 prstat/1
  1165 root     0.0 0.5 0.0 0.0 0.0 0.0  99 0.5 528   0   0   0 zpool-tank/3
Total: 43 processes, 693 lwps, load averages: 2.84, 3.11, 3.29
   PID USERNAME USR SYS TRP TFL DFL LCK SLP LAT VCX ICX SCL SIG PROCESS/LWP   
  1236 cockroachdb 7.8 4.8 0.2 0.0 0.0  56  22  10  1K 596  3K   0 cockroach/18
  1236 cockroachdb 6.2 4.3 0.2 0.0 0.0  66  13  11  1K 631  3K   0 cockroach/14
  1236 cockroachdb 4.9 3.3 0.1 0.0 0.0  74  13 5.1  1K 368  2K   0 cockroach/23
  1236 cockroachdb 4.9 3.2 0.1 0.0 0.0  76 9.6 6.6 937 372  2K   0 cockroach/7
  1236 cockroachdb 3.8 2.4 0.1 0.0 0.0  77  11 5.3 838 330  1K   0 cockroach/25
  1236 cockroachdb 3.4 2.4 0.1 0.0 0.0  74  16 3.2 784 207  1K   1 cockroach/35
  1236 cockroachdb 3.1 2.0 0.1 0.0 0.0  86 5.8 3.3 562 230  1K   0 cockroach/31
  1236 cockroachdb 2.9 1.7 0.1 0.0 0.0  88 5.1 2.4 521 176  1K   0 cockroach/20
  1236 cockroachdb 2.8 1.5 0.0 0.0 0.0  90 4.3 1.2 454  87  1K   0 cockroach/13
  1236 cockroachdb 0.2 0.9 0.0 0.0 0.0  35  63 1.0 807   7 838   0 cockroach/2
  1236 cockroachdb 0.7 0.4 0.0 0.0 0.0  98 0.5 0.3  87  25 205   0 cockroach/32
  1332 root     0.2 0.7 0.0 0.0 0.0 0.0  99 0.0  16  13  2K   0 prstat/1
  1165 root     0.0 0.8 0.0 0.0 0.0 0.0  98 0.8 454   1   0   0 zpool-tank/105
  1236 cockroachdb 0.3 0.2 0.0 0.0 0.0  99 0.0 0.1  35  11  92   0 cockroach/8
  1165 root     0.0 0.5 0.0 0.0 0.0 0.0  99 0.4 457   0   0   0 zpool-tank/3
Total: 43 processes, 693 lwps, load averages: 2.84, 3.11, 3.29
root@nvmedb1:~# 
root@nvmedb1:~# dtrace -x ustackframes=40 -n 'profile-97/pid == 1236 && arg1/{ @[ustack()] = count(); }' -c 'sleep 60' > stacks-user.out
dtrace: description 'profile-97' matched 1 probe
dtrace: pid 1336 has exited
root@nvmedb1:~# prstat -mLc 1
Please wait...
   PID USERNAME USR SYS TRP TFL DFL LCK SLP LAT VCX ICX SCL SIG PROCESS/LWP
  1337 root      27  71 0.0 0.0 0.0 0.0 0.0 2.0   0 151 82K   0 prstat/1
  1236 cockroachdb 5.0 2.9 0.1 0.0 0.0  83 1.8 7.1 739 230  2K   0 cockroach/20
  1236 cockroachdb 4.9 2.9 0.1 0.0 0.0  83 1.9 7.0 704 226  1K   0 cockroach/33
  1236 cockroachdb 4.5 2.5 0.1 0.0 0.0  85 1.4 6.2 621 201  1K   0 cockroach/26
  1236 cockroachdb 4.4 2.5 0.1 0.0 0.0  85 1.6 6.2 615 201  1K   0 cockroach/31
  1236 cockroachdb 4.3 2.6 0.1 0.0 0.0  85 1.4 6.2 629 193  1K   0 cockroach/32
  1236 cockroachdb 4.4 2.5 0.1 0.0 0.0  86 1.4 6.1 628 199  1K   0 cockroach/16
  1236 cockroachdb 4.1 2.4 0.1 0.0 0.0  86 1.2 5.9 618 187  1K   0 cockroach/19
  1236 cockroachdb 4.0 2.3 0.1 0.0 0.0  85 3.1 5.7 583 186  1K   0 cockroach/11
  1236 cockroachdb 4.0 2.4 0.1 0.0 0.0  86 1.4 5.6 586 183  1K   0 cockroach/14
  1236 cockroachdb 4.0 2.3 0.1 0.0 0.0  86 1.5 5.6 559 181  1K   0 cockroach/22
  1236 cockroachdb 4.0 2.3 0.1 0.0 0.0  87 1.1 5.6 581 181  1K   0 cockroach/15
  1236 cockroachdb 3.9 2.3 0.1 0.0 0.0  86 2.2 5.6 569 180  1K   0 cockroach/9
  1165 root     0.0 6.0 0.0 0.0 0.0 0.0  93 1.3   2  29   0   0 zpool-tank/89
  1236 cockroachdb 3.8 2.1 0.1 0.0 0.0  88 1.1 5.3 544 169  1K   0 cockroach/21
Total: 43 processes, 693 lwps, load averages: 2.42, 2.83, 3.14
   PID USERNAME USR SYS TRP TFL DFL LCK SLP LAT VCX ICX SCL SIG PROCESS/LWP
  1236 cockroachdb  11  23 0.1 0.0 0.6  30 4.8  26  2K 271  4K   1 cockroach/14
  1236 cockroachdb 8.9  21 0.1 0.0 0.4  37 4.5  26  2K 340  3K   1 cockroach/20
  1236 cockroachdb 9.7  19 0.1 0.0 0.3  41 3.2  22  2K 281  3K   0 cockroach/19
  1236 cockroachdb 9.1  18 0.1 0.0 0.4  41 4.0  26  2K 343  3K   0 cockroach/16
  1236 cockroachdb  13  10 0.1 0.0 0.3  61 1.8  13  1K 225  4K   5 cockroach/17
  1236 cockroachdb  13 8.9 0.1 0.0 0.1  57 1.9  19  1K 329  4K   1 cockroach/22
  1236 cockroachdb 1.8 3.4 0.0 0.0 0.0  91 0.9 2.7 336  63 680   0 cockroach/32
  1236 cockroachdb 1.0 2.8 0.0 0.0 0.0  94 0.8 1.1 216  27 369   0 cockroach/7
  1236 cockroachdb 1.3 1.8 0.0 0.0 0.0  95 0.6 1.0 219  15 478   0 cockroach/18
  1236 cockroachdb 0.7 0.7 0.0 0.0 0.0  97 0.3 1.1 123  17 276   0 cockroach/9
  1337 root     0.3 1.0 0.0 0.0 0.0 0.0  99 0.0  16   4  2K   0 prstat/1
  1236 cockroachdb 0.3 0.8 0.0 0.0 0.0  98 0.0 1.0  75  28 157   0 cockroach/31
  1236 cockroachdb 0.3 0.5 0.0 0.0 0.0  99 0.1 0.1  44   7 104   0 cockroach/36
  1236 cockroachdb 0.2 0.5 0.0 0.0 0.0  99 0.2 0.3  31  12  54   0 cockroach/11
  1236 cockroachdb 0.1 0.3 0.0 0.0 0.0 0.0  96 3.4 242   1 353   0 cockroach/2
Total: 43 processes, 693 lwps, load averages: 2.41, 2.83, 3.13
   PID USERNAME USR SYS TRP TFL DFL LCK SLP LAT VCX ICX SCL SIG PROCESS/LWP
  1236 cockroachdb 9.3  22 0.1 0.0 0.4  31 5.1  28  2K 327  3K   0 cockroach/19
  1236 cockroachdb 9.1  22 0.0 0.0 0.5  37 5.2  24  2K 269  3K   0 cockroach/16
  1236 cockroachdb 7.4  21 0.0 0.0 0.2  37 4.2  26  2K 269  3K   0 cockroach/14
  1236 cockroachdb 8.1  20 0.0 0.0 0.2  37 5.7  25  2K 264  3K   0 cockroach/20
  1236 cockroachdb  12  12 0.1 0.0 0.2  51 4.6  20  1K 294  4K   0 cockroach/22
  1236 cockroachdb 6.4  12 0.1 0.0 0.3  59 2.8  18  1K 265  2K   0 cockroach/32
  1236 cockroachdb 1.7 4.7 0.0 0.0 0.1  89 1.0 2.7 472  58 839   0 cockroach/11
  1236 cockroachdb 1.6 3.2 0.0 0.0 0.0  92 0.5 2.2 333  46 690   0 cockroach/31
  1236 cockroachdb 0.9 1.0 0.0 0.0 0.0  97 0.2 0.7 120  16 250   0 cockroach/9
  1236 cockroachdb 0.4 1.1 0.0 0.0 0.0  98 0.1 0.5  89  20 200   0 cockroach/7
  1337 root     0.3 0.9 0.0 0.0 0.0 0.0  99 0.2  17  15  2K   0 prstat/1
  1236 cockroachdb 0.3 0.7 0.0 0.0 0.0  97 0.6 0.8 121  18 225   0 cockroach/18
  1236 cockroachdb 0.2 0.7 0.0 0.0 0.0  99 0.2 0.1  52   1  84   0 cockroach/28
  1236 cockroachdb 0.4 0.3 0.0 0.0 0.0  99 0.0 0.4  58  10 135   0 cockroach/36
  1236 cockroachdb 0.1 0.2 0.0 0.0 0.0 0.0  98 2.0  99   2 198   0 cockroach/2
Total: 43 processes, 693 lwps, load averages: 2.40, 2.82, 3.13
root@nvmedb1:~#
----

In short: we're seeing lots of LAT on this system and lots of CPU usage, and it's both userland and system time.  I've got the data to make a userland and kernel flame graph.  This happened over several minutes.

I collected these again with longer stacks:

[source,text]
----
root@nvmedb1:~# dtrace -x ustackframes=80 -n 'profile-97/pid == 1236 && arg1/{ @[ustack()] = count(); }' -c 'sleep 60' > stacks-user.out; dtrace -x stackframes=80 -n 'profile-97/pid == 1236 && arg0/{ @[stack()] = count(); }' -c 'sleep 60' > stacks-kernel.out
dtrace: description 'profile-97' matched 1 probe
dtrace: pid 1346 has exited
dtrace: description 'profile-97' matched 1 probe
dtrace: pid 1348 has exited
root@nvmedb1:~#
----

I copied these locally and ran:

[source,text]
----
for x in kernel user; do stackvis dtrace flamegraph-d3 < stacks-$x.out > stacks-$x.htm; stackvis dtrace flamegraph-svg < stacks-$x.out > stacks-$x.svg; done
----

The d3 flame graphs have a clipping problem with the text.  The SVG ones are okay.  I've copied them into this repo.  It kind of looks like an ARC stress test.


=== Revisiting the long-running workload

I have useful data now from 2020-09-26T01:00:00Z through 2020-09-29T15:00:00Z.  At that endpoint, I tweaked the Prometheus labels and it's hard to get a graph that spans both sides of this timestamp.  This period covers 3d14h (86 hours).

With a few exceptions mentioned below:

* Throughput is reasonably steady around 2461 selects per second and 615 inserts per second.
* Average p95 latency is about 6ms and average p99 latency is about 9ms.
* The throughput and latency is balanced across all three nodes.  In terms of distribution across stores: as we've seen before, there are two modes, around 580qps and 680qps -- this is still fairly balanced.
* CPUs on db1, db2, db3, db4, and db5 (all the busy nodes) range in utilization from about 60% to about 75%.  Each CPU is fairly consistent, and there's some range there.
* Disk utilization on busy nodes ranges from 15-30%, with no outliers above 28%.  (These are all 10-second averages or more.)  Average I/O time is largely under 1ms, and there are no outliers above 2.7ms.
* CockroachDB reports no range operations during this period (no splits, adds, removes, or merges).  There are no bad ranges.
* Heaertbeat latency has seen a few outliers above 1s, including one at about 4.5s, but the p99 has averaged about 15-30ms.
* Mean RPC clock offset has peaked at 270us, representing a maximum range of about 500us.

While digging into some other stuff, I found that this database is not as large as I had thought it was, and it may not be large enough to really represent "much bigger than DRAM".  On 9/22 I wrote that the table had reached 35 GiB, but I think that was measuring all copies of the data.  Over 5 nodes, that's only using about 7 GiB per node.  On the other hand, when I look at used space on db1's cockroachdb dataset, it's over 17 GiB.  That's true on db2 and db3, too.  But on db4 and db5 it's only 3-3.5 GiB.  That's about 57 GiB in total.  CockroachDB reports 22.7 GiB in use and confirms that three nodes have way more in use than the other two (although they all have about the same number of replicas).  The "kv" database is reported to be 20.5 GiB.  CockroachDB says that this is "across all replicas", so I'm not sure how it's possible that two nodes have so little space used.

As I mentioned in yesterday's notes (9/28), there were a few notable spikes in latency (and reductions in throughput). 

* At 9/26 16:00Z and 9/26 20:20Z, p95 spiked to 7ms across the cluster and throughput dropped to 1700 selects per second and 420 inserts per second.  These were short spikes.  When I zoom in on this 12-hour period, those two spikes don't look so large, but rather there are a bunch of smaller dips.  This wasn't correlated with anything obvious in the other graphs.
* At 9/29 10:42Z, there was a much bigger latency spike, with p95 shooting to 15ms and p99 shooting to 40ms.  Throughput correspondingly plummeted.  Latency and throughput seemed to go back to normal quickly.  However, from that point, store 12 started doing a lot less qps and the other stores did more.  CPU utilization and disk utilization shifted around a bit. There was also a big spike in network traffic -- only about 6-8 MBps, but that's 6-8 times what was happening before that.  There didn't appear to be any I/O problems.  There were a bunch of leaseholder changes, though -- db5 gave up 4 of its 17 leases.  Generally, the cluster became more balanced in terms of leaseholders.  p99 Heartbeat latency shot up briefly to 2.5s.  There were no SQL errors.


== 2020-09-30

Plan for the day:

* Review overnight results from NVME cluster
* Run expansion, contraction tests on the NVME cluster
* Review README to see what other experiments are worth doing before writing up results
* Review another day of the long-running workload?

=== Overnight workload on NVME cluster

When I got on around 15:30Z, I found that the monitoring VM ran out of disk space.  I fixed this by freeing up a small amount of space (removing the bootstrap stuff in `/var/tmp`), halting the VM, using Terraform to set the volume size from 10 GiB to 30 GiB, and then booting the VM again.  It came up with 20 GiB free.  Now that it's back up, based on when the metric graphs crashed, it looks to me like it ran out of disk space around 14:50Z -- I didn't miss it by that much (but I probably wouldn't have noticed anyway).

Ironically, I had tried to add a graph of disk space available on the monitoring VM yesterday, but I couldn't because the only Prometheus metrics we have for that right now come from CockroachDB, which isn't running there.

15:48Z: The NVME cluster database is at 73.7 GiB per node.  CockroachDB reports 221 GiB used -- right on for the three nodes.  This is plenty big enough.  I'm going to stop the "kv" workload running on the load generator and check the results.

It looks like one of them bombed out not long ago:

[source,text]
----
root@loadgen0:~/nvme# fuser *.out
1.out:      628o
2.out:      633o
3.out:
root@loadgen0:~/nvme# tail 3.out
54356.9s        0           27.1           36.7    142.6    285.2    369.1   1073.7 write
54416.9s        0           34.1           36.7     96.5    234.9    419.4    838.9 write
54476.9s        0           37.6           36.7     83.9    226.5    419.4    738.2 write
54536.9s        0           33.3           36.7     92.3    285.2    570.4    771.8 write
54596.9s        0           26.0           36.7    151.0    302.0    436.2    604.0 write
54656.9s        0           27.9           36.7    142.6    268.4    469.8    771.8 write
54716.9s        0           41.2           36.7     83.9    184.5    243.3    704.6 write
54776.9s        0           37.5           36.7     92.3    201.3    285.2    486.5 write
54836.9s        0           27.9           36.7    134.2    260.0    318.8    486.5 write
Error: ERROR: result is ambiguous (error=unable to dial n1: breaker open [exhausted]) (SQLSTATE 40003)
root@loadgen0:~/nvme# ls -l 3.out
-rw-------   1 root     root       86464 Sep 30 14:09 3.out
root@loadgen0:~/nvme#
----

That may be worth looking into.  The other two report no problems:

[source,text]
----
root@loadgen0:~/nvme# tail 1.out 2.out
==> 1.out <==
60416.6s        0           32.2           37.1     65.0    419.4    604.0   1040.2 write
60476.6s        0           26.8           37.1     67.1    453.0    570.4   1006.6 write
60536.6s        0           31.0           37.1     65.0    419.4    520.1    805.3 write
60596.6s        0           18.7           37.1    184.5    486.5    637.5    704.6 write
60656.6s        0           27.9           37.1     67.1    436.2    536.9    738.2 write
60716.6s        0           30.8           37.1     65.0    419.4    570.4    704.6 write
60776.6s        0           27.7           37.1     67.1    419.4    520.1    704.6 write
60836.6s        0           14.7           37.0    302.0    570.4    704.6   1409.3 write
60896.6s        0           24.7           37.0     71.3    469.8    604.0    838.9 write
60956.6s        0           24.7           37.0     67.1    503.3    637.5    704.6 write

==> 2.out <==
60416.6s        0           30.0           36.0     67.1    453.0    637.5    872.4 write
60476.6s        0           27.1           36.0     67.1    436.2    604.0    872.4 write
60536.6s        0           29.0           36.0     71.3    419.4    570.4    838.9 write
60596.6s        0           19.1           36.0    109.1    520.1    637.5    738.2 write
60656.6s        0           27.0           36.0     71.3    453.0    604.0    771.8 write
60716.6s        0           29.5           36.0     67.1    419.4    604.0    738.2 write
60776.6s        0           25.7           36.0     71.3    453.0    536.9    671.1 write
60836.6s        0           14.2           35.9    318.8    604.0    738.2    872.4 write
60896.6s        0           25.5           35.9     71.3    469.8    604.0    838.9 write
60956.6s        0           21.9           35.9     75.5    520.1    637.5    771.8 write
----

The overnight metrics show three spikes in error rate up to 0.05 errors per second.  Throughput per node and per store are reasonably balanced.  Latency was pretty okay until p95 started shooting up around 14:30Z.  I wonder if this is somehow related to the monitoring VM issue?  I don't see a corresponding increase in p99 in the CockroachDB adminui.  The increase in the Grafana graph may be some Prometheus/Grafana artifact?  I confirm that Prometheus just shows a big gap here, not a big slow climb.  I don't know why Grafana shows a big slow climb for the exact same query.

A few notes about the workload while it's running:

* CPU utilization was high on all db CPUs -- above 80% nearly all the time.
* Disk busyness increased slowly most of the time, but never exceeded about 30% utilization.  Average disk I/o was generally under 1ms, with occasional outliers up to 2ms.  Writing 50-100 MBps, which isn't bad, but not that big.  Reading about 50 MBps.
* There were some splits, not surprisingly, but these weren't correlated with the errors.
* Heartbeat latency was generally under 400ms, but did see excursions into the single-digit seconds up to about 4.5s.

Let's take a look at the balance of ranges per node:

[source,text]
----
root@loadgen0:~/nvme# cockroach sql --host 192.168.1.53 --execute 'select lease_holder, count(*) from [show ranges from database kv] group by lease_holder'
  lease_holder | count
---------------+--------
             1 |    93
             3 |    84
             2 |    79
(3 rows)

Time: 462.191277ms

root@loadgen0:~/nvme# cockroach sql --host 192.168.1.53 --execute 'select replicas,count(*) from [show ranges from database kv] group by replicas'
  replicas | count
-----------+--------
  {1,2,3}  |   256
(1 row)

Time: 438.805587ms
----

That's a good number of ranges and their leases are well distributed.  The "kv" database is reported to be 219.7 GiB.


=== Online expansion testing on NVME cluster

16:40Z: I'm now going to start some mixed read-write workloads, one for each current database node:

[source,text]
----
cockroach workload run kv --histograms kv-histograms-$(date +%FT%TZ).out --concurrency 4 --display-every=60s --read-percent 80 --tolerate-errors postgresql://root@192.168.1.53:26257/kv?sslmode=disable
cockroach workload run kv --histograms kv-histograms-$(date +%FT%TZ).out --concurrency 4 --display-every=60s --read-percent 80 --tolerate-errors postgresql://root@192.168.1.63:26257/kv?sslmode=disable
cockroach workload run kv --histograms kv-histograms-$(date +%FT%TZ).out --concurrency 4 --display-every=60s --read-percent 80 --tolerate-errors postgresql://root@192.168.1.181:26257/kv?sslmode=disable
----

1800Z: looking back at the last 80m or so:

* throughput: 1300 selects / sec + 330 inserts / sec
* latency: p95 ~ 12ms, p99 ~20ms
* balance: pretty even, both by node and by store
* no errors
* CPU utilization: 80-90% on all nodes
* disk utilization: 20-30% busy on all nodes
* disk I/O time: all under 400us
* all metrics crashed around 17:09: network I/O, disk I/O, throughput, CPU utilization.  It's not clear why.  There were no range operations.  There were some internal errors.  RT latency shot up above 5s.  It does look like there were a few lease transfers.

I think it might be a mistake to do this test with CPUs as busy as they are.  How can I change that?  I could make the test I/O bound, maybe by doing larger writes -- at a cost in latency and throughput, which probably would make it a little less consistent.  That seems worth poking at anyway.  I'm going to try it, replacing the load generators with:

[source,text]
----
cockroach workload run kv --max-block-bytes 1024 --min-block-bytes 1024 --histograms kv-histograms-$(date +%FT%TZ).out --concurrency 4 --display-every=60s --read-percent 80 --tolerate-errors postgresql://root@192.168.1.53:26257/kv?sslmode=disable
cockroach workload run kv --max-block-bytes 1024 --min-block-bytes 1024 --histograms kv-histograms-$(date +%FT%TZ).out --concurrency 4 --display-every=60s --read-percent 80 --tolerate-errors postgresql://root@192.168.1.63:26257/kv?sslmode=disable
cockroach workload run kv --max-block-bytes 1024 --min-block-bytes 1024 --histograms kv-histograms-$(date +%FT%TZ).out --concurrency 4 --display-every=60s --read-percent 80 --tolerate-errors postgresql://root@192.168.1.181:26257/kv?sslmode=disable
----

1K isn't that large, though, and it may not make a big difference in being I/O bound or not.  It could even make it worse, if we're memcpy'ing this around.

1804Z: applied this change.  Will observe for about 10 minutes.  If CPU utilization is still high, it may make more sense to just reduce the load generator concurrency.  This isn't supposed to be a major stress test so much as what happens when under moderate load and we expand or contract the cluster.

In the meantime, I'll provision the next three NVME db nodes with Terraform.  Ran into the DNS issue with that.

1816Z: the change in workload didn't really help.  CPU utilization is now a bit more spread out, which means some CPUs are actually busier.  I'm going to cut back the concurrency of the load generators instead (but I'll keep the block size change because that seems more representative of a real workload anyway).

1838Z: actually made the concurrency change at the load generators.

1929Z: since I made that change, CPU utilization did decrease and ranges from about 58% to 85%, which is still high, but has some headroom.  We'll give it a shot.  I don't want to reduce the concurrency much further.

1931Z: enabled CockroachDB on nvmedb4.

2038Z: summary of impact of that:

* initial crash in throughput, but not quite to zero. p95 from about 8ms to a 23ms.  p99 from about 18ms to 125ms.
* after that: about 30 minutes of degraded performance, lasting until about 20:03Z: p95 latency ranged from 15-25ms, p99 ranged from 30-80ms.  Throughput ranged from 150-300 qps per node (120 inserts and about 500 selects overall).
* balance: QPS per node remained pretty even throughout.  (I did not create a load generator for the new node so it didn't get any new queries.)  QPS per store also evened out by about 20:03Z.
* errors: no runtime errors reported by CockroachDB
* CPU utilization: spread out quite a bit, from 20% to 85%, from 19:31 to 20:03 or so.  After that, the bands became tighter than before, with most CPUs around 65%-75% utilization.  The new node, nvmedb4, is notably less busy than the others, around 40-50% per CPU.
* Disk utilization: aside from the new node, disk %busy spread out more, but it didn't really go up a whole lot, peaking around 25-30%.  After things settled down, the bands re-established around 13% and 9%.
* Disk I/O average latency: essentially unaffected in most nodes.  On nvmedb4, latency rose up to 3-4ms.
* Disk I/O: some nodes read read a lot more data than they were before (up to about 200 MBps at times).  nvmedb4 wrote a ton of data, up to about 200 MBps at times.
* Network I/O: many nodes did a lot more outbound network throughput (up to 30 MBps).  nvmedb4 was the only one with significant inbound throughput.
* Disk saturation: the device queue depth was largely 1 for most nodes except for nvmedb4, where it rose up to 11 at times.  Average I/O time queued in the driver was well under 15us.
* The "replicas per node" graph and "leaseholders per node" graph show a lot of rebalancing happening between 19:31 and 20:03 or so.  The leaseholders graph in particular shows a lot of erratic behavior, while the replicas graph is quite smooth.  We see a good number of adds and removes during this period, but no splits or merges.
* Heartbeat latency saw a few multi-second excursions, but they seemed to be largely outside the window where we were doing work.
* There were some internal errors for most of the rebalance period.
* The clock offset showed brief spikes much higher than we typically see, but still under about 3ms in variance between nodes.
* In terms of disk space used (as reported by CockroachDB): the existing nodes fell from 75.5 GiB to 59.2 GiB.  The new node went from zero to 56.6 GiB.

Overall, this seems pretty good.  Obviously there's some considerable degradation while rebalancing is underway.  However, we haven't tuned this system at all, not in terms of I/O (e.g., I/Os issued concurrently to devices, ZFS throttle), nor networking, nor any CockroachDB tunables, etc.

Let's do this a few more times.

I'm going to take this opportunity to use Joshua's new image that fixes the DNS problem (see "Debugging DNS issue" section from today).
Oops: I forgot that there's no way for me to tell Terraform to do this only for the last two nodes.  This took some cleaning up because one of the two new VMs ran into the DNS bug again.  I know how to workaround that now, but doing so clobbers the system's hostname.  Fixed.

21:09Z: enable cockroachdb on nvmedb5.

For the next round, I have the idea of going back up to concurrency 4 to see what happens.  I wonder whether the rebalancing activities increase latency, but do not decrease throughput capacity.  That is, latency goes up because there's more I/O going on and so some queueing, but we're not at the limits of CPU, disk, or network.

22:10Z: looking at impact.  As before:

* there's a long period of rebalancing after the node comes up.  This was from 21:09Z to 21:33Z.
* there's an initial big latency spike and throughput drop, but not nearly to zero throughput.  peak p95 = 20ms, peak p99 = 55ms.  This lasts about 4 minutes.
* during the rest of the rebalancing period, latency is elevated and throughput reduced, but not as severely.  peak p95 = 15ms, pea p99 = 40ms.
* balance is good in terms of queries per second per node and per store, particularly after rebalancing is done.
* There were no errors reported by CockroachDB (nor the clients).
* The rest of the metrics behaved similarly to the last restart.

22:15Z: I'm going to enable CockroachDB on nvmedb6.

22:26Z: We're partway through the rebalance period.  I'm going to double the concurrency of the load generators.

22:35Z: rebalancing is done.

As expected, this looks a lot like the previous rounds.  Critically, there were no errors, and p95/p99 was no worse than the other cases.  In this case, I increased concurrency on the clients partway through, and we can see that in the throughput numbers: they went up to about what they were before, though they were less stable.

I'm going to reduce the concurrency on the clients again in anticipation of shrinking the cluster.

22:44Z: reducing client concurrency back to two.

For more, see below section on "Online contraction"


=== Debugging DNS issue

I've found a few of the NVME nodes to come up with empty /etc/resolv.conf files.  I'm not sure why.  I added a workaround in vminit.sh for a temporary DNS issue, but this appears to require operator intervention.  Also, the workaround seems to have a bug: it seems to iterate much longer than 60 seconds.  It did eventually give up, though: I think the problem was really that the failures took a long time (13 seconds, in one case).

What about /etc/resolv.conf?  Well it hasn't been touched since Sep 7, which is long before I did this, and it's the same date this image was created:

[source,text]
----
root@ip-192-168-1-138:~/.ssh# ls -l /etc/resolv.conf
-rw-r--r--   1 root     root           0 Sep  7 08:51 /etc/resolv.conf
root@ip-192-168-1-138:~/.ssh#
----

The DNS server is in the DHCP info:

[source,text]
----
root@ip-192-168-1-138:~/.ssh# dhcpinfo DNSserv
192.168.0.2
root@ip-192-168-1-138:~/.ssh#
----

What normally creates that file?  Maybe we can guess from its timestamp:

[source,text]
----
root@nvmedb4:~# ls -lE /etc/resolv.conf
-rw-r--r--   1 root     root          57 2020-09-30 18:08:08.094641845 +0000 /etc/resolv.conf
root@nvmedb4:~#
----

That's after every SMF service has started:

[source,text]
----
root@nvmedb4:~# svcs | tail
online         18:08:11 svc:/milestone/multi-user-server:default
online         18:08:11 svc:/system/intrd:default
online         18:08:11 svc:/system/boot-config:default
online         18:08:11 svc:/system/zones:default
online         18:08:11 svc:/network/rpc/gss:default
online         18:08:11 svc:/network/security/ktkt_warn:default
online         18:08:14 svc:/system/update-man-index:default
online         18:08:55 svc:/application/node-exporter:default
online         18:08:55 svc:/application/illumos-exporter:default
online         18:09:02 svc:/network/chrony:default
root@nvmedb4:~# 
----

I had suspected maybe the metadata agent did this, but the `illumos/metadata` agent seems to do everything in its start method, and of course finished before that time too:

[source,text]
----
root@nvmedb4:~# less $(svcs -L illumos/metadata)
Sep 30 18:08:07.005 INFO no user-data?
Sep 30 18:08:07.006 INFO ----- WRITE FILE: /var/metadata/stamp ------ [
    "i-05af0ba567578e56e",
]
Sep 30 18:08:07.006 INFO ok, run complete
[ Sep 30 18:08:07 Method "start" exited with status 0. ]
----

We can look for services that wrote to their log file after that point:

[source,text]
----
root@nvmedb4:/var/svc/log# ls -lrtE
...
-rw-r--r--   1 root     root         162 2020-09-30 18:08:08.069886224 +0000 network-netmask:default.log
-rw-r--r--   1 root     root         286 2020-09-30 18:08:08.148259354 +0000 network-service:default.log
-rw-r--r--   1 root     root         125 2020-09-30 18:08:08.212350534 +0000 network-dns-client:default.log
-rw-r--r--   1 root     root         511 2020-09-30 18:08:08.238971476 +0000 system-name-service-cache:default.log
-rw-r--r--   1 root     root          80 2020-09-30 18:08:08.260438210 +0000 milestone-name-services:default.log
-rw-r--r--   1 root     root         174 2020-09-30 18:08:08.307891082 +0000 milestone-single-user:default.log
-rw-r--r--   1 root     root          80 2020-09-30 18:08:08.374340219 +0000 milestone-sysconfig:default.log
-rw-r--r--   1 root     root         205 2020-09-30 18:08:08.546184138 +0000 system-utmp:default.log
-rw-r--r--   1 root     root         115 2020-09-30 18:08:08.596736120 +0000 system-console-login:default.log
-rw-r--r--   1 root     root         185 2020-09-30 18:08:08.648592906 +0000 system-filesystem-local:default.log
-rw-r--r--   1 root     root         146 2020-09-30 18:08:08.788037812 +0000 system-sac:default.log
-rw-r--r--   1 root     root         170 2020-09-30 18:08:08.970424814 +0000 system-boot-archive-update:default.log
-rw-r--r--   1 root     root         204 2020-09-30 18:08:08.986720293 +0000 system-cron:default.log
-rw-r--r--   1 root     root         167 2020-09-30 18:08:09.479851668 +0000 network-shares-group:default.log
-rw-r--r--   1 root     root         165 2020-09-30 18:08:09.675849537 +0000 network-routing-legacy-routing:ipv4.log
-rw-r--r--   1 root     root         163 2020-09-30 18:08:09.762223579 +0000 network-shares-group:zfs.log
-rw-r--r--   1 root     root         165 2020-09-30 18:08:09.772953297 +0000 network-routing-route:default.log
-rw-r--r--   1 root     root         165 2020-09-30 18:08:09.834463578 +0000 network-routing-rdisc:default.log
-rw-r--r--   1 root     root         210 2020-09-30 18:08:09.856237220 +0000 network-rpc-bind:default.log
-rw-r--r--   1 root     root         120 2020-09-30 18:08:09.868664609 +0000 network-routing-legacy-routing:ipv6.log
-rw-r--r--   1 root     root         396 2020-09-30 18:08:09.935838304 +0000 network-routing-setup:default.log
-rw-r--r--   1 root     root         120 2020-09-30 18:08:09.944938547 +0000 network-routing-ripng:default.log
-rw-r--r--   1 root     root         212 2020-09-30 18:08:09.961278299 +0000 system-filesystem-autofs:default.log
-rw-r--r--   1 root     root         120 2020-09-30 18:08:09.978216364 +0000 network-ipv4-forwarding:default.log
-rw-r--r--   1 root     root         120 2020-09-30 18:08:10.022797764 +0000 network-ipv6-forwarding:default.log
-rw-r--r--   1 root     root         206 2020-09-30 18:08:10.135243259 +0000 system-system-log:default.log
-rw-r--r--   1 root     root         162 2020-09-30 18:08:10.156535793 +0000 system-dumpadm:default.log
-rw-r--r--   1 root     root         154 2020-09-30 18:08:10.464242805 +0000 system-fmd:default.log
-rw-r--r--   1 root     root         267 2020-09-30 18:08:10.999871774 +0000 network-ssh:default.log
-rw-r--r--   1 root     root        2787 2020-09-30 18:08:11.283491673 +0000 network-inetd-upgrade:default.log
-rw-r--r--   1 root     root         205 2020-09-30 18:08:11.332285930 +0000 network-inetd:default.log
-rw-r--r--   1 root     root         542 2020-09-30 18:08:11.480240435 +0000 milestone-multi-user:default.log
-rw-r--r--   1 root     root         144 2020-09-30 18:08:11.629491617 +0000 milestone-multi-user-server:default.log
-rw-r--r--   1 root     root         166 2020-09-30 18:08:11.654995907 +0000 system-boot-config:default.log
-rw-r--r--   1 root     root         205 2020-09-30 18:08:11.662760725 +0000 system-intrd:default.log
-rw-r--r--   1 root     root         211 2020-09-30 18:08:11.710819459 +0000 system-zones:default.log
-rw-r--r--   1 root     root         283 2020-09-30 18:08:14.613863809 +0000 system-update-man-index:default.log
-rw-r--r--   1 root     root        1504 2020-09-30 18:08:55.271910301 +0000 application-node-exporter:default.log
-rw-r--r--   1 root     root         302 2020-09-30 18:09:02.015323471 +0000 network-chrony:default.log
-rw-r--r--   1 root     root         165 2020-09-30 18:38:46.431883271 +0000 application-cockroachdb:default.log
-rw-r--r--   1 root     root       27753 2020-09-30 18:42:05.774647039 +0000 application-illumos-exporter:default.log
----

That's obviously quite a few.  dns-client looks relevant, but doesn't seem to do anything.  network-service's start method _does_ look relevant!  It rewrites /etc/resolv.conf.  We don't have XTRACE logs from this run, but if I evaluate it by hand, it does look like it should do the right thing.  I wonder if we hadn't fetched the DHCP params when it ran?  On the busted system, it ran at 18:07:53:

[source,text]
----
root@ip-192-168-1-138:~/.ssh# tail $(svcs -L network/service)
[ Sep 30 18:07:51 Enabled. ]
[ Sep 30 18:07:53 Executing start method ("/lib/svc/method/net-svc start"). ]
[ Sep 30 18:07:53 Timeout override by svc.startd.  Using infinite timeout. ]
[ Sep 30 18:07:53 Method "start" exited with status 0. ]
[ Sep 30 18:08:00 Rereading configuration. ]
[ Sep 30 18:08:00 No 'refresh' method defined.  Treating as :true. ]
root@ip-192-168-1-138:~/.ssh# 
----

What time did the illumos/metadata agent configure the xnf0 interface with DHCP?

[source,text]
----
root@ip-192-168-1-138:~/.ssh# cat $(svcs -L illumos/metadata)
[ Sep 30 18:07:50 Enabled. ]
[ Sep 30 18:07:53 Executing start method ("/usr/lib/metadata | tee /dev/msglog"). ]
...
Sep 30 18:07:57.989 INFO ENSURE IPv4 DHCP INTERFACE: xnf0
Sep 30 18:07:57.989 INFO ENSURE INTERFACE: xnf0
Sep 30 18:07:58.101 INFO INTERFACES: [
    "lo0",
]
Sep 30 18:07:58.101 INFO interface xnf0 NEEDS CREATION
Sep 30 18:07:58.195 INFO target IP name: xnf0/dhcp
Sep 30 18:07:58.218 INFO ADDRESSES: [IpadmAddress { name: "lo0/v4", type_: "static", state: "ok", cidr: "127.0.0.1/8" }, IpadmAddress { name: "lo0/v6", type_: "static", state: "ok", cidr: "::1/128" }]
Sep 30 18:07:58.218 INFO ipadm DHCP address NEEDS CREATION
Sep 30 18:08:03.554 INFO address state: Some(IpadmAddress { name: "xnf0/dhcp", type_: "dhcp", state: "ok", cidr: "192.168.1.138/24" })
Sep 30 18:08:03.554 INFO ok, interface xnf0 address 192.168.1.138/24 (dhcp) complete
----

This looks to me like a missing dependency -- network/service may not to start after illumos/metadata -- but I'm not sure that's correct either.

Joshua has fixed this issue in a new version of the AMI: `ami-01bb2865054e219c1`.


=== Online contraction testing on NVME cluster

This section resumes after the above testing on online expansion.

22:58Z: begin decommissionoing node 6 using:

[source,text]
----
root@loadgen0:~# COCKROACH_HOST=192.168.1.181 cockroach node status
  id |       address       |     sql_address     |                 build                  |            started_at            |            updated_at            | locality | is_available | is_live
-----+---------------------+---------------------+----------------------------------------+----------------------------------+----------------------------------+----------+--------------+----------
   1 | 192.168.1.53:26257  | 192.168.1.53:26257  | v20.2.0-alpha.1-1729-ge9c7cc561c-dirty | 2020-09-29 21:08:45.761724+00:00 | 2020-09-30 22:30:10.494985+00:00 |          | true         | true
   2 | 192.168.1.181:26257 | 192.168.1.181:26257 | v20.2.0-alpha.1-1729-ge9c7cc561c-dirty | 2020-09-29 21:08:46.553751+00:00 | 2020-09-30 22:30:12.317176+00:00 |          | true         | true
   3 | 192.168.1.63:26257  | 192.168.1.63:26257  | v20.2.0-alpha.1-1729-ge9c7cc561c-dirty | 2020-09-29 21:08:46.632391+00:00 | 2020-09-30 22:30:14.237919+00:00 |          | true         | true
   4 | 192.168.1.243:26257 | 192.168.1.243:26257 | v20.2.0-alpha.1-1729-ge9c7cc561c-dirty | 2020-09-30 19:30:58.847358+00:00 | 2020-09-30 22:30:13.533486+00:00 |          | true         | true
   5 | 192.168.1.123:26257 | 192.168.1.123:26257 | v20.2.0-alpha.1-1729-ge9c7cc561c-dirty | 2020-09-30 21:09:25.493063+00:00 | 2020-09-30 22:30:11.898783+00:00 |          | true         | true
   6 | 192.168.1.211:26257 | 192.168.1.211:26257 | v20.2.0-alpha.1-1729-ge9c7cc561c-dirty | 2020-09-30 22:14:51.639274+00:00 | 2020-09-30 22:30:14.112185+00:00 |          | true         | true
(6 rows)
root@loadgen0:~# time COCKROACH_HOST=192.168.1.181 cockroach node decommission 6
----

This took 11m16s.

23:26Z: started decommissioning node 5.

23:41Z: This took 14m.

I'm now going to shut down cockroachdb on nodes 6 and 5.  This should have no impact.  I'll wait 5 minutes to be sure.

23:47Z: begin decommissioning node 4

This took 22m41s and finished around 2020-10-01T00:10Z.

Through this whole process, the load generators have reported a total of 3 failed queries:

[source,text]
----
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
 1259.9s        0           90.3          365.5      7.9     30.4     71.3    738.2 read
 1259.9s        0           22.4           91.4     11.0    226.5    637.5    805.3 write
 1319.9s        0          113.0          354.1      6.0     24.1     79.7    939.5 read
 1319.9s        0           27.4           88.5      9.4    117.4    536.9    805.3 write
 1379.9s        0           95.4          342.8      7.3     30.4     75.5    738.2 read
 1379.9s        0           24.1           85.7     12.1    167.8    604.0   1208.0 write
 1439.9s        0           87.3          332.2      6.0     25.2     52.4  10200.5 read
 1439.9s        0           20.5           83.0     10.5    302.0    671.1  10200.5 write
E200930 23:08:39.587973 1 workload/cli/run.go:445  ERROR: result is ambiguous (error=unable to dial n5: breaker open [exhausted]) (SQLSTATE 40003)
 1499.9s        2           63.0          321.4      8.1     35.7    218.1   5100.3 read
 1499.9s        2           15.5           80.3     10.0     60.8    486.5   9126.8 write
 1559.9s        2          133.1          314.2      4.5     17.8     39.8   7247.8 read
 1559.9s        2           33.0           78.5      6.6     56.6    536.9    973.1 write
...
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
 4859.7s        2           72.0          247.7     11.5     39.8     83.9    352.3 read
 4859.7s        2           17.8           61.8     17.8    285.2    570.4    771.8 write
 4919.7s        2           62.1          245.5     12.6     44.0     83.9    536.9 read
 4919.7s        2           15.3           61.3     19.9    385.9    604.0   1208.0 write
 4979.7s        2           65.4          243.3     13.6     46.1     88.1    520.1 read
 4979.7s        2           17.3           60.7     19.9    268.4    520.1    805.3 write
 5039.7s        2           40.0          240.9     13.6     48.2    100.7   9126.8 read
 5039.7s        2           10.7           60.1     22.0    453.0    671.1  11274.3 write
E201001 00:08:29.690420 1 workload/cli/run.go:445  ERROR: result is ambiguous (error=unable to dial n1: breaker open [exhausted]) (SQLSTATE 40003)
 5099.7s        3           29.8          238.4     12.1     79.7    268.4   8053.1 read
 5099.7s        3            7.6           59.5     19.9    503.3    872.4  13421.8 write
 5159.7s        3           82.4          236.6      8.1     29.4     56.6    318.8 read
 5159.7s        3           21.7           59.1     11.5    385.9    671.1    771.8 write
...
----

These were all on the load generator pointed at n3 (192.168.1.181).

The impact was similar to expansion: some increased latency and throughput during the rebalancing period.

00:13Z: disable cockroachdb on nvmedb4.

0250Z: stopped workload

Also gathered updated graphs with updated Prometheus (see below).

=== Another review of long-running workload

For some reason, all the metrics went to zero from 2020-09-29T12:00 to 2020-09-30T20:00Z.  There was the monitoring issue mentioned above, where the monitoring zone ran out of disk space, but I thought that didn't affect the other cluster until 15:00 and it was resolved by 16:00.  As I look at the other cluster, though, I see a similar gap from 12:00 to 20:00Z.  That doesn't make sense, though -- I was using those metrics just a few hours ago!  I've confirmed it's just gone from Prometheus for that 8-hour block.  Looking at the Prometheus log, I do see it reporting corruption around 15:44Z and deleting records newer than the corrupted segment, which is probably not unreasonable.  However, I don't know why any data after that point would be missing.  There were some failed compactions after that:

[source,text]
----
[ Sep 30 15:44:03 Method "start" exited with status 0. ]
level=info ts=2020-09-30T15:44:04.334Z caller=main.go:343 msg="Starting Prometheus" version="(version=2.20.1, branch=HEAD, revision=983ebb4a513302315a8117932ab832815f85e3d2)"
level=info ts=2020-09-30T15:44:04.334Z caller=main.go:344 build_context="(go=go1.14.7, user=dap@ip-192-168-1-134.us-west-2.compute.internal, date=20200831-21:15:27)"
level=info ts=2020-09-30T15:44:04.335Z caller=main.go:345 host_details=(illumos)
level=info ts=2020-09-30T15:44:04.335Z caller=main.go:346 fd_limits="(soft=65536, hard=65536)"
level=info ts=2020-09-30T15:44:04.335Z caller=main.go:347 vm_limits="(soft=unlimited, hard=unlimited)"
level=error ts=2020-09-30T15:44:04.339Z caller=query_logger.go:71 component=activeQueryTracker msg="Failed to read query log file" err=EOF
level=info ts=2020-09-30T15:44:04.344Z caller=main.go:684 msg="Starting TSDB ..."
level=info ts=2020-09-30T15:44:04.344Z caller=web.go:524 component=web msg="Start listening for connections" address=0.0.0.0:9090
level=info ts=2020-09-30T15:44:04.355Z caller=repair.go:59 component=tsdb msg="Found healthy block" mint=1599606341347 maxt=1599688800000 ulid=01EHXHP2KD96XQYKHQQ55JFWCG
level=info ts=2020-09-30T15:44:04.358Z caller=repair.go:59 component=tsdb msg="Found healthy block" mint=1599775200000 maxt=1600300800000 ulid=01EJGX72VDPKAVDHTNQPD1YHJ8
level=info ts=2020-09-30T15:44:04.360Z caller=repair.go:59 component=tsdb msg="Found healthy block" mint=1600300800000 maxt=1600884000000 ulid=01EJYC8A6J90S612N76YPK71J0
level=info ts=2020-09-30T15:44:04.363Z caller=repair.go:59 component=tsdb msg="Found healthy block" mint=1600884000000 maxt=1601078400000 ulid=01EK45MYJ9VZHB61XEJFGGCWJH
level=info ts=2020-09-30T15:44:04.365Z caller=repair.go:59 component=tsdb msg="Found healthy block" mint=1601078400000 maxt=1601272800000 ulid=01EK9Z1WVHX606D5Q9X50GDZQS
level=info ts=2020-09-30T15:44:04.369Z caller=repair.go:59 component=tsdb msg="Found healthy block" mint=1601272800000 maxt=1601337600000 ulid=01EKBWV3YC0A0BCGBHN62W2D92
level=info ts=2020-09-30T15:44:04.371Z caller=repair.go:59 component=tsdb msg="Found healthy block" mint=1601337600000 maxt=1601402400000 ulid=01EKDTMNM5AH6K827A5E6T4AXB
level=info ts=2020-09-30T15:44:04.373Z caller=repair.go:59 component=tsdb msg="Found healthy block" mint=1601402400000 maxt=1601424000000 ulid=01EKEF7RSGVAT2TQ4KMZ3ABDEM
level=info ts=2020-09-30T15:44:04.377Z caller=repair.go:59 component=tsdb msg="Found healthy block" mint=1601445600000 maxt=1601452800000 ulid=01EKF3TT08JDJHPK92WNAR0DMM
level=info ts=2020-09-30T15:44:04.379Z caller=repair.go:59 component=tsdb msg="Found healthy block" mint=1601424000000 maxt=1601445600000 ulid=01EKF3TZ6TKK4K7QCT57CSP4FN
level=info ts=2020-09-30T15:44:04.382Z caller=repair.go:59 component=tsdb msg="Found healthy block" mint=1601452800000 maxt=1601460000000 ulid=01EKFAPH4VS7R183BPX43XWGQQ
level=info ts=2020-09-30T15:44:04.384Z caller=repair.go:59 component=tsdb msg="Found healthy block" mint=1601460000000 maxt=1601467200000 ulid=01EKFHJ8DGN5NR4SZ52CE6QMZB
level=info ts=2020-09-30T15:44:04.646Z caller=head.go:641 component=tsdb msg="Replaying on-disk memory mappable chunks if any"
level=error ts=2020-09-30T15:44:05.660Z caller=head.go:646 component=tsdb msg="Loading on-disk chunks failed" err="iterate on on-disk chunks: out of sequence m-mapped chunk for series ref 294163"
level=info ts=2020-09-30T15:44:05.661Z caller=head.go:757 component=tsdb msg="Deleting mmapped chunk files"
level=info ts=2020-09-30T15:44:05.661Z caller=head.go:760 component=tsdb msg="Deletion of mmap chunk files failed, discarding chunk files completely" err="cannot handle error: iterate on on-disk chunks: out of sequence m-mapped chunk for series ref 294163"
level=info ts=2020-09-30T15:44:05.661Z caller=head.go:655 component=tsdb msg="On-disk memory mappable chunks replay completed" duration=1.01411139s
level=info ts=2020-09-30T15:44:05.661Z caller=head.go:661 component=tsdb msg="Replaying WAL, this may take a while"
level=info ts=2020-09-30T15:44:05.912Z caller=head.go:687 component=tsdb msg="WAL checkpoint loaded"
level=info ts=2020-09-30T15:44:08.724Z caller=head.go:713 component=tsdb msg="WAL segment loaded" segment=365 maxSegment=370
level=info ts=2020-09-30T15:44:12.372Z caller=head.go:713 component=tsdb msg="WAL segment loaded" segment=366 maxSegment=370
level=info ts=2020-09-30T15:44:16.634Z caller=head.go:713 component=tsdb msg="WAL segment loaded" segment=367 maxSegment=370
level=warn ts=2020-09-30T15:44:20.404Z caller=db.go:634 component=tsdb msg="Encountered WAL read error, attempting repair" err="read records: corruption in segment /mon/prometheus/var/data/wal/00000368 at 121864192: last record is torn"
level=warn ts=2020-09-30T15:44:20.404Z caller=wal.go:354 component=tsdb msg="Starting corruption repair" segment=368 offset=121864192
level=warn ts=2020-09-30T15:44:20.405Z caller=wal.go:362 component=tsdb msg="Deleting all segments newer than corrupted segment" segment=368
level=warn ts=2020-09-30T15:44:20.405Z caller=wal.go:384 component=tsdb msg="Rewrite corrupted segment" segment=368
level=info ts=2020-09-30T15:44:21.660Z caller=main.go:700 fs_type=unknown
level=info ts=2020-09-30T15:44:21.661Z caller=main.go:701 msg="TSDB started"
level=info ts=2020-09-30T15:44:21.661Z caller=main.go:805 msg="Loading configuration file" filename=/mon/prometheus/prometheus.yml
level=info ts=2020-09-30T15:44:21.757Z caller=main.go:833 msg="Completed loading of configuration file" filename=/mon/prometheus/prometheus.yml
level=info ts=2020-09-30T15:44:21.757Z caller=main.go:652 msg="Server is ready to receive web requests."
level=info ts=2020-09-30T15:44:25.478Z caller=compact.go:495 component=tsdb msg="write block" mint=1601467200000 maxt=1601474400000 ulid=01EKFTZ7BJNE8DB6GRN36N7DWW duration=3.091098927s
level=info ts=2020-09-30T15:44:25.599Z caller=head.go:804 component=tsdb msg="Head GC completed" duration=30.681058ms
level=error ts=2020-09-30T15:44:25.600Z caller=db.go:685 component=tsdb msg="compaction failed" err="reload blocks: head truncate failed: truncate chunks.HeadReadWriter: maxt of the files are not set"
level=info ts=2020-09-30T17:00:01.620Z caller=compact.go:495 component=tsdb msg="write block" mint=1601474400000 maxt=1601481600000 ulid=01EKFZ9PR4N2EZCR9QS0SN7BYR duration=1.48784649s
level=info ts=2020-09-30T17:00:01.751Z caller=head.go:804 component=tsdb msg="Head GC completed" duration=36.009434ms
level=error ts=2020-09-30T17:00:01.753Z caller=db.go:685 component=tsdb msg="compaction failed" err="reload blocks: head truncate failed: truncate chunks.HeadReadWriter: maxt of the files are not set"
level=info ts=2020-09-30T19:00:03.327Z caller=compact.go:495 component=tsdb msg="write block" mint=1601481600000 maxt=1601488800000 ulid=01EKG65DX7A5B74KGCDGBZCX7Z duration=3.288402215s
level=info ts=2020-09-30T19:00:03.483Z caller=head.go:804 component=tsdb msg="Head GC completed" duration=33.100722ms
level=error ts=2020-09-30T19:00:03.483Z caller=db.go:685 component=tsdb msg="compaction failed" err="reload blocks: head truncate failed: truncate chunks.HeadReadWriter: maxt of the files are not set"
level=info ts=2020-09-30T21:00:04.981Z caller=compact.go:495 component=tsdb msg="write block" mint=1601488800000 maxt=1601496000000 ulid=01EKGD1615A94M0XTXHHPGNFPZ duration=4.047633854s
level=info ts=2020-09-30T21:00:05.163Z caller=head.go:804 component=tsdb msg="Head GC completed" duration=43.65002ms
level=error ts=2020-09-30T21:00:05.163Z caller=db.go:685 component=tsdb msg="compaction failed" err="reload blocks: head truncate failed: truncate chunks.HeadReadWriter: maxt of the files are not set"
level=error ts=2020-09-30T21:09:52.550Z caller=api.go:1486 component=web msg="error writing response" bytesWritten=0 err="write tcp [::1]:9090->[::1]:34017: write: broken pipe"
level=error ts=2020-09-30T21:09:52.566Z caller=api.go:1486 component=web msg="error writing response" bytesWritten=0 err="write tcp [::1]:9090->[::1]:38283: write: broken pipe"
level=error ts=2020-09-30T21:09:52.592Z caller=api.go:1486 component=web msg="error writing response" bytesWritten=0 err="write tcp [::1]:9090->[::1]:48519: write: broken pipe"
level=error ts=2020-09-30T21:09:52.606Z caller=api.go:1486 component=web msg="error writing response" bytesWritten=0 err="write tcp [::1]:9090->[::1]:58922: write: broken pipe"
level=info ts=2020-09-30T23:00:04.628Z caller=compact.go:495 component=tsdb msg="write block" mint=1601496000000 maxt=1601503200000 ulid=01EKGKWXFBP5VS22J30PS0GA4G duration=3.49728677s
level=info ts=2020-09-30T23:00:04.827Z caller=head.go:804 component=tsdb msg="Head GC completed" duration=51.64078ms
level=error ts=2020-09-30T23:00:04.827Z caller=db.go:685 component=tsdb msg="compaction failed" err="reload blocks: head truncate failed: truncate chunks.HeadReadWriter: maxt of the files are not set"
----

A https://github.com/prometheus/prometheus/issues/7245#issuecomment-646753932[similar experience is reported here].  Here's https://github.com/prometheus/prometheus/issues/5867[a related issue].  This is extremely frustrating.

On the plus side, we do have the graphs from CockroachDB for this time.  They show consistent throughput and latency for the last 24 hours.

We also seem to be hitting https://github.com/prometheus/prometheus/issues/7753[this issue with compaction].  This actually gives a promising way forward.

0250Z update: updating to Prometheus 2.21 has gotten the data back!  It wasn't lost -- just unavailable.  I've updated the screenshots.

=== Next steps

* Review README for any more tests that we want to do now.  If none, start preparing write-up of results so far.
* Shut down clusters!

== 2020-10-01

I've got a bunch of other stuff to do over the next few days, so I don't think it makes sense to keep the NVME cluster running.  I'm going to first reboot a node to see what happens (do we lose the local instance storage?  I assume so.)  Then I'm planning to undeploy them all.

Yes, we do lose the local NVME device contents when we bounce the instance.  The failure mode is that we know about the "tank" pool, but ZFS reports it as faulted due to corrupt data.

16:40Z: destroyed the whole NVME cluster and db6 from the other cluster (which hasn't been used in it for a while)

== 2020-10-05

15:22Z: The workload on the non-NVME cluster has been running for over a week.  I've confirmed that it's still running, and running about the same, so I'm going to shut down the load generators and summarize the status.

Recall that I initially expected to have a `N.out` summary file for each load generator (N=1 through N=5), but by mistake I clobbered 2.out and 3.out last week.  However, I _do_ have per-second histogram files for all five clients.  We can see these as the most recently updated files:

[source,text]
----
root@loadgen0:~# ls -lrt
total 91561
...
-rw-r--r--   1 root     root        5030 Sep 25 15:45 kv-histograms-2020-09-25T15:43:35Z.out
-rw-r--r--   1 root     root        6774 Sep 25 15:47 kv-histograms-2020-09-25T15:43:57Z.out
drwxr-xr-x   3 root     root          21 Sep 30 22:44 nvme
-rw-------   1 root     root     2721308 Oct  5 15:22 4.out
-rw-r--r--   1 root     root     24477624 Oct  5 15:22 kv-histograms-2020-09-25T15:45:34Z.out
-rw-------   1 root     root     2720618 Oct  5 15:22 5.out
-rw-r--r--   1 root     root     24501003 Oct  5 15:22 kv-histograms-2020-09-25T15:45:45Z.out
-rw-------   1 root     root     2721218 Oct  5 15:23 1.out
-rw-r--r--   1 root     root     24408415 Oct  5 15:23 kv-histograms-2020-09-25T15:44:53Z.out
-rw-r--r--   1 root     root     24402901 Oct  5 15:23 kv-histograms-2020-09-25T15:45:01Z.out
-rw-r--r--   1 root     root     24472346 Oct  5 15:23 kv-histograms-2020-09-25T15:45:22Z.out
----

We can use these to find and kill the five load generators:

[source,text]
----
root@loadgen0:~# fuser 4.out 5.out 1.out kv-histograms-2020-09-25T15:45:01Z.out kv-histograms-2020-09-25T15:45:22Z.out
4.out:      804o
5.out:      810o
1.out:      783o
kv-histograms-2020-09-25T15:45:01Z.out:      790o
kv-histograms-2020-09-25T15:45:22Z.out:      797o
root@loadgen0:~# pargs 804 810 783 790 797
804:    cockroach workload run kv --histograms kv-histograms-2020-09-25T15:45:34Z.out -
argv[0]: cockroach
argv[1]: workload
argv[2]: run
argv[3]: kv
argv[4]: --histograms
argv[5]: kv-histograms-2020-09-25T15:45:34Z.out
argv[6]: --concurrency
argv[7]: 2
argv[8]: --display-every=60s
argv[9]: --read-percent
argv[10]: 80
argv[11]: --tolerate-errors
argv[12]: postgresql://root@192.168.1.252:26257/kv?sslmode=disable

810:    cockroach workload run kv --histograms kv-histograms-2020-09-25T15:45:45Z.out -
argv[0]: cockroach
argv[1]: workload
argv[2]: run
argv[3]: kv
argv[4]: --histograms
argv[5]: kv-histograms-2020-09-25T15:45:45Z.out
argv[6]: --concurrency
argv[7]: 2
argv[8]: --display-every=60s
argv[9]: --read-percent
argv[10]: 80
argv[11]: --tolerate-errors
argv[12]: postgresql://root@192.168.1.18:26257/kv?sslmode=disable

783:    cockroach workload run kv --histograms kv-histograms-2020-09-25T15:44:53Z.out -
argv[0]: cockroach
argv[1]: workload
argv[2]: run
argv[3]: kv
argv[4]: --histograms
argv[5]: kv-histograms-2020-09-25T15:44:53Z.out
argv[6]: --concurrency
argv[7]: 2
argv[8]: --display-every=60s
argv[9]: --read-percent
argv[10]: 80
argv[11]: --tolerate-errors
argv[12]: postgresql://root@192.168.1.24:26257/kv?sslmode=disable

790:    cockroach workload run kv --histograms kv-histograms-2020-09-25T15:45:01Z.out -
argv[0]: cockroach
argv[1]: workload
argv[2]: run
argv[3]: kv
argv[4]: --histograms
argv[5]: kv-histograms-2020-09-25T15:45:01Z.out
argv[6]: --concurrency
argv[7]: 2
argv[8]: --display-every=60s
argv[9]: --read-percent
argv[10]: 80
argv[11]: --tolerate-errors
argv[12]: postgresql://root@192.168.1.14:26257/kv?sslmode=disable

797:    cockroach workload run kv --histograms kv-histograms-2020-09-25T15:45:22Z.out -
argv[0]: cockroach
argv[1]: workload
argv[2]: run
argv[3]: kv
argv[4]: --histograms
argv[5]: kv-histograms-2020-09-25T15:45:22Z.out
argv[6]: --concurrency
argv[7]: 2
argv[8]: --display-every=60s
argv[9]: --read-percent
argv[10]: 80
argv[11]: --tolerate-errors
argv[12]: postgresql://root@192.168.1.121:26257/kv?sslmode=disable
root@loadgen0:~# kill 804 810 783 790 797
root@loadgen0:~# 
----

All the data that I'm saving will go into this repo, data/2020-10-05-long-workload.  I've saved the complete cluster settings into "cluster_settings.txt".  I've copied the load generator client-side output files there as well.

That did it -- throughput is now zero.

The workload covers the range 2020-09-26T01:00Z (which is after the futzing I had done with the cluster that day) through 2020-10-05T15:00 (which is before I shut down the workload).  This covers 9d14h (240 hours).

I collected:

* screenshots from CockroachDB's AdminUI
* the raw client data from the load generator
* cluster settings from the database itself

I've created a new Grafana dashboard for this workload to work around the change in metric labeling that otherwise makes it impossible to show the full workload duration in a single graph.  I also tweaked it a bit (e.g., de-selected db6, which wasn't being used during this workload) to better show the data.  I've got screenshots of this for the whole duration.

I've shut down the CockroachDB, load generator, and monitoring nodes because I believe we don't need these any more.  I'm not going to undeploy them yet just in case there's useful data there.

== 2020-10-15

Did some final work for the initial write-up.

I took new screenshots from the 9/24 work because the ones I had were not from a consistent point in time, and plus the dashboard had been updated a bunch since then.

I looked for CockroachDB crashes that were _not_ related to time synchronization.  To find crashes, I looked for:

[source,text]
----
grep 'Stopping because all processes in service exited.' $(svcs -L cockroachdb)
----

That gave me the timestamps when CockroachDB exited.  Then I looked for the string '500ms' in '/cockroachdb/data/logs/*'.

Here's what I found:

[source,text]
----
db1:
Oct 15 (ignored -- postdates all my testing)

db2:
Sep 23 23:11, 23:25 (x4) (rotated out)
Sep 24 15:55:18 (rotated out)

db3:
Sep 23 23:25 (x2) (rotated out)
Sep 24 15:55 (x4) (confirmed time sync)
Sep 25 14:54 (x4) (confirmed time sync)

db4:
Oct 15 (ignored -- postdates all my testing)

db5:
Oct 15 (ignored -- postdates all my testing)
----

In summary, we don't have a lot of data to confirm this.  We're missing data from nodes that we destroyed (like the NVME cluster) and from log files that were rotated out.  However, we don't have any evidence of crashes that _weren't_ related to time sync.

From what I've seen, when CockroachDB panics, it just exits rather than dumping core, so that's why I looked for that.  For good measure, I also looked for "core" in the SMF log files on all five nodes, and I also looked in `/` for core files, and I found none.


== 2020-10-20

Notes from the last few days of making this process more reproducible.

The main thing to fix was adding documentation and packaging up all the illumos binaries and related files so that others could easily use them.  Previously, these were just untracked files in git.  I initially thought I'd use a submodule -- that way, they'd be version-controlled, but people wouldn't necessarily _need_ to download them to deploy a basic cluster (if they were using the same S3 bucket I've been using and want to just use the same binaries I'd already uploaded there).  This didn't work because GitHub has a limit on file size of something like 100 MiB.

I wound up just putting these files into a "proto-raw.tgz" tarball and uploading them to the S3 bucket that I've been using.

I ran into a bunch of issues along the way:

* I had never tested my fix to modify the "db" deployment to use external EBS volumes.  This had 1-2 small issues with it.
* I couldn't create a parallel Terraform universe because some of my Terraform resources are singletons within an account and I didn't want to destroy my existing resources.  Since Adam is already moving our AWS infrastructure to separate accounts, I swithced to using a new account for this work.  I ran into several issues around this (but they were good to invest in fixing):
** My account needed to have multi-factor auth (MFA) enabled.  Without this, the attempt to add a new role in the Console just fails with a generic security error.
** There was some confusing in configuring the CLI to use a new profile using this role.  With this resolved, I can set AWS_PROFILE=oxide-sandbox-dap in order to run commands under the new account.
** Terraform doesn't handle running under a role with MFA, so we use https://github.com/transposit/awscli-as-session[awscli-as-session] to manage this.  This sort of thing (if not this exact implementation) seems to be the way to do this.
** I ran into various IAM issues, needing to create S3 buckets, etc. that I either hadn't run into before or forgot about.  For example, the AMI images I've been using weren't available to my new account.
** Another example is that the instances weren't able to read from my main bucket.  Adam later told me this may be because the policy that would allow them to do so is missing the account number from the ARN identifying the bucket.  Anyway, I decided to work around this problem by having the user (that's following the README) create their own S3 bucket for storing the VM artifacts.  This is a little more annoying, but it has the nice property that people aren't going to clobber each others' binaries.  (It's not that I expect people to be doing this a lot, but I could definitely see a person doing this for the first time accidentally copying the same binaries that I'm using.)
* With the AWS issues out of the way, my deployments kept running into the same issue I'd seen previously where `/etc/resolv.conf` is empty on startup.  I talked with Joshua about this and he made a new image for me to use.

I've now tested the full deployment, following the README instructions, to deploy both non-NVME "db" instances and NVME "nvmedb" instances and all that works.



Now, trying to convince myself I haven't left state in AWS somewhere.  The reason is that on Friday:

> I think I have landed in some sort of Terraform/AWS hell...I wanted to create a totally parallel world for testing...I changed the terraform state S3 key to something else, figuring it would not find anything and try to recreate a parallel world.  And it did...but it failed because of a name conflict, and I didn't look carefully at the message, and I assumed it was because some of these resources were given names that had to be unique in the AZ, so I changed my config to be in us-west-1 for a minute and tried again...but the real problem was that the IAM roles have to be unique...and those are AWS-wide...So I've presumably got some resources in us-west-1 and us-west-2...I just did a successful terraform destroy in us-west-1, so I think that probably got everything there...Now that I did the destroy, I guess it clobbered its state file, so I can't even refresh to see what it finds (which makes sense, I guess -- it would only find what was in the state)...

Within my new account:

* there are now no S3 buckets
* prior to removing the S3 buckets, I checked for Terraform state files.  I made sure that I had `terraform destroy`ed everything.
* I confirmed that there are no non-terminated Instances.  There are various VPC objects, but they seem to be the default VPC, default subnet, etc. using different IP ranges than I had been using, so I don't believe these were created by Terraform.

In the main account (aka my old account):

* There *are* resources, mostly shutdown, from my last run.  At the very least, I want to keep the "mon" VM around for a while because we might want to revisit the raw data.
* As a result, there's also Terraform state in bucket "oxide-terraform-backend" key "crdb-exploration".
* The "oxide-cockroachdb-exploration" bucket exists and includes "vminit" tarballs (a few weeks old at this point), plus proto-raw.tgz and the original CockroachDB binaries "cockroachdb.tar.gz".
* In us-west-1, I don't see any EC2 resources (instances, volumes, etc.), VPC resources (elastic IPs, non-default security groups, etc.), S3 buckets, or IAM resources that came from Terraform.

That's all fine.  Here's what might need cleanup: there's a Terraform state file called "crdb-exploration-test" in the oxide-terraform-backend bucket.  I believe this one I created on Friday as part of my aborted attempt to create a parallel universe for testing.  However, its contents are empty.  I'm just going to remove the file.

What about any state left behind?  There are three versions of this file.  The last one is empty.  So there are only two to consider.

First one (serial 0):

* resources with no instances: instance profile, iam role policy, "db" instance, "db_nvme" instance, "loadgen" instance, "mon" instance, gateway
* resources with instances: gateway, route, security group, security group rule, security group rule, security group rule, subnet, vpc.  The ones with ARNs all are in west-2.

Second one (serial 1):

* resources with no instances: instnace profile, iam role policy (x2), "db" instance, "db_nvme" instance, "loadgen" instance, "mon" instance
* resources with instances: gateway (west-1), route, security group (west-1), security group rule (x3), vpc (west-1).

I believe none of these resources costs money, anyway.

I also went through the Terraform output by hand and confirmed that it didn't appear to try to create any resources other than the ones named above, and it appears to have destroyed all of those, at least in west-1 (the second attempt).  I _don't_ think I ever ran "terraform destroy" on west-2, probably because many of these resources overlapped with the ones in use by my primary cluster.

I've now confirmed that there _is_ a second VPC resource in west-2 for "crdb_exploration".  I'm going to go ahead and try to remove these by:

(1) cloning this repo into a "cleanup" version
(2) resurrecting the Terraform config that I had deleted for "crdb-exploration-test" that referred to resources in west-2 (this would be serial 0, I believe)
(3) modifying the Terraform config in the clone to point to that Terraform state file
(4) run "terraform destroy" to destroy those resources.
(5) check that they seem to be gone.
(6) clean up my "cleanup" clone.  Remove Terraform state file from S3 bucket.

I did all this successfully.

For future reference, I'm going to put the raw Terminal transcripts for both the mess-up session and the cleanup session into this repo with this commit, but I'll remove it shortly after since it's not useful unless we find something was lost and want to understand why.


== 2020-10-26

Summary of the day:

- reviewed data from expansion/contraction testing on NVME with large database size to summarize min workload throughput
- reviewed data from fault testing on non-NVME to summarize min workload throughput

=== Expansion/contraction testing

Today I want to better characterize the results of some of the cluster expansion/contraction testing.

As part of this, I dug into Prometheus, Grafana, choice of rate intervals, etc.  This is a real rabbit hole.  As part of this, I discovered that Grafana had assumed a default Prometheus scrape interval of 15s, but we're using 10s.  However, I can't correct this because:

> This datasource was added by config and cannot be modified using the UI. Please contact your server admin to update this datasource.

This was because I marked this datasource uneditable in the config because it would be clobbered by a subsequent update.

It took some time but I figured out how to update this in an existing config (adding a deleteDatasource to the same provisioning file).  I also figured out the right config for specifying that the Prometheus scrape interval is 10s.

Now let's get to the experiment and the data.

Key questions:

* How many requests are _actually_ completing?
* How badly is latency impacted?

Timeline:

    2020-09-30T19:31Z Enabled CockroachDB node 4
    2020-09-30T21:09Z Enabled CockroachDB node 5
    2020-09-30T22:15Z Enabled CockroachDB node 6
    2020-09-30T22:26Z Increase load generator concurrency from 2 per node to 4 per node
    2020-09-30T22:44Z Reduce load generator concurrency back to 2 per node
    2020-09-30T22:58Z Begin decommissioning node 6 (took 11m)
    2020-09-30T23:26Z Begin decommissioning node 5 (took 23m)
    2020-09-30T23:47Z Begin decommissioning node 4 (took 23m)

General note: metrics are scraped every 10s

Initial conditions: 19:20Z to 19:30Z (10 minutes):

* metric note: used 40 second averages, consistent with recommendations based on 4x scrape interval
* throughput: 1148 rps (avg), 1031 rps (min) to 1254 rps (max)
** selects: 918 rps (avg)
** inserts: 230 rps (avg)
** per-node: 327-355 rps (range of averages)
** There's a clear oscillation with a period of about 90s
* latency:
** p95: 8.4ms (avg of avgs), 7.4ms (min) to 10ms (max)
** p99: 16ms (avg of avgs), 13ms (min) to 20ms (max)
* CPU utilization: total range 51%-92%, avg range 64-73%
* Disk utilization: peaked at 43%, avg 18%-20%
* Disk I/O:
** writes: 1.8 - 100.5 MBps writes (avg of avgs: 13.1 MBps)
** reads: 57-108 MBps reads (avg of avgs: 84 MBps)

Next interval: after bringing up node 4: 19:31Z to 19:41Z (10 minutes):

* throughput: 638 rps; minimum 246 rps @ 19:32Z
** selects: 510 rps (avg); minimum 196 rps
** inserts: 128 rps (avg); minimum 50 rps
** per-node: 198-221 rps (range of averages)
* latency:
** p95: 13ms (avg of avgs)
** p99: 54ms (avg of avgs)
* CPU utilization peaked at 94% (may reflect periods of saturation)
* Disk utilization peaked at 71%
* Disk average I/O time peaked at 3.6ms -- all data points above 2.4ms were on the new node

Next interval: before bringing up node 5: 20:58Z to 21:08Z (10 minutes):

* throughput: 1431 rps (avg), 1270 rps (min) to 1520 rps (max)
** selects: 1145 rps (avg)
** inserts: 286 rps (avg)
** per-node: 460-485 (avg) -- nodes 1-3 only (node 4 not being used as a gateway)
** There's still a clear oscillation like before.
* latency:
** p95: 6.2ms (avg of avgs), 5.5ms (min) to 6.9ms (max)
** p99: 11ms (avg of avgs), 8.5ms (min) to 13.7ms (max)
* CPU utilization: avg range 42%-68% (a little less than before)

At this point, the data summary is pretty time-consuming, and I'm not sure we care about this level of detail.  I'm going to focus on getting graphs of the transitions and documenting the minimum values.

The following summarize minimum throughput in the 20 minutes surrounding each event (about 10 minutes on either side):

Bringing up node 4: min throughput = 196rps +  50rps = 246 rps
Bringing up node 5: min throughput = 488rps + 121rps = 609 rps
Bringing up node 6: min throughput = 685rps + 167rps = 852 rps
Decommission node 6: min throughput = 237rps + 61rps = 298 rps
Decommission node 5: min throughput = 123rps + 33rps = 156 rps (very high CPU utilization)
Decommission node 4: min throughput = 169rps + 44rps = 213 rps

I want to see how fine-grained a graph I can get without sacrificing any data points.  There are 563 data points per graph with a min interval of 10s, or 93 minutes.  That's enough to cover the decommissioning set, but not the bringup set.

I saved a screenshot of the decommission set from 22:50Z to 00:20Z.  There are two worrisome periods here that weren't captured above:

23:09Z (during decommission n6): 78rps + 19rps = 97rps
00:08Z (during decommission n4): 33rps +  8rps = 41rps

I looked at longer windows of time after each bring-up.

- 19:30Z - 20:00Z: worst part captured above
- 21:08Z - 21:38Z: worst part captured above
- 22:15Z - 22:58Z: slightly worse data point @ 22:23:40Z @ 672 rps + 165 rps = 837 rps

=== Fault testing

Recall this was the non-NVME database.

Created a more fine-grained graph for the period 2020-09-25T19:50Z to 2020-09-25T21:20Z.  Interval is still 10s, which is good.  These are 40s rate intervals.

Looking at the graph, the only visible decreases in throughput are around 20:45Z (1296 rps + 328rps = 1624 rps) and 20:52Z (min for the whole period, 982 rps + 250 rps = 1232 rps).  Timeline:

19:55Z: SIGKILL
20:02Z: SIGKILL
20:08Z: SIGKILL
20:19Z: SIGKILL
20:44Z: OS reboot (one of the events noted above)
20:51Z: OS panic (one of the events noted above)

I then looked at a graph from 23:40Z to 00:40Z (1 hour) to cover the partition testing.

The throughput impacts are a little more complicated to summarize in writing.  The minimum throughput dropped to two notable minima:

00:08Z: 693 rps + 177 rps = 870 rps (corresponds with 5 minutes into the long partition)
00:31Z: 736 rps + 178 rps = 914 rps (corresponds with _end_ of the long partition)

These are the two periods where lots of rebalancing would have kicked off.

TODO:

* caveat with all of the above: 40-second averages.  Any black hole shorter than that would not be visible.
* look for any client data to address ^
* organize screenshots that I took
* update RFD 110


== 2020-10-27

Time to look at client data.  I have the three output files: 1.out, 2.out, 3.out.  These have mtimes 2020-09-30T18:04Z.  From 1.out, I subtracted the last data point (4941.6s) to calculate the actual start time, which was 16:42:02Z.  That matches my notes.  The problem is that this was for a load generator run _before_ the expansion testing -- indeed, the mtime alone tells us that they stopped running well before the real testing started.  (This is consistent with my notes, too.)  This is unfortunate.  It means I either need to make sense of the histogram files (figure out which are which and parse the histograms, too), look for other per-minute summary files I might have left on the load generator, or else run the test again.

I checked the load generators but did not see any more useful files there.

It's not so hard to find the right kv-histograms files, at least.  From my notes, I'd expect to see load generators that ran from about 1838Z until about 22:26Z.  Indeed, we have three files named for 18:37Z whose mtimes end at 22:25Z.

Looks like I should write a tool to parse these.  This seems useful but will take some time.  Maybe it's worth updating RFD 110 with my findings so far first.

I spent some time on the tool.  This seems like a custom JSON serialization of HdrHistograms, for which there's a Rust library, but no JSON import (or "snapshot" import) of it.  I looked at the Golang implementation of Import and decided it looked reasonably safe to just try to fill in the fields of the struct directly.  I'm now trying to sanity test the result.

TODO (copied some from yesterday):

* caveat with all of the above: 40-second averages.  Any black hole shorter than that would not be visible.
* look for any client data to address ^
** finish tool to dump these histogram files:
*** add a field for number of operations represented in the histogram.  This would be a good next step to sanity-check correctness
*** compare what the tool reports for p50/p95/etc against the {1,2,3}.out files
** if that works out, then write a mode that prints out the information I need for each timestamp and check the output for the tests I ran.  Characterize the result.  This should be the answer I'm looking for.
* organize screenshots that I took
* update RFD 110


== 2020-10-28

Summary of the day (see yesterday TODO):

* got chist working / test it
* characterized client impact during expansion/contraction history based on the client histogram data -- generated some useful graphs
* set up new NVME cluster and started filling it up
* planned backup testing and rolling upgrade testing

=== More analysis from previous testing

I updated chist to include pMax and throughput, which are both contained in the histogram data.  Now that I'm comparing it to the right files, the data matches the output that `cockroach workload` printed.  For example, I compared these two:

[source,text]
----
$ chist data/2020-09-30-contraction-grafana-partial/client-data/kv-histograms-2020-09-30T16:42:01Z.out

$ awk 'NR == 1 || $NF == "read"' data/2020-09-30-contraction-grafana-partial/client-data/1.out
----

and they matched up in elapsed time (within a second), throughput, p50, p95, p99, and pMax (though I only spot-checked).

This is good, but...the server data is a 40-second average and the client data is a 60-second average, so it won't solve that problem.

I decided it would be worth extending "chist" to aggregate data from multiple files.  This seems to work and prints correct output for a single file.  It does have a limitation though that it's potentially aggregating time-buckets that are offset by as much as 30 seconds or so.  It would have helped to have kept these histograms per-second.

Now I'm looking at the actual data.  I think it would be compelling to plot it.  Here's how I'm generating files:

[source,text]
----
$ ../../tools/chist/target/debug/chist print client-data/kv-histograms-2020-09-30T18\:37\:* > chist-1837-2225.out
$ ../../tools/chist/target/debug/chist print client-data/kv-histograms-2020-09-30T22\:44\:1* > chist-2244-end.out
$ ls -l
total 144
-rw-r--r--  1 dap  staff  33966 Oct 28 14:08 chist-1837-2225.out
-rw-r--r--  1 dap  staff  36630 Oct 28 14:08 chist-2244-end.out
lrwxr-xr-x  1 dap  staff     62 Oct 28 14:06 client-data@ -> ../../data/2020-09-30-contraction-grafana-partial/client-data/
----

Ideally I'd add a mode to chist to combine reads and writes, but for now I'm going to do these separately.  I'll split the data with:

[source,text]
----
$ awk 'NR == 1 || $2 == "write"' chist-1837-2225.out > chist-1837-2225-writes.out
$ awk 'NR == 1 || $2 == "read"' chist-1837-2225.out > chist-1837-2225-reads.out
$ awk 'NR == 1 || $2 == "read"' chist-2244-end.out > chist-2244-end-reads.out
$ awk 'NR == 1 || $2 == "write"' chist-2244-end.out > chist-2244-end-writes.out
----

I want to check for missing data points:

[source,text]
----
$ awk '$3 != 3' chist-*.out
TIME                     OPNAM  # OPS/SEC p50(ms) p95(ms) p99(ms)    pMax
2020-09-30T22:25:39.647Z  read  2   647.3     4.7    14.2    37.7   453.0
TIME                     OPNAM  # OPS/SEC p50(ms) p95(ms) p99(ms)    pMax
2020-09-30T22:25:39.647Z write  2   158.2     6.6    21.0   100.7   604.0
TIME                     OPNAM  # OPS/SEC p50(ms) p95(ms) p99(ms)    pMax
2020-09-30T22:25:39.647Z  read  2   647.3     4.7    14.2    37.7   453.0
2020-09-30T22:25:39.647Z write  2   158.2     6.6    21.0   100.7   604.0
TIME                     OPNAM  # OPS/SEC p50(ms) p95(ms) p99(ms)    pMax
TIME                     OPNAM  # OPS/SEC p50(ms) p95(ms) p99(ms)    pMax
TIME                     OPNAM  # OPS/SEC p50(ms) p95(ms) p99(ms)    pMax
----

Just a handful of data points, and they're all at the end of one file.  I'm going to ignore these for now.

Here's how I'm creating separate GNUplot data files for the series I want to plot:

[source,text]
----
$ cat chist-1837-2225-reads.out chist-2244-end-reads.out | awk '$1 != "TIME"{ print $1,$4 }' > plot-data-reads-throughput.out
$ cat chist-1837-2225-reads.out chist-2244-end-reads.out | awk '$1 != "TIME"{ print $1,$8 }' > plot-data-reads-pMax-latency.out
$ cat chist-1837-2225-reads.out chist-2244-end-reads.out | awk '$1 != "TIME"{ print $1,$7 }' > plot-data-reads-p99-latency.out
$ cat chist-1837-2225-writes.out chist-2244-end-writes.out | awk '$1 != "TIME"{ print $1,$4 }' > plot-data-writes-throughput.out
$ cat chist-1837-2225-writes.out chist-2244-end-writes.out | awk '$1 != "TIME"{ print $1,$8 }' > plot-data-writes-pMax-latency.out
$ cat chist-1837-2225-writes.out chist-2244-end-writes.out | awk '$1 != "TIME"{ print $1,$7 }' > plot-data-writes-p99-latency.out

----

Then I decided to explicitly add a blank line between 22:26Z and 22:44Z so that GNUplot draws a discontinuity.

I created a GNUplot file that produces acceptable graphs.  This is probably enough data collection.  The next step will be to write this up.

=== Set up NVME cluster for more testing

Other action items from RFD 110 are to do a full backup, a rolling upgrade, and a schema upgrade.  For these, I'll need a new cluster.  To make this realistic, it may as well be an NVME cluster.  I may just do another expansion/contraction test, since it doesn't take that long now that I'm familiar enough with it.

To recreate the cluster, I'm going to repeat my steps from 2020-09-29, which is basically to deploy a new NVME-based database cluster with three nodes, then start a load generator like this:

[source,text]
----
cockroach workload run kv --init --concurrency 4 --display-every=60s --batch 10 --max-block-bytes 1024 --min-block-bytes 1024 postgresql://root@192.168.1.53:26257/kv?sslmode=disable
----

I'll let this run until tomorrow morning and see if it's big enough.

When it comes to the expansion/contraction testing, I will probably try a lighter workload in order to see if there's less impact on throughput when latency increases (which is what we'd expect).

To keep things simple (I hope), I'm going to use a wholly separate environment than the one I was using before.  I'm doing this from the cockroachdb-testing-round2 clone using assets bucket and Terraform bucket "oxide-cockroachdb-exploration-round2".

The three nvmedb instances' local IPs are .243, .232, and .29.  I'm going to run an instance of this:

[source,text]
----
nohup cockroach workload run kv --init --concurrency 4 --display-every=60s --batch 10 --max-block-bytes 1024 --min-block-bytes 1024 postgresql://root@192.168.1.243:26257/kv?sslmode=disable &
mv nohup.out bigdb-1.out
nohup cockroach workload run kv --init --concurrency 4 --display-every=60s --batch 10 --max-block-bytes 1024 --min-block-bytes 1024 postgresql://root@192.168.1.232:26257/kv?sslmode=disable &
mv nohup.out bigdb-2.out
nohup cockroach workload run kv --init --concurrency 4 --display-every=60s --batch 10 --max-block-bytes 1024 --min-block-bytes 1024 postgresql://root@192.168.1.29:26257/kv?sslmode=disable &
mv nohup.out bigdb-3.out
----

The last time I ran this, I let it run 18.5 hours and it reached 73 GiB per node, or 221 GiB altogether.  I started running all this at about 2020-10-28T22:54Z, so I'd expect it to reach this size around 10:30am PT tomorrow.  I'll evaluate in the morning how big it is and if that's big enough.  For my purposes, like before, it probably only needs to be 48 - 72 GiB per node (144-216 GiB overall).

=== Planning more testing

==== Backup/restore

https://www.cockroachlabs.com/docs/stable/backup-and-restore.html#perform-core-backup-and-restore[Cockroach dump]:

[source,text]
----
cockroach dump kv | gzip > backup.sql.gz
----

Import:

[source,text]
----
gzcat backup.sql.gz | cockroach sql --database=kv
----

Could also try S3 import, but it's probably not useful for our purposes:

[source,text]
----
cockroach sql --execute="IMPORT PGDUMP 's3://your-external-storage/backup.sql?AWS_ACCESS_KEY_ID=[placeholder]&AWS_SECRET_ACCESS_KEY=[placeholder]'" \
 <flags>
----

Notes from https://www.cockroachlabs.com/docs/v20.1/cockroach-dump[`cockroach dump`]:

> If the dump takes longer than the ttlseconds replication setting for the table (25 hours by default), the dump may fail.
> Reads, writes, and schema changes can happen while the dump is in progress, but will not affect the output of the dump.

Flags that might be useful:

[source,text]
----
--as-of TIMESTAMP
--host ...
--insecure
----

For backup, we'll probably want to run this from NVME DB host 192.168.1.243 (nvmedb1):

[source,text]
----
nohup bash -c "time cockroach dump kv --insecure --host 192.168.1.243 | gzip > backup-kv.sql.gz" &
mv nohup.out cockroach_dump.out
----

To test restore to a second copy, we could run:

[source,text]
----
nohup bash -c "time gzcat backup-kv.sql.gz | cockroach sql --insecure --host 192.168.1.243 --database=kv2" &
mv nohup.out cockroach_restore.out
----

==== Rolling upgrade

I've obviously got to build a new CockroachDB binary for a new release.  It does not look like they've issued a new release yet.  I could try https://github.com/cockroachdb/cockroach/releases/tag/v20.2.0-rc.3[v20.2.0-rc.3], tagged October 20.  I'll presumably want to build this with garbage-compactor in lennier.  I probably want to build Grafana 7.2, too, but that's less urgent because I'm probably not going to replace the monitoring zone soon, and I don't _need_ the $__rate_interval as long as I use per-second client-side metrics for the next round of analysis.

See https://www.cockroachlabs.com/docs/v20.2/upgrade-cockroach-version.html[rolling upgrade docs] are here.  It's not a given that I can actually do this with the database version that I'm starting with, which is a prerelease.

Policy note to include in RFD: "you must first be on a production release of the previous version. The release does not need to be the latest production release of the previous version, but it must be a production release rather than a testing release (alpha/beta)."

"Finalization" seems to refer to things we call "deferred upgrades".

Basic procedure:

* drain (`svcadm disable` should automatically do this if we have it sending SIGTERM) and stop the process
* replace the binary
* start the process again
* wait at least one minute and repeat for the next nodes
* if we disabled auto-finalization above, undo that

Review from recent releases:

* breaking changes
* upgrade finalization features

This broadly seems pretty sane, though.  Unfortunately, to test it, I don't think I'll be able to start with the version that I'm currently using.  Maybe the thing to do is to build copies of v19 and v20, deploy v19, and then do a rolling upgrade to v20.  Alternatively, I could just build v20.2.0-rc.3 (see above), try it, and see what happens!

So the plan will be:

* on lennier, build v20.2.0-rc.3
* get those binaries into S3 somewhere
* fetch those binaries onto each of my database nodes
* rejigger the directory tree on each database node so that I have `.../cockroach/version/{bin/cockroach,lib/...}`, where `version` is a symlink to the current version.  Then create a parallel tree for the second version.  I'll probably want to do all this with CockroachDB offline.  Then bring it up on the old version, let it run for a while under some load, then do the rolling upgrade.

**Take snapshots on all three nodes while they're offline.  That way if the rolling upgrade breaks (since it's not guaranteed to work), I can go back to what I had.**

== 2020-10-29

Plan (includes items copied from yesterday):

* Check on overnight cluster growth.  See below.
* Check on build of cockroachdb.
* While waiting for that, plan tests:
** done (plan only): run full cluster backup under moderate load
** mostly done (plan only): run rolling upgrade
** Run schema upgrade
* When the big cluster is ready:
** create ZFS snapshots on all three nodes
** Rerun expansion/contraction tests under load (more fine-grained data, less load)
** Run full cluster backup under moderate load
** Run schema upgrade
** Run rolling upgrade
* Update RFD 110 with analysis of this test case
** caveat: server-side data are 40-second averages
** caveat: client-side data are 60-second averages, potentially mixed across 90-second windows
** organize the screenshots that I took
* See PR for more action items

=== Overnight cluster growth

I let this run until 17:52Z.  One of the load generators bombed out here:

[source,text]
----
60779.2s        0           30.3           37.2    117.4    234.9    335.5    704.6 write
60839.2s        0           23.0           37.2    159.4    318.8    604.0   2550.1 write
Error: ERROR: result is ambiguous (error=unable to dial n3: breaker open [exhausted]) (SQLSTATE 40003)
----

That appears to have been around 15:48Z, based on the timestamp of the file.  This also reflects when the overall concurrency went down in the cluster and when QPS to one of the nodes went to zero.  Note that CPU utilizationw as 80-100% the whole time, so the system was close to its limits in that way.  I probably should have run this with `--tolerate-errors`.  The load generator in this case was pointed at 192.168.1.29.  I took a brief look at the CockroachDB log for this node at this time and didn't find much.  All of the messages either come from compaction.go or they indicate a new WAL created, which looks routine.  There's very little that looks different for nvmedb3 at that time in terms of CPU usage (which was pegged like the rest), average disk I/O latency, disk or network throughput, etc.  p99 hearbeat latency did see outliers up to 500ms, but it had seen a bit higher previously.  p99 round trip latency did shoot up to _almost_ 5s for some nodes.

Anyway, the database wound up at 70.7 GiB ZFS used per node (3 nodes).  CockroachDB reports 218.7 GiB used and 256 total ranges.  That should be fine for my testing.

Having done this, I'm going to take a ZFS snapshot on each node (while CockroachDB is down).

Two of them came down cleanly with `svcadm disable -s cockroachdb`.  The other one went into maintenance because it was still running after 60 seconds.  Anyway, they're all down now so I can take my snapshot.  I took that with `zfs snapshot tank/cockroachdb@2020-10-29-bigdb`.  (There are no child datasets to worry about.)  Then I enabled all of the cockroachdb instances.  This all happened around 18:10Z.

=== Build of CockroachDB

I did successfully finish a build of CockroachDB v20.2.0-rc.3.illumos (after running into some issues related to some broken changes I had made to garbage-compactor).

=== Testing time

I'm now going to redo the expansion/contraction testing using load generators that are keeping per-second data.  I'm also going to have them target a lower rate of load -- maybe just 1000 total requests per second, or about 333 per load generator.

Based on 9/30's workload, and making a few improvements to the automation and applying this lower target rate, I'm going to do it this way:

[source,text]
----
date="$(date +%FT%TZ)"
for node in 243 232 29; do
	nohup cockroach workload run kv --histograms kv-histograms-$node-$date.out --concurrency 4 --max-rate=333 --display-every=1s --read-percent 80 --tolerate-errors postgresql://root@192.168.1.$node:26257/kv?sslmode=disable > loadgen-summary-$node-$date.out 2>&1 &
done
----

I kicked this off at 18:21:06Z.  I'm going to let this run for a little while before changing anything.  I hope to do expansion/contraction testing this afternoon, and maybe a rolling upgrade.

20:25Z: I'm taking a look at the last two hours.  It's a little strange:

* There have been a fair number of notable dips in throughput.  These have mostly been associated with spikes in latency, especially large p99 spikes.  They've largely been associated with increases in average disk I/O time on nvmedb2, which has had quite a lot of outliers in the tens of milliseconds.  That huge spike is associated with a 140ms *average* disk I/O latency outlier on nvmedb2.
* The p95 and p99 latency had huge spikes -- up to over 1s -- around 20:06Z.
* There was also a surge disk bytes read per second on all nodes around 19:30.  This also correlated with a spike in CPU utilization.  This was associated with a spike in GC runs.

It looks like nvmedb2 just has a really crappy disk for some reason.  Although I don't expect any throttling from AWS here, it's conceivable if this system did a lot more IOPS.  Based on the graphs, that doesn't seem to be true.  That said, the sadness from this system appears to have started around 14:32Z -- it was okay for the most part overnight.  That was about the time the count of replicas per node jumped up significantly, from about 162 to about 290.  Some splits happened at that time.  CPU utilization looks about the same as the other systems.  Disk utilization is a fair bit worse, which is consistent with the poor latency.

I would consider throwing away nvmedb2 and replacing it with another system.  That basically looks like an expansion + contraction anyway, so I guess I'll go ahead and do that.

I mentioned this in chat and Bryan wants to take a look so I'm going to hold tight for a bit.

> You can reach the system with `ssh -L3000:192.168.1.209:3000 root@52.24.193.14`.  That will also open up an SSH tunnel if you want to check out the Grafana stats at http://localhost:3000 (username and password both "admin").
> The data I'm looking at is average I/O latency reported by kstats and averaged by Prometheus over a 40s interval.  I deployed these systems yesterday afternoon, ran a write-heavy workload overnight, and started a mixed workload this morning.  Two of the three nodes have had a max average latency of 2.4ms and 3.4ms.  The other has seen 135ms, with quite a lot of data points in the tens of milliseconds.  The latency outliers started around 13:00Z (6am PT, well before I changed the workload).  That's about the time that performance of the write workload started varying quite a lot more than before and CockroachDB reported a brief spike in p95 from 200ms to 8 *seconds*.  The real reason I noticed is that my mixed read-write workload is also seeing lots of latency spikes that, at least by eye, seem to correlate with average disk I/O latency outliers.
> It's surprising to me that this node is much worse than the others.

I decided to trace this using this script:

[source,text]
----
#pragma D option quiet

BEGIN
{
        printf("tracing.\n");
}

io:::start
{
        starts[arg0] = timestamp;
}

io:::done
/starts[arg0]/
{
        @[args[1]->dev_statname, args[0]->b_flags & B_WRITE ? "write" : "read"] =
            quantize((timestamp - starts[arg0]) / 1000);
        starts[arg0] = 0;
}

tick-10s,END
{
        printf("%Y (latency in microseconds)\n", walltimestamp);
        printa(@);
        trunc(@);
}
----

and I caught data points like this one, showing I/Os between 500ms and 1s, for both read and write:

[source,text]
----
2020 Oct 29 21:32:54 (latency in microseconds)

  xdf0                                                write
           value  ------------- Distribution ------------- count
             128 |                                         0
             256 |@@@@                                     7
             512 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@       66
            1024 |@@                                       4
            2048 |                                         0

  blkdev0                                             write
           value  ------------- Distribution ------------- count
              16 |                                         0
              32 |@@@@@@@@@@@@@@@                          1055
              64 |@@@@@@@@@@@@@@@@@@@@@@@                  1684
             128 |@                                        101
             256 |                                         8
             512 |                                         3
            1024 |                                         2
            2048 |                                         1
            4096 |                                         3
            8192 |                                         5
           16384 |                                         17
           32768 |                                         25
           65536 |                                         0
          131072 |                                         3
          262144 |                                         1
          524288 |                                         1
         1048576 |                                         0

  blkdev0                                             read
           value  ------------- Distribution ------------- count
              64 |                                         0
             128 |@@@@@@@@@@@@@@                           877
             256 |@@@@@@@@@@@@@@@@@@                       1118
             512 |@                                        64
            1024 |@                                        34
            2048 |                                         18
            4096 |                                         6
            8192 |@                                        44
           16384 |@@@@@                                    301
           32768 |                                         5
           65536 |                                         5
          131072 |                                         18
          262144 |                                         1
          524288 |                                         7
         1048576 |                                         0
----

So the device really is doing this.

Some of these instances are very well correlated with crashes in throughput, as around 21:36:40 to 21:40:20Z.

I'm basically holding these as-is for Bryan to look.

== 2020-10-30

Going to mostly skip CockroachDB for today.  Bryan's going to continue looking at this cluster.  One takeaway from the discussion at the control plane huddle was to use 2xlarge or 4xlarge instances in order to get dedicated NVME hardware to avoid the latency outliers.  Whether I do this or not, I could stand up a new cluster and resume testing in parallel with keeping this one for Bryan to keep looking at.  But it'll take 15+ hours to populate it with data, and I'm not going to be available this weekend even to stop it, so I'm just going to wait til Monday to start this.

== 2020-11-02

Plan for today:

* Working on supporting multiple clusters concurrently.  This is blocked on AWS support to bump a resource limit.  This work is in the -round3 clone of my repo.  Need to make sure I clean up all these resources when I'm done.  (As of 9am PT, there are no resources here because I cleaned them up when I ran into the AWS support issue.)
* Re-evaluate the cluster that was busted last week.  I/O outliers stopped and performance got more stable.  Let's get more specific.
* Resume testing, either with a new cluster or the previous nvmedb one.
** Rerun expansion/contraction tests under load (more fine-grained data, less load)
** Run full cluster backup under moderate load
** Run schema upgrade
** Run rolling upgrade
* Update RFD 110 with analysis of this test case
** caveat: server-side data are 40-second averages
** caveat: client-side data are 60-second averages, potentially mixed across 90-second windows
** organize the screenshots that I took
* See PR for more action items

=== Expansion testing

Workload performance stabilized (want to take some notes on this above).  It's been running for several days now.  So I'm going to run an expansion test.

I brought up nvmedb4 at

[source,text]
----
root@nvmedb4:~# date +%FT%TZ; svcadm enable -s cockroachdb; date +%FT%TZ
2020-11-02T17:29:28Z
2020-11-02T17:29:29Z
root@nvmedb4:~#
----

Since I will be out for a little while, and it would be pretty helpful to have results today, I'm going to automate bringup of nvmedb5 in about an hour, using:

[source,text]
----
date +%FT%TZ; sleep 3600; date +%FT%TZ; svcadm enable -s cockroachdb; date +%FT%TZ
----

I ran that at 17:32Z.  The automated one kicked off at 18:32:

[source,text]
----
# date +%FT%TZ; sleep 3600; date +%FT%TZ; svcadm enable -s cockroachdb; date +%FT%TZ
2020-11-02T17:32:09Z
2020-11-02T18:32:09Z
2020-11-02T18:32:10Z
----

At 19:43 or so, I'm going to decommission node 5:

[source,text]
----
# date +%FT%TZ; time cockroach node decommission 5 --host 192.168.1.232 --insecure
2020-11-02T19:44:50Z

  id | is_live | replicas | is_decommissioning |   membership    | is_draining
-----+---------+----------+--------------------+-----------------+--------------
   5 |  true   |      123 |        true        | decommissioning |    false
(1 row)
...
No more data reported on target nodes. Please verify cluster health before removing the nodes.

real    5m27.341s
user    0m0.351s
sys     0m0.355s
----

I'm going ahead with decommissioning node 4, too:

[source,text]
----
# date +%FT%TZ; time cockroach node decommission 4 --host 192.168.1.232 --insecure
2020-11-02T20:00:19Z

  id | is_live | replicas | is_decommissioning |   membership    | is_draining
-----+---------+----------+--------------------+-----------------+--------------
   4 |  true   |      142 |        true        | decommissioning |    false
(1 row)
...
No more data reported on target nodes. Please verify cluster health before removing the nodes.

real    9m33.137s
user    0m0.457s
sys     0m0.461s
----

I now have two rounds each of expansion and contraction.  By far, the worst impact was going from 3 -> 4 (the first expansion), where clients saw latencies as high as 3 seconds with no queries completing.  p95: 5ms -> 20-40ms.  p99: 10ms -> 50-125ms.  During contraction, p99 up to 200ms.  (Not clear why the impact in throughput and pMax seemed less for the contraction.)  In all of these, CPU utilization showed moments close to saturation.  Disks never got all that busy and average I/O time was generally good.  Replication did proceed as expected (based on "replicas per node" graphs).  I will try to get some graphs together.

At 20:27:46 I disabled CockroachDB on nodes 4 and 5.

My suspicion at this point is that these boxes were pegged for CPU during that initial window, and that larger VCPU counts won't see this problem.  That might be another experiment worth doing.o

Another experiment worth doing is to just disable one node for 5 minutes and see if we see the same degradation during rebalancing.

I did a similar analysis of the client-side data as I did for the 9/30 data.

First, on the load generator I filtered the historams files to only contain today's data:

[source,text]
----
for x in kv-*; do grep 2020-11-02T $x > $x-1102.out; done
----

Then I tar'ed them up along with the summary files, copied the tarball locally, and untar'ed it (to preserve mtimes).  That went into ./data/2020-11-02-nvme-expansion-contraction.  Locally:

[source,text]
----
dap@zathras cockroachdb-testing-round2 $ mkdir -p analysis/2020-11-02-nvme-expansion-contraction
dap@zathras cockroachdb-testing-round2 $ cd !$
cd analysis/2020-11-02-nvme-expansion-contraction
dap@zathras 2020-11-02-nvme-expansion-contraction $ ln -s ../../data/2020-11-02-nvme-expansion-contraction/client-data
dap@zathras 2020-11-02-nvme-expansion-contraction $ time chist print client-data/kv-histograms-2* > chist-all.out

real	1m49.973s
user	1m47.634s
sys	0m1.908s
dap@zathras 2020-11-02-nvme-expansion-contraction $ ls -l
total 22536
-rw-r--r--  1 dap  staff  10959918 Nov  2 12:43 chist-all.out
lrwxr-xr-x  1 dap  staff        60 Nov  2 12:38 client-data@ -> ../../data/2020-11-02-nvme-expansion-contraction/client-data
dap@zathras 2020-11-02-nvme-expansion-contraction $ awk 'NR == 1 || $2 == "write"' chist-all.out > chist-writes.out
dap@zathras 2020-11-02-nvme-expansion-contraction $ awk 'NR == 1 || $2 == "read"' chist-all.out > chist-reads.out
dap@zathras 2020-11-02-nvme-expansion-contraction $ awk '$3 != 3' chist-*.out
TIME                     OPNAM  # OPS/SEC p50(ms) p95(ms) p99(ms)    pMax
2020-11-02T20:34:03.018Z  read  2   525.3     2.6     5.2     9.4    12.6
2020-11-02T20:34:03.018Z write  2   138.8     4.5     9.4    14.7    15.7
2020-11-02T20:34:04.017Z  read  2   510.7     2.4     5.8    24.1    46.1
2020-11-02T20:34:04.017Z write  2   132.2     3.9     7.3    41.9    62.9
2020-11-02T20:34:05.018Z  read  2   516.4     2.2     3.8     4.7    12.6
2020-11-02T20:34:05.018Z write  2   148.8     3.9     5.5     7.3     7.3
2020-11-02T20:34:06.017Z  read  2   538.4     2.2     4.2     5.8    11.0
2020-11-02T20:34:06.017Z write  2   126.1     3.9     6.0     8.9     8.9
2020-11-02T20:34:07.017Z  read  1   280.1     2.2     4.7    12.1    19.9
2020-11-02T20:34:07.017Z write  1    53.0     3.7     6.3     9.4     9.4
2020-11-02T20:34:08.017Z  read  1   256.0     2.1     3.5     5.0    13.6
2020-11-02T20:34:08.017Z write  1    76.0     3.5     5.0     6.3     6.3
2020-11-02T20:34:09.018Z  read  1   257.8     2.2     4.5    26.2    39.8
2020-11-02T20:34:09.018Z write  1    67.9     3.5     5.5    13.6    13.6
2020-11-02T20:34:10.017Z  read  1   267.3     2.2     4.2     5.5     8.1
2020-11-02T20:34:10.017Z write  1    67.1     3.5     5.8     7.9     7.9
2020-11-02T20:34:11.018Z  read  1   267.6     2.4     4.2    14.7    29.4
2020-11-02T20:34:11.018Z write  1    62.9     3.9     6.3    17.8    17.8
TIME                     OPNAM  # OPS/SEC p50(ms) p95(ms) p99(ms)    pMax
2020-11-02T20:34:03.018Z  read  2   525.3     2.6     5.2     9.4    12.6
2020-11-02T20:34:04.017Z  read  2   510.7     2.4     5.8    24.1    46.1
2020-11-02T20:34:05.018Z  read  2   516.4     2.2     3.8     4.7    12.6
2020-11-02T20:34:06.017Z  read  2   538.4     2.2     4.2     5.8    11.0
2020-11-02T20:34:07.017Z  read  1   280.1     2.2     4.7    12.1    19.9
2020-11-02T20:34:08.017Z  read  1   256.0     2.1     3.5     5.0    13.6
2020-11-02T20:34:09.018Z  read  1   257.8     2.2     4.5    26.2    39.8
2020-11-02T20:34:10.017Z  read  1   267.3     2.2     4.2     5.5     8.1
2020-11-02T20:34:11.018Z  read  1   267.6     2.4     4.2    14.7    29.4
TIME                     OPNAM  # OPS/SEC p50(ms) p95(ms) p99(ms)    pMax
2020-11-02T20:34:03.018Z write  2   138.8     4.5     9.4    14.7    15.7
2020-11-02T20:34:04.017Z write  2   132.2     3.9     7.3    41.9    62.9
2020-11-02T20:34:05.018Z write  2   148.8     3.9     5.5     7.3     7.3
2020-11-02T20:34:06.017Z write  2   126.1     3.9     6.0     8.9     8.9
2020-11-02T20:34:07.017Z write  1    53.0     3.7     6.3     9.4     9.4
2020-11-02T20:34:08.017Z write  1    76.0     3.5     5.0     6.3     6.3
2020-11-02T20:34:09.018Z write  1    67.9     3.5     5.5    13.6    13.6
2020-11-02T20:34:10.017Z write  1    67.1     3.5     5.8     7.9     7.9
2020-11-02T20:34:11.018Z write  1    62.9     3.9     6.3    17.8    17.8
dap@zathras 2020-11-02-nvme-expansion-contraction $ ls -l
total 43944
-rw-r--r--  1 dap  staff  10959918 Nov  2 12:43 chist-all.out
-rw-r--r--  1 dap  staff   5479996 Nov  2 12:44 chist-reads.out
-rw-r--r--  1 dap  staff   5479996 Nov  2 12:44 chist-writes.out
lrwxr-xr-x  1 dap  staff        60 Nov  2 12:38 client-data@ -> ../../data/2020-11-02-nvme-expansion-contraction/client-data
dap@zathras 2020-11-02-nvme-expansion-contraction $ cat chist-reads.out | awk '$1 != "TIME"{ print $1,$4 }' > plot-data-reads-throughput.out
dap@zathras 2020-11-02-nvme-expansion-contraction $ cat chist-reads.out | awk '$1 != "TIME"{ print $1,$8 }' > plot-data-reads-pMax-latency.out
dap@zathras 2020-11-02-nvme-expansion-contraction $ cat chist-reads.out | awk '$1 != "TIME"{ print $1,$7 }' > plot-data-reads-p99-latency.out
dap@zathras 2020-11-02-nvme-expansion-contraction $ cat chist-writes.out | awk '$1 != "TIME"{ print $1,$4 }' > plot-data-writes-throughput.out
dap@zathras 2020-11-02-nvme-expansion-contraction $ cat chist-writes.out | awk '$1 != "TIME"{ print $1,$8 }' > plot-data-writes-pMax-latency.out
dap@zathras 2020-11-02-nvme-expansion-contraction $ cat chist-writes.out | awk '$1 != "TIME"{ print $1,$7 }' > plot-data-writes-p99-latency.out
dap@zathras 2020-11-02-nvme-expansion-contraction $ 
dap@zathras 2020-11-02-nvme-expansion-contraction $ ls -l
total 70264
-rw-r--r--  1 dap  staff  10959918 Nov  2 12:43 chist-all.out
-rw-r--r--  1 dap  staff   5479996 Nov  2 12:44 chist-reads.out
-rw-r--r--  1 dap  staff   5479996 Nov  2 12:44 chist-writes.out
lrwxr-xr-x  1 dap  staff        60 Nov  2 12:38 client-data@ -> ../../data/2020-11-02-nvme-expansion-contraction/client-data
-rw-r--r--  1 dap  staff   2211495 Nov  2 12:45 plot-data-reads-p99-latency.out
-rw-r--r--  1 dap  staff   2220779 Nov  2 12:45 plot-data-reads-pMax-latency.out
-rw-r--r--  1 dap  staff   2295035 Nov  2 12:45 plot-data-reads-throughput.out
-rw-r--r--  1 dap  staff   2217896 Nov  2 12:46 plot-data-writes-p99-latency.out
-rw-r--r--  1 dap  staff   2221103 Nov  2 12:46 plot-data-writes-pMax-latency.out
-rw-r--r--  1 dap  staff   2294626 Nov  2 12:46 plot-data-writes-throughput.out
dap@zathras 2020-11-02-nvme-expansion-contraction $ vim gengraphs.gnuplot 
dap@zathras 2020-11-02-nvme-expansion-contraction $ gnuplot gengraphs.gnuplot 
dap@zathras 2020-11-02-nvme-expansion-contraction $ ls -l
total 70480
-rw-r--r--  1 dap  staff  10959918 Nov  2 12:43 chist-all.out
-rw-r--r--  1 dap  staff   5479996 Nov  2 12:44 chist-reads.out
-rw-r--r--  1 dap  staff   5479996 Nov  2 12:44 chist-writes.out
lrwxr-xr-x  1 dap  staff        60 Nov  2 12:38 client-data@ -> ../../data/2020-11-02-nvme-expansion-contraction/client-data
-rw-r--r--  1 dap  staff      1876 Nov  2 12:49 gengraphs.gnuplot
-rw-r--r--  1 dap  staff     38496 Nov  2 12:49 graph-read-latency.png
-rw-r--r--  1 dap  staff     22663 Nov  2 12:49 graph-throughput.png
-rw-r--r--  1 dap  staff     38788 Nov  2 12:49 graph-write-latency.png
-rw-r--r--  1 dap  staff   2211495 Nov  2 12:45 plot-data-reads-p99-latency.out
-rw-r--r--  1 dap  staff   2220779 Nov  2 12:45 plot-data-reads-pMax-latency.out
-rw-r--r--  1 dap  staff   2295035 Nov  2 12:45 plot-data-reads-throughput.out
-rw-r--r--  1 dap  staff   2217896 Nov  2 12:46 plot-data-writes-p99-latency.out
-rw-r--r--  1 dap  staff   2221103 Nov  2 12:46 plot-data-writes-pMax-latency.out
-rw-r--r--  1 dap  staff   2294626 Nov  2 12:46 plot-data-writes-throughput.out
----

This wasn't as useful as I'd hoped because there are so many more data points than last time.  I'm going to modify it to look only at each individual window.

Looking at the close up graphs, I see that the first expansion involved a 40+ second window in which no queries were completing at all.  p99 and pMax for two seconds after that period were over 47 seconds.  This was from 17:29:57.167Z to 17:30:35.167Z and affected all three clients.  During this time, there was not a ton of I/O, but some CPUs were over 90% utilized (again, 40-second average).  We saw a large spike in distsender errors, including "notleaseholder" and "rpc_sent_nextreplicaerror" and generic "exec_error".  (These are all internal errors.)  Round-trip latency also shot up to a few seconds in some cases.

I want to retry this with more CPU.  I'm going to bump up the instance types to i3.2xlarge.  To do this, I'm first going to shut down the kv workload runners.  This is 21:43Z.

This turned out to be a big mistake because changing the instance type of a VM destroys any data on the local drive.  So that pushes back any further testing.

At this point, the most useful things I can probably do are:

* finish testing my changes from this morning to support multiple concurrent clusters
** done: update (hot patch) the Prometheus config in the remaining "mon" zone
** done: taint and undeploy the NVME database nodes that were destroyed above
** blocked: deploy a new cluster with a new name.  This is blocked on AWS answering my support mail.

Plan for whenever this becomes unblocked:

* switch to a release version of CockroachDB
* deploy new cluster with a new name using better instance type
* consider adding tracing for fsync time

== 2020-11-03

The AWS support case was resolved.  They didn't give me what I asked for, but they gave me enough.

Plan (revised):

* build CockroachDB v20.2.0-rc.4
** while doing this I ran into a bunch of issues and I'm not positive my build of v20.2.0-rc.3 is correct.
* update vminit tarball to support multiple CockroachDB versions and include v20.2.0-rc.3 and v20.2.0-rc.3.
* don't forget to update proto-raw.tgz when I'm done here
* repair Terraform cluster
* figure out what workload to run
** "kv" to build up the cluster?
* take a ZFS snapshot backup + "cockroach dump" after the database is big enough

=== Repairing cluster

In trying to remove the security group, I see that the two remaining instances (loadgen0 and mon0) are still using it.  They appear to need _some_ security group.  Here's what I'm thinking:

- create a new dummy security group
- associate these instances with that SG
- destroy the SG
- rerun terraform and see what it wants to do.  My hope is that it continues with what it was doing before, including tearing down the VPC, etc.  Try to let it proceed.  I suspect that it will wind up failing in a similar way trying to delete the VPC though.  I can potentially repeat the process.  In both cases, I wonder if it will attach the instances to the correct security group and maybe VPC, then all I'll have to do is remove the dummy ones.

It looks like there's already another security group attached to this VPC -- the default one sg-042c66bef19dfa619.  The one I need to remove is sg-00bcb0d4628a63d0a.

I changed the security group of both Instances to sg-042c66bef19dfa619, then ran Terraform, and it did do what I expected (destroyed the old SG, created a new one, and assigned the instances to it).

=== Cockroach build

I tried to build v20.2.0-rc.4 today.  I ran into a bunch of problems.

When I didn't set my PATH correctly, I ran into what looks like https://github.com/golang/go/issues/32817.  I'm not sure why my PATH would matter for this, as go wasn't in my PATH to begin with.  However, without fixing PATH, I ran into this pretty reliably.  After fixing PATH, I don't.

When I tried to build Cockroach v20.2.0-rc.4:

[source,text]
----
...
github.com/pquerna/cachecontrol
golang.org/x/crypto/ed25519
gopkg.in/square/go-jose.v2/cipher
gopkg.in/square/go-jose.v2/json
vitess.io/vitess/go/vt/proto/vtgate
gopkg.in/square/go-jose.v2
vitess.io/vitess/go/sqltypes
vitess.io/vitess/go/vt/sqlparser
github.com/coreos/go-oidc
gmake: *** [Makefile:1007: cockroach] Error 2
----

which isn't obvious what went wrong.  When I reran it, I happened to notice this fragment (which also appeared in the first build):

[source,text]
----
# github.com/cockroachdb/pebble
vendor/github.com/cockroachdb/pebble/options.go:573:10: undefined: vfs.WithDiskHealthChecks
go: downloading golang.org/x/sys v0.0.0-20200824131525-c12d262b63d8
gmake: *** [Makefile:1007: cockroach] Error 2
gmake: *** Waiting for unfinished jobs....
----

This looks like maybe a platform problem.  I believed I had successfully built v20.2.0-rc.3 just recently, so I checked the deltas between the tags, and there was nothing that would seem to change this.  Strange!  Let me double check the binary I had.

Unfortunately, it reports:

[source,text]
----
$ ./cockroach-v20.2.0-rc.3/bin/cockroach version
Build Tag:    4ad5cdf
Build Time:   2020/10/29 16:15:09
Distribution: CCL
Platform:     illumos amd64 (x86_64-pc-solaris2.11)
Go Version:   go1.14.6
C Compiler:   gcc 9.3.0
Build SHA-1:  4ad5cdfec1315c10668c8ca1b3774fef4723f636
Build Type:   development
$
----

That SHA is from the garbage-compactor repo, not CockroachDB.  Where does it come from?  It seems to be in garbage-compactor/cockroach/cache/gopath/src/github.com/cockroachdb/cockroach/pkg/build/info.go, which says:

[source,text]
----
// These variables are initialized via the linker -X flag in the
// top-level Makefile when compiling release binaries.
tag             = "unknown" // Tag of this build (git describe --tags w/ optional '-dirty' suffix)
----

Sure enough, the build does something like this:

[source,text]
----
GOFLAGS= go build -o cockroach -v  -mod=vendor -tags 'make x86_64_pc_solaris2.11' -ldflags '-X github.com/cockroachdb/cockroach/pkg/build.typ=development -extldflags "" -X "github.com/cockroachdb/cockroach/pkg/build.tag=e4ddb96" -X "github.com/cockroachdb/cockroach/pkg/build.rev=e4ddb9687709beae43f3543ffd01560584b80480" -X "github.com/cockroachdb/cockroach/pkg/build.cgoTargetTriple=x86_64-pc-solaris2.11" -X "github.com/cockroachdb/cockroach/pkg/build.channel=source-archive"  -X "github.com/cockroachdb/cockroach/pkg/build.utcTime=2020/11/03 17:52:52"' ./pkg/cmd/cockroach
----

This appears to be executed in Makefile at the top of the repo, L1007 right now.  These come from LINKFLAGS, which are set a few lines above.  It looks like you can override these with BUILDINFO_TAG.  They normally come from .buildinfo/{tag,rev}, which are built.

At this point, Josh tells me that he's got a fix for the VFS issue, so I'm going to hang tight on this until I hear from him that it's either working or he could use help.

=== Next steps

Plan (revised):

* blocked: build CockroachDB v20.2.0-rc.4
* blocked: build CockroachDB v20.2.0-rc.3
* blocked: update vminit tarball to support multiple CockroachDB versions and include v20.2.0-rc.3 and v20.2.0-rc.3.
* blocked: don't forget to update proto-raw.tgz when I'm done here
* blocked: deploy new cluster
** start with v20.2.0-rc.3
** load up the database to something large with "kv" workload
** take a ZFS snapshot backup + "cockroach dump"
** separate cluster: restore the backup
** do a rolling upgrade to v20.2.0-rc.4


== 2020-11-09

After a few days trying to work through the build (and distracted with other activities), I decided to take Joshua's builds of v20.2.0-rc.3 and v20.2.0-rc.4 and start building a cluster with v20.2.0-rc.3.  I've downloaded these from https://illumos.org/downloads/cockroach-v20.2.0-rc.3.illumos.tar.gz[here] and https://illumos.org/downloads/cockroach-v20.2.0-rc.4.illumos.tar.gz[here].  I've unpacked them into "vminit/cockroachdb/cockroach/bin" as `cockroach-$version` and created a symlink mapping `cockroach` to v20.2.0-rc.3 (so that I can do a rolling upgrade to rc.4 later).  I have _not_ yet updated the proto tarball upstream.  But I'm putting it into use with:

[source,text]
----
$ AWS_PROFILE=oxide-sandbox-dap aws s3 cp vminit-cockroachdb.tgz s3://oxide-cockroachdb-exploration-round2/vminit-cockroachdb.tgz
----

I went ahead and started the "mon" and "loadgen" instances I had before.  I archived the client-side testing data that I had to a tarball on my local machine, then I marked that instance "tainted" in Terraform so that it will replace it in order to ensure that I'm testing with the correct version of the cockroach client.  I ran `terraform apply` to deploy a new cluster.  Unfortunately, enabling CockroachDB on this new cluster blew up with:

[source,text]
----
[ Nov 10 00:38:50 Executing start method ("/cockroachdb/bin/cockroach start --insecure --store /cockroachdb/data --listen-addr=192.168.1.228 --join=192.168.1.228,192.168.1.152,192.168.1.172 --background"). ]
ld.so.1: cockroach-v20.2.0-rc.3: fatal: libstdc++.so.6: version 'GLIBCXX_3.4.26' not found (required by file /cockroachdb/bin/cockroach-v20.2.0-rc.3)
ld.so.1: cockroach-v20.2.0-rc.3: fatal: libstdc++.so.6: open failed: No such file or directory
[ Nov 10 00:38:50 Method "start" failed due to signal KILL. ]
----

This is because I neglected to update the libstdc++.so.6 binary in the vminit tarball from the Cockroach build tarball.  I erroneously assumed that it hadn't changed.  I'm fixing this by copying the libraries out of the RC.4 tarball.  (A less brittle solution would be to create separate trees including both "bin" and "lib" for each separate Cockroach version.  This will require more surgery than I'm prepared to do now and should not be necessary in this case.)

I've built a new vminit tarball and I'm uploading it to S3.  I've also tainted the db_nvme and loadgen instances in Terraform.  I'm going to test the library files to make sure they work.  (They do.)

The new deployment mostly worked.  However, I get a "Not found" screen at http://localhost:8080, which redirects to http://localhost:8080/#/overview.  The other screens seem to work.

Based on 2020-10-28's notes, and the current cluster IPs 192.168.1.52, 192.168.1.159, 192.168.1.43, I'm going to run this on the load generator:

[source,text]
----
nohup cockroach workload run kv --init --concurrency 4 --display-every=60s --batch 10 --max-block-bytes 1024 --min-block-bytes 1024 postgresql://root@192.168.1.52:26257/kv?sslmode=disable &
mv nohup.out bigdb-1.out
nohup cockroach workload run kv --init --concurrency 4 --display-every=60s --batch 10 --max-block-bytes 1024 --min-block-bytes 1024 postgresql://root@192.168.1.159:26257/kv?sslmode=disable &
mv nohup.out bigdb-2.out
nohup cockroach workload run kv --init --concurrency 4 --display-every=60s --batch 10 --max-block-bytes 1024 --min-block-bytes 1024 postgresql://root@192.168.1.43:26257/kv?sslmode=disable &
mv nohup.out bigdb-3.out
----

Based on last time, I'd expect this to take 18-20 hours to reach a useful size.  18 hours will be tomorrow morning around 11:20am PT.

TODO next:

* fix my Grafana dashboard -- the illumos metrics (and maybe other metrics?) need to use `cluster="$cluster"` instead of `role="$cluster"`.
* once the bigdb workload is done, do a rolling upgrade
* if it still doesn't work, debug why the Overview screen in Admin UI doesn't work
* don't forget to update proto-raw.tgz when I'm done here

== 2020-11-10

Plan for the day:

* Check on "bigdb" workload
* Fix Grafana dashboard
* Do a rolling upgrade
* Debug the "Admin UI" Overview screen page problem
* don't forget to update proto-raw.tgz when I'm done here

=== Fix dashboard

I exported the stock dashboard from Grafana and compared it to what's in my repo to make sure there were no unsaved changes.  It looked fine.  Then I ran this in vim over it: `%s/role=\\"\$cluster/cluster=\\"$cluster/gc`.  I couldn't update this in Grafana directly or delete the existing dashboard, so I moved the old dashboard file aside (from /mon/grafana/stock_dashboards) and put the new file in its place, then restarted Grafana.  This seems to have worked.

=== Overnight workload

When I estimated how long this would take, I did not account for these being much larger instances that might get a lot more done in the same time.  After only about 16 hours, we're at about 124 GiB of space per node.  That said, these nodes have 64 GiB of memory, so that's still only 2x DRAM.

There was an unexpected period of several hours from 0700 to 1000 where the latency really shot up and throughput was low.  This was about half an hour after a lot more replicas had been created (lots of splits).  Round-trip latency actually went down during this period, and CPU utilization of the load generators went down.

With the updated dashboard, I can see that CPU utilization, disk average I/O time, disks %busy, disks IOPS and throughput -- they were all down significantly during this period.  So Cockroach was less busy with I/O and CPU, but latency nonetheless went up.  The above is true about the load generators too -- they were more idle in CPU and disk activity.  The active query count reported by CockroachDB did not go down either -- if anything, there were fewer samples with fewer active queries (i.e., there were always active queries).  Queries were pretty balanced by store and by node.

Interestingly, when I look at the CockroachDB KV execution latency @ p99: I see that n1 was gradually getting slower up until about 06:00, where it leveled off.  But the other two nodes were steadily quite quick until right around 07:00Z, when their latency shot up to match n1's.

There was less compaction and flush activity during this period.

One data point of note: the queue processing time for the Raft log rose from about 1s to almost 2s for most of this period, and suddenly went down again after that.  It's really not clear what happened at the beginning or end of this period to cause this.  We might suspect drops in network traffic, but network traffic throughput went down significantly during this period, too.


=== Rolling upgrade

To prepare for a rolling upgrade, I'm going to do this:

* shut down load generators
* shut down CockroachDB
* save zfs snapshots on all nodes
* bring up CockroachDB (still on old version)
* bring up much lighter-level load generators (analogous to what I did during past expansion/contraction testing)
* run through rolling upgrade

Shutting down load generators:

[source,text]
----
root@loadgen0:~/bigdb# fuser bigdb-*
bigdb-1.out:     1248o
bigdb-2.out:     1253o
bigdb-3.out:     1260o
root@loadgen0:~/bigdb# pargs 1248 1253 1260
1248:   cockroach workload run kv --init --concurrency 4 --display-every=60s --batch 10
argv[0]: cockroach
argv[1]: workload
argv[2]: run
argv[3]: kv
argv[4]: --init
argv[5]: --concurrency
argv[6]: 4
argv[7]: --display-every=60s
argv[8]: --batch
argv[9]: 10
argv[10]: --max-block-bytes
argv[11]: 1024
argv[12]: --min-block-bytes
argv[13]: 1024
argv[14]: postgresql://root@192.168.1.52:26257/kv?sslmode=disable

1253:   cockroach workload run kv --init --concurrency 4 --display-every=60s --batch 10
argv[0]: cockroach
argv[1]: workload
argv[2]: run
argv[3]: kv
argv[4]: --init
argv[5]: --concurrency
argv[6]: 4
argv[7]: --display-every=60s
argv[8]: --batch
argv[9]: 10
argv[10]: --max-block-bytes
argv[11]: 1024
argv[12]: --min-block-bytes
argv[13]: 1024
argv[14]: postgresql://root@192.168.1.159:26257/kv?sslmode=disable

1260:   cockroach workload run kv --init --concurrency 4 --display-every=60s --batch 10
argv[0]: cockroach
argv[1]: workload
argv[2]: run
argv[3]: kv
argv[4]: --init
argv[5]: --concurrency
argv[6]: 4
argv[7]: --display-every=60s
argv[8]: --batch
argv[9]: 10
argv[10]: --max-block-bytes
argv[11]: 1024
argv[12]: --min-block-bytes
argv[13]: 1024
argv[14]: postgresql://root@192.168.1.43:26257/kv?sslmode=disable
root@loadgen0:~/bigdb# kill 1248 1253 1260
root@loadgen0:~/bigdb# date
November 10, 2020 at 04:28:27 PM UTC
----

Then I shut down CockroachDB on all three instances.  It timed out after 60s on nvmedb3.  On all three I took a snapshot with `zfs snapshot tank/cockroachdb@2020-11-10-0-before-upgrade`.  Then I started up CockroachDB again.  The cluster looks healthy and I've confirmed that it's on v20.2.0-RC.3.

I'm going to start a light workload with this script:

[source,text]
----
#!/bin/bash

date="$(date +%FT%TZ)"
for node in 52 159 43; do
        nohup cockroach workload run kv --histograms kv-histograms-$node-$date.out --concurrency 4 --max-rate=333 --display-every=1s --read-percent 80 --tolerate-errors postgresql://root@192.168.1.$node:26257/kv?sslmode=disable > loadgen-summary-$node-$date.out 2>&1 &
done
----

I started this at 2020-11-10T16:40:09.  I'm going to let this run for a little while, then begin the rolling upgrade process.  Using https://www.cockroachlabs.com/docs/v20.2/upgrade-cockroach-version.html[docs here].

Returning to this at 23:00Z.  Upgrade steps from the doc above:

* version: it's not totally obvious that this should work from v20.2.0-RC.3 to v20.2.0-RC.4, but I'd expect it to work.
* load balancing: not an issue -- load generators talking directly to db nodes and they will see impact.  Understood.
* node status / replication status: I can't directly check this because of the Admin UI issue.  However, the Metrics page shows a summary with 0 unavailable ranges and 3 total nodes and no warnings about suspect or dead nodes.  My Grafana dashboard shows 0 under-replicated ranges, too.
* node list: can't easily check node list because of the above Admin UI issue but there are plenty of CPU, memory, and disk resources available and everything should be on RC.3.
* breaking changes: none in this RC and none that should affect us
* ongoing bulk operations: no bulk imports or schema changes in progress.  No jobs shown.
* finalization: going to let it do this automatically.
* the upgrade itself.  for each node:
** drain and stop the node with SIGTERM followed by SIGKILL after 60 seconds. (`svcadm disable -s cockroachdb`)
** replace binary
** start the cluster again (`svcadm enable -s cockroachdb`)
** verify that the node is back online and part of the cluster (e.g., through log)
** verify SQL connectivity
** wait at least 1 minute before moving on to the next one

First node:

[source,text]
----
root@nvmedb1:/cockroachdb/bin# ls -l
total 514444
lrwxrwxrwx   1 cockroachdb staff         22 Nov 10 01:00 cockroach -> cockroach-v20.2.0-rc.3
-rwxr-xr-x   1 cockroachdb staff    337463488 Nov  6 22:02 cockroach-v20.2.0-rc.3
-rwxr-xr-x   1 cockroachdb staff    337463824 Nov  3 22:57 cockroach-v20.2.0-rc.4
-rwxr-xr-x   1 cockroachdb staff        552 Oct 28 22:25 configure_cluster
-rwxr-xr-x   1 cockroachdb staff        576 Oct 28 22:25 configure_haproxy
-rwxr-xr-x   1 cockroachdb staff    17089752 Sep  1 15:21 haproxy
root@nvmedb1:/cockroachdb/bin# date +%FT%TZ; time svcadm disable -s cockroachdb
2020-11-10T23:13:24Z

real    0m2.869s
user    0m0.007s
sys     0m0.020s
root@nvmedb1:/cockroachdb/bin# ln -s -f cockroach-v20.2.0-rc.4 cockroach
root@nvmedb1:/cockroachdb/bin# ls -l
total 514444
lrwxrwxrwx   1 root     root          22 Nov 10 23:13 cockroach -> cockroach-v20.2.0-rc.4
-rwxr-xr-x   1 cockroachdb staff    337463488 Nov  6 22:02 cockroach-v20.2.0-rc.3
-rwxr-xr-x   1 cockroachdb staff    337463824 Nov  3 22:57 cockroach-v20.2.0-rc.4
-rwxr-xr-x   1 cockroachdb staff        552 Oct 28 22:25 configure_cluster
-rwxr-xr-x   1 cockroachdb staff        576 Oct 28 22:25 configure_haproxy
-rwxr-xr-x   1 cockroachdb staff    17089752 Sep  1 15:21 haproxy
root@nvmedb1:/cockroachdb/bin# time svcadm enable -s cockroachdb; date +%FT%TZ

real    0m1.498s
user    0m0.008s
sys     0m0.023s
2020-11-10T23:13:55Z
root@nvmedb1:/cockroachdb/bin# date -u
November 10, 2020 at 11:14:55 PM GMT
----

Second node: I tried to automate this one but it timed out during shut down and required me to intervene:

WARN: See notes on 11/19.  nvmedb2 and nvmedb3 are backwards here.

[source,text]
----
root@nvmedb2:/cockroachdb/bin# ls -l && date +%FT%TZ && time svcadm disable -s cockroachdb && ln -s -f cockroach-v20.2.0-rc.4 cockroach && ls -l && time svcadm enable -s cockroachdb && echo okay; date +%FT%TZ
total 514446
lrwxrwxrwx   1 cockroachdb staff         22 Nov 10 01:00 cockroach -> cockroach-v20.2.0-rc.3
-rwxr-xr-x   1 cockroachdb staff    337463488 Nov  6 22:02 cockroach-v20.2.0-rc.3
-rwxr-xr-x   1 cockroachdb staff    337463824 Nov  3 22:57 cockroach-v20.2.0-rc.4
-rwxr-xr-x   1 cockroachdb staff        552 Oct 28 22:25 configure_cluster
-rwxr-xr-x   1 cockroachdb staff        576 Oct 28 22:25 configure_haproxy
-rwxr-xr-x   1 cockroachdb staff    17089752 Sep  1 15:21 haproxy
2020-11-10T23:15:54Z
^C
real    1m36.908s
user    0m0.011s
sys     0m0.030s

root@nvmedb2:/cockroachdb/bin# ls -l; ln -s -f cockroach-v20.2.0-rc.4 cockroach && ls -l && time svcadm enable -s cockroachdb && echo okay; date +%FT%TZ
total 514446
lrwxrwxrwx   1 cockroachdb staff         22 Nov 10 01:00 cockroach -> cockroach-v20.2.0-rc.3
-rwxr-xr-x   1 cockroachdb staff    337463488 Nov  6 22:02 cockroach-v20.2.0-rc.3
-rwxr-xr-x   1 cockroachdb staff    337463824 Nov  3 22:57 cockroach-v20.2.0-rc.4
-rwxr-xr-x   1 cockroachdb staff        552 Oct 28 22:25 configure_cluster
-rwxr-xr-x   1 cockroachdb staff        576 Oct 28 22:25 configure_haproxy
-rwxr-xr-x   1 cockroachdb staff    17089752 Sep  1 15:21 haproxy
total 514446
lrwxrwxrwx   1 root     root          22 Nov 10 23:17 cockroach -> cockroach-v20.2.0-rc.4
-rwxr-xr-x   1 cockroachdb staff    337463488 Nov  6 22:02 cockroach-v20.2.0-rc.3
-rwxr-xr-x   1 cockroachdb staff    337463824 Nov  3 22:57 cockroach-v20.2.0-rc.4
-rwxr-xr-x   1 cockroachdb staff        552 Oct 28 22:25 configure_cluster
-rwxr-xr-x   1 cockroachdb staff        576 Oct 28 22:25 configure_haproxy
-rwxr-xr-x   1 cockroachdb staff    17089752 Sep  1 15:21 haproxy
svcadm: Instance "svc:/application/cockroachdb:default" is in maintenance state.

real    0m0.060s
user    0m0.006s
sys     0m0.018s
2020-11-10T23:17:47Z
root@nvmedb2:/cockroachdb/bin# svcadm clear cockroachdb
root@nvmedb2:/cockroachdb/bin# svcs cockroachdb
STATE          STIME    FMRI
online         23:17:54 svc:/application/cockroachdb:default
----

Last node (had a false start in here):

WARN: See notes on 11/19.  nvmedb2 and nvmedb3 are backwards here.

[source,text]
----
root@nvmedb3:~# svcs cockroachdb
STATE          STIME    FMRI
online         16:35:28 svc:/application/cockroachdb:default
root@nvmedb3:~# ls -l && date +%FT%TZ && time svcadm disable -s cockroachdb && ln -s -f cockroach-v20.2.0-rc.4 cockroach && ls -l && time svcadm enable -s cockroachdb && echo okay; date +%FT%TZ
total 0
2020-11-10T23:19:26Z
^C
real    1m13.367s
user    0m0.010s
sys     0m0.027s

root@nvmedb3:~# tail $(svcs -L cockroachdb)
* consider --accept-sql-without-tls instead. For other options, see:
*
* - https://go.crdb.dev/issue-v/53404/v20.2
* - https://www.cockroachlabs.com/docs/v20.2/secure-a-cluster.html
*
[ Nov 10 16:35:28 Method "start" exited with status 0. ]
[ Nov 10 23:19:26 Stopping because service disabled. ]
[ Nov 10 23:19:26 Executing stop method (:kill). ]
initiating graceful shutdown of server
[ Nov 10 23:20:26 Method or service exit timed out.  Killing contract 4190. ]
root@nvmedb3:~# svcs cockroachdb
STATE          STIME    FMRI
maintenance    23:20:26 svc:/application/cockroachdb:default
root@nvmedb3:~# svcadm clear cockroachdb
root@nvmedb3:~# svcs cockroachdb
STATE          STIME    FMRI
disabled       23:21:00 svc:/application/cockroachdb:default
root@nvmedb3:~# ls -l && date +%FT%TZ && time svcadm disable -s cockroachdb && ln -s -f cockroach-v20.2.0-rc.4 cockroach && ls -l && time svcadm enable -s cockroachdb && echo okay; date +%FT%TZ
total 0
2020-11-10T23:21:08Z

real    0m0.060s
user    0m0.006s
sys     0m0.018s
total 1
lrwxrwxrwx   1 root     root          22 Nov 10 23:21 cockroach -> cockroach-v20.2.0-rc.4

real    0m0.471s
user    0m0.007s
sys     0m0.021s
okay
2020-11-10T23:21:08Z
root@nvmedb3:~# ls -l
total 1
lrwxrwxrwx   1 root     root          22 Nov 10 23:21 cockroach -> cockroach-v20.2.0-rc.4
root@nvmedb3:~# rm -f cockroach
root@nvmedb3:~# cd /cockroachdb/bin/
root@nvmedb3:/cockroachdb/bin# ls -l
total 514442
lrwxrwxrwx   1 cockroachdb staff         22 Nov 10 01:00 cockroach -> cockroach-v20.2.0-rc.3
-rwxr-xr-x   1 cockroachdb staff    337463488 Nov  6 22:02 cockroach-v20.2.0-rc.3
-rwxr-xr-x   1 cockroachdb staff    337463824 Nov  3 22:57 cockroach-v20.2.0-rc.4
-rwxr-xr-x   1 cockroachdb staff        552 Oct 28 22:25 configure_cluster
-rwxr-xr-x   1 cockroachdb staff        576 Oct 28 22:25 configure_haproxy
-rwxr-xr-x   1 cockroachdb staff    17089752 Sep  1 15:21 haproxy
root@nvmedb3:/cockroachdb/bin# ls -l && date +%FT%TZ && time svcadm disable -s cockroachdb && ln -s -f cockroach-v20.2.0-rc.4 cockroach && ls -l && time svcadm enable -s cockroachdb && echo okay; date +%FT%TZ
total 514442
lrwxrwxrwx   1 cockroachdb staff         22 Nov 10 01:00 cockroach -> cockroach-v20.2.0-rc.3
-rwxr-xr-x   1 cockroachdb staff    337463488 Nov  6 22:02 cockroach-v20.2.0-rc.3
-rwxr-xr-x   1 cockroachdb staff    337463824 Nov  3 22:57 cockroach-v20.2.0-rc.4
-rwxr-xr-x   1 cockroachdb staff        552 Oct 28 22:25 configure_cluster
-rwxr-xr-x   1 cockroachdb staff        576 Oct 28 22:25 configure_haproxy
-rwxr-xr-x   1 cockroachdb staff    17089752 Sep  1 15:21 haproxy
2020-11-10T23:21:35Z

real    0m2.558s
user    0m0.007s
sys     0m0.020s
total 514442
lrwxrwxrwx   1 root     root          22 Nov 10 23:21 cockroach -> cockroach-v20.2.0-rc.4
-rwxr-xr-x   1 cockroachdb staff    337463488 Nov  6 22:02 cockroach-v20.2.0-rc.3
-rwxr-xr-x   1 cockroachdb staff    337463824 Nov  3 22:57 cockroach-v20.2.0-rc.4
-rwxr-xr-x   1 cockroachdb staff        552 Oct 28 22:25 configure_cluster
-rwxr-xr-x   1 cockroachdb staff        576 Oct 28 22:25 configure_haproxy
-rwxr-xr-x   1 cockroachdb staff    17089752 Sep  1 15:21 haproxy

real    0m0.935s
user    0m0.007s
sys     0m0.022s
okay
2020-11-10T23:21:38Z
----

This seems to have worked so far.  The dashboard issue is not fixed.  Will want to review the client data at some point.


=== Admin UI (DB Console) Overview page missing

I get a "page not found" error when I go to the Overview page on both v20.2.0-RC.3 and v20.2.0-RC.4.  I see this in Chrome, Firefox, and Safari.  I see one error in the console in both, but it looks related to parsing a font.

I spent some time trying to debug the JavaScript, but it appears to be both obfuscated and minified, so that's pretty hard.  I suspect a few possible things:

- some build problem that failed to include some important file but due to some bug was not fatal to the build
- this is accidentally not included in a `buildoss` build
- maybe a problem with gitattributes, if it's related to the error on the font and if that's related to git's mishandling of a binary file

The error on the font is:

[source,text]
----
Failed to decode downloaded font: http://localhost:8080/d9db862be7068cd30e3fe1c09b2f3931.woff2
OTS parsing error: invalid version tag
----

I've asked Josh about getting a build log to see if something went wrong during the build.  I will also reach out to the community tomorrow, either via Slack or an issue.


=== Wrap up for the day

I'm going to shut down the load generators.  This happened around 2020-11-11T00:09Z.

Plan for next:

* debug/fix admin ui problem
* looking at rolling upgrade client-side data
* don't forget to update proto-raw.tgz when I'm done here
* expansion test?
* kick off backup
* test restore

== 2020-11-11

Josh root-caused the problem with the Overview screen last night and built a fix this morning.  My plan for the day:

* apply and verify this fix
* kick off `cockroach dump` of "kv" database
* submit a PR to Cockroach

Next time, I'll do:

* expansion test?
* kick off distributed backup
* test restore
* look at rolling upgrade client-side data
* don't forget to update proto-raw.tgz when I'm done here

=== Console fix

I submitted a https://github.com/cockroachdb/cockroach/pull/56591[PR with Josh's fix to CockroachDB].  I got Bryan's permission to sign the CLA on our behalf.
 
=== `cockroach dump`

I'm arbitrarily going to do this on nvmedb1.  The database is using 131 GiB and the pool has another 1.5 TiB, so I'm not worried about disk space.

I ran this with:

[source,text]
----
$ nohup /cockroachdb/bin/cockroach dump --url postgresql://root@192.168.1.52:26257/?sslmode=disable kv > cockroach-dump.out 2> cockroach-dump.err
----

Some notes about the setup:

* CockroachDB v20.2.0, still illumos
* 3-node cluster in AWS using i3.2xlarge instance types (local NVME, 8 CPUs, 61 GiB of memory)
* local storage: 1769 GiB (for the CockroachDB pool, being used for both the database and the backup file).  Separate pool for root filesystem
* No client workload going on at all
* 308 total replicas, 383 GiB (entire cluster).  381.6 GiB in 272 ranges ("kv" database).

This appears to have run from 2020-11-11T22:04:26.527958311 to 2020-11-11T22:37:49.135255832 and produced 81 GiB of data.  I expected more like 127 GiB.  I'm not sure why it's different.  It's possible that there's some fragmentation?  Or some overwritten values?  I could try tuning down the "gc" interval.

XXX new open question: is there a way to see how much data is to-be-GC'd?

As a sanity check:

[source,text]
----
$ wc cockroach-dump.out
 44637940 90151145 87348554912 cockroach-dump.out

...

root@192.168.1.159:26257/kv> select count(*) from kv;
   count
------------
  43762680
(1 row)

Time: 192.303s total (execution 192.303s / network 0.001s)

root@192.168.1.159:26257/kv>
----

That's _nearly_ the same number of total rows.  The dump has an extra 875,000 lines for some reason.  This shouldn't happen because there was no workload running.

It looks like the dump has an INSERT INTO line every 100 lines or so, plus an extra blank line there.  So that might inflate things by about 2% of the real value.  If we add 2% to the `select count(*)` number, we get within 7 of the `wc` number -- that sounds good (and there are 6 lines up front for schema, too).  I confirmed that there are 437627 "INSERT INTO" lines.  With associated blank lines, that's 875254, which is the difference above.

=== Plan for next

* copy the backup file to S3 so I can reuse it
* check on CockroachDB PR
* expansion test?
* kick off distributed backup
* test restore
** cockroach dump
** distributed backup
* look at rolling upgrade client-side data
* don't forget to update proto-raw.tgz when I'm done here

== 2020-11-13

* The CockroachDB PR was merged.

=== Copying the backup file to S3

I spent some time trying to copy the cockroach-dump.out backup file from nvmedb 1 into S3 for future use.  This is surprisingly painful.

* I can't easily use the AWS CLI to copy it because the AWS CLI does not even have setup instructions for anything other than Docker, Linux, MacOS, or Windows.
* I can't easily create a presigned PUT URL to upload it with curl because the AWS CLI does not generate presigned URLs for anything other than GET requests.
* I wrote a little program to generate a presigned PUT URL, but now I can't complete the upload because only 5 GiB can be uploaded in a single object without using multi-part uploads.
* I started trying to copy it through my laptop, but this is going to suck.  Being 80 GiB, and while I was getting almost 100 Mbit at first, I'm only getting about 30 Mbps now and I only have 6 Mbps upstream, so this is no good.
* I considered splitting into N 5 GiB files.  This would be annoying, both up front and when downloading it in the future.
* I'm now considering provisioning an Amazon Linux VM solely for copying data.  This was incredibly slow -- I think I provisioned way too small an instance.
* Josh got back to me with https://gist.github.com/jclulow/06c0e9ed8bd18ff3efed0371c831b919[instructions] for installing the AWS CLI on illumos.  This would be a better investment of time.  I tried this on nvmedb1, but its root filesystem wasn't large enough for the build-essential install.  I then started doing this on "mon" before deciding it makes more sense to do this in lennier, my build VM.  However, that VM needs much more space to store the database dump.  I bumped up its volume size and also made the instance type bigger while I was at it.  I installed and configured the AWS CLI.

Finally, I got the file uploaded to S3 as `s3://oxide-cockroachdb-exploration/cockroach-dump-kv-2020-11-11.sql`.

=== Distributed backup

https://www.cockroachlabs.com/docs/stable/backup.html[Docs] here.

[source,text]
----
BACKUP DATABASE kv TO "nodelocal://1/backup-2020-11-13" AS OF SYSTEM TIME "2020-11-13T23:40:00Z" WITH DETACHED
----

This failed and I filed an issue about it:
https://github.com/cockroachdb/cockroach/issues/56683

=== Plan for next

* check on CockroachDB issue: https://github.com/cockroachdb/cockroach/issues/56683
* blocked on above: kick off distributed backup
* expansion test
* test restore
** cockroach dump
** distributed backup
* look at rolling upgrade client-side data
* don't forget to update proto-raw.tgz when I'm done here

== 2020-11-18

=== Licensing and Distributed Backup

Re: issue https://github.com/cockroachdb/cockroach/issues/56683[56683].

Conclusion of the https://github.com/cockroachdb/cockroach/issues/56683[CockroachDB issue] is that they made the feature free to use, but it's not part of the open-source product, apparently because that was too much work.

Our internal conclusion is to stick with the OSS build for now.  There's some suspicion that the CCL allows Cockroach Labs to start charging for existing features at any time.  The licensing situation definitely seems like a mess for a few reasons:

* The CCL has a definition of Cockroach Core (the free version) that appears to include everything on github.com/cockroachdb/cockroach, which includes CCL and Enterprise-features code.
* The CCL claims that you agree to it by just downloading Cockroach, let alone using it -- even the free version.
* There are a bunch of free features licensed under CCL for some reason (of which Backup appears to be one).  It was confusing enough that you had ((Apache, BSL), CCL) when I thought that corresponded with (free, enterprise-paid), but now it's even more confusing.  Also, the blog post announcing distributed backup says it's part of Cockroach Core, even though it's not under BSL.

As a result, I'll be skipping subsequent "distributed backup" testing.

=== Expansion / contraction, once more

Now that I've got a cluster with a lot more resources (in terms of memory, CPU, and I/O), we're going to do another expansion/contraction test.  For workload, I'm going to repeat what I did on 10/29, which was used for the 11/2 expansion testing:

[source,text]
----
date="$(date +%FT%TZ)"
for node in 52 43 159; do
	nohup cockroach workload run kv --histograms kv-histograms-$node-$date.out --concurrency 4 --max-rate=333 --display-every=1s --read-percent 80 --tolerate-errors postgresql://root@192.168.1.$node:26257/kv?sslmode=disable > loadgen-summary-$node-$date.out 2>&1 &
done
----

Here's me running it:

[source,text]
----
root@loadgen0:~# mkdir expansion
root@loadgen0:~# cd !$
cd expansion
root@loadgen0:~/expansion# ls
root@loadgen0:~/expansion# date="$(date +%FT%TZ)"
root@loadgen0:~/expansion# for node in 52 43 159; do
> nohup cockroach workload run kv --histograms kv-histograms-$node-$date.out --concurrency 4 --max-rate=333 --display-every=1s --read-percent 80 --tolerate-errors postgresql://root@192.168.1.$node:26257/kv?sslmode=disable > loadgen-summary-$node-$date.out 2>&1 &
> done
[1] 26884
[2] 26885
[3] 26886
root@loadgen0:~/expansion#
root@loadgen0:~/expansion# ls -l
total 54
-rw-r--r--   1 root     root        7677 Nov 18 22:23 kv-histograms-159-2020-11-18T22:23:39Z.out
-rw-r--r--   1 root     root        7682 Nov 18 22:23 kv-histograms-43-2020-11-18T22:23:39Z.out
-rw-r--r--   1 root     root        7680 Nov 18 22:23 kv-histograms-52-2020-11-18T22:23:39Z.out
-rw-r--r--   1 root     root        1260 Nov 18 22:23 loadgen-summary-159-2020-11-18T22:23:39Z.out
-rw-r--r--   1 root     root        1160 Nov 18 22:23 loadgen-summary-43-2020-11-18T22:23:39Z.out
-rw-r--r--   1 root     root        1258 Nov 18 22:23 loadgen-summary-52-2020-11-18T22:23:39Z.out
root@loadgen0:~/expansion#
----

I'd like to let this run a little while before beginning expansion.

Summary of current situation:

- database "kv": 381.2 GiB replicated size. 258 ranges.
- cluster: 3 nodes, 7.7% capacity used (disk space), idle for the last several days
- instances: i3.2xlarge (60 GiB memory, 8 VCPUs, 1700 GiB NVME SSD)
- OS: illumos omnios-r151034-b3d6e2addc
- CockroachDB: local OSS build of v20.2.0
- load generators: 62

As of 22:44Z, performance has been quite stable for over 20 minutes, so I'm going to try the first expansion shortly.  There's plenty of CPU headroom, disk %busy headroom, etc.

Bringing up nvmedb4:

[source,text]
----
root@nvmedb4:~# date +%FT%TZ; svcadm enable -s cockroachdb; date +%FT%TZ
2020-11-18T22:45:04Z
2020-11-18T22:45:04Z
root@nvmedb4:~#
----

I should have started this sooner: I had wanted to instrument fsync times.  I started this on all four existing nodes:

[source,text]
----
# dtrace -q -n 'syscall::f*sync:entry{ self->start = timestamp; } syscall::f*sync:return/self->start/{ printf("%Y.%09u %-8s %7d us\n", walltimestamp, walltimestamp % 1000000000, probefunc, (timestamp - self->start) / 1000); self->start = 0; }' | tee /cockroachdb/fsync_times.out
----

I'm also starting this on node 5 _before_ bringing it up.  I will have fsync output for that whole expansion and subsequent contractions.

I ended up starting CockroachDB on nvmedb4 at:

[source,text]
----
root@nvmedb4:~# svcs cockroachdb
STATE          STIME    FMRI
online         22:45:04 svc:/application/cockroachdb:default
root@nvmedb4:~#
----

It wasn't until 23:53Z that it appeared to finish replicating all ranges.  That's considerably slower than before, but th eimpact was a lot smaller.  There was definitely some impact to p95 and especially p99, but this wasn't as visible in throughput (possibly because we weren't running it full tilt like last time).  CPU utilization peaked around 50% per CPU, and about the same for disk %busy.

I'm going to start another expansion now:

[source,text]
----
root@nvmedb5:~# date +%FT%TZ; svcadm enable -s cockroachdb; date +%FT%TZ
2020-11-19T00:03:20Z
2020-11-19T00:03:20Z
root@nvmedb5:~#
----

I've still got the fsync instrumentation running everywhere.

This expansion converged at about 00:56Z.

Next, I'll try a contraction operation.  At 02:18Z, I ran `cockroach node decommission 5`.

While I wait for this, I'm thinking about the data that would be useful to graph:

* client-side: p95, p99, throughput (similar to what I did before)
* server side: CPU utilization, disk utilization
* fsync times:
** try plotting every data point, but this will probably go badly
** plot max() for each wall clock second across all data points, all nodes
** compute overall distribution of latency for the whole period

Decommissioning completed around 02:39:57Z -- about 22 minutes.

I'm going to let the workload run undisturbed for another 15 minutes before decommissioning node 4.

[source,text]
----
root@loadgen0:~# date +%FT%TZ && sleep 900 && cockroach node decommission 4 && date +%FT%TZ
2020-11-19T02:41:44Z

  id | is_live | replicas | is_decommissioning |   membership    | is_draining
-----+---------+----------+--------------------+-----------------+--------------
   4 |  true   |      225 |        true        | decommissioning |    false
...
(1 row)
  id | is_live | replicas | is_decommissioning |   membership   | is_draining
-----+---------+----------+--------------------+----------------+--------------
   4 |  true   |        0 |        true        | decommissioned |    false
(1 row)

No more data reported on target nodes. Please verify cluster health before removing the nodes.
2020-11-19T03:30:56Z
root@loadgen0:~#
----

I stopped fsync data collection and the workload at 15:15Z (the next morning PT).

== 2020-11-19

Timeline from yesterday's testing:

[source,text]
----
2020-11-18T22:23:39Z start client workload
2020-11-18T22:45:04Z start node 4
2020-11-18T23:53:00Z range movement completed
2020-11-19T00:03:20Z start node 5
2020-11-19T00:56:00Z range movement completed
2020-11-19T02:18:00Z decommission node 5 started
2020-11-19T02:39:57Z decommission node 5 completed
2020-11-19T02:56:44Z decommission node 4 started
2020-11-19T03:30:56Z decommission node 4 completed
----

Based on this, we're probably interested in data from 2020-11-18T22:23:39Z until about 2020-11-19T04:00:00Z.  I'm going to trim the fsync data files accordingly to make them more manageable.  I'm doing this:

[source,text]
----
time awk '{ print; } /04:00:/{ exit }' /cockroachdb/fsync_times.out > /cockroachdb/fsync_times_trimmed.out
----

The output looks about right.  It's still quite a lot of data per node, though -- too much to comfortably put into this repository.  I'm going to write an awk script that will take the maximum value in any calendar second.

[source,text]
----
#
# Given output of DTrace script that traced fsync times, produce a summary file
# with the maximum fsync time for each calendar second that had any data points
# in the input file.
#
# The input file consists of lines like this:
#
#     2020 Nov 19 00:03:20.536045823 fdsync       123 us
#

BEGIN{
	current_second = 0;
	current_max = 0;
}

$2 != "Nov"{
	printf("hardcoded for november!");
	exit(1);
}

{
	split($4, a, ".");
	clocksecond = $1 "-11-" $3 "T" a[1];
	if (current_second != clocksecond) {
		if (current_max > 0) {
			printf("%s %d\n", current_second, current_max);
		}

		current_second = clocksecond;
		current_max = $6;
	} else if ($6 > current_max) {
		current_max = $6;
	}
}

END {
	if (current_max > 0) {
		printf("%s %d\n", current_second, current_max);
	}
}
----

Run with:

[source,text]
----
time awk -f fsync_summarize.awk /cockroachdb/fsync_times_trimmed.out > /cockroachdb/fsync_summary.out
----

This produced much smaller files.

=== Analyzing fsync times

As I described above, I modified the data a little bit.  For each of the five database nodes, I have:

- fsync_times.out: raw data from DTrace enabling for the entire period
- fsync_times_trimmed.out: same as fsync_times.out, but ignoring all records after 0400Z
- fsync_summary.out: contains the *maximum* value of fsync latency for any given wall clock second

I copied all five "trimmed" files to a dev zone (lennier), where I will archive these to S3.

I'm also doing some basic analysis here.  I concatenated all the files together and created a histogram of all the latency data points.  I also sorted them by time and again by value:

[source,text]
----
cat * > all-fsync-times-trimmed.out
sort all-fsync-times-trimmed.out > all-fsync-times-trimmed-sorted.out
sort -k6n,6 all-fsync-times-trimmed.out > all-fsync-times-trimmed-sorted-by-value.out
----

I created a histogram using the Node `hist` tool:

[source,text]
----
$ cat all-fsync-times-trimmed.out | awk '{print $6}' | ./node_modules/.bin/hist
           value  ------------- Distribution ------------- count
               1 |                                         0
               2 |                                         569
               4 |                                         7908
               8 |                                         137
              16 |                                         40
              32 |                                         35
              64 |                                         125
             128 |                                         42345
             256 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@   21000896
             512 |@@                                       846972
            1024 |@                                        354598
            2048 |                                         70011
            4096 |                                         31505
            8192 |                                         17402
           16384 |                                         8142
           32768 |                                         505
           65536 |                                         323
          131072 |                                         404
          262144 |                                         522
          524288 |                                         157
         1048576 |                                         2
         2097152 |                                         0
----

This summary annotates this data with the maximum percentile contained in each bucket:

[source,text]
----
Min lat(us) Count       Max %ile in bucket
----------- ----------- ------------------
1           0           0.000%
2           569         0.003%
4           7908        0.038%
8           137         0.038%
16          40          0.039%
32          35          0.039%
64          125         0.039%
128         42345       0.229%
256         21000896    94.055%
512         846972      97.840%
1024        354598      99.424%
2048        70011       99.737%
4096        31505       99.877%
8192        17402       99.955%
16384       8142        99.991%
32768       505         99.994%
65536       323         99.995%
131072      404         99.997%
262144      522         99.999%
524288      157         100.000%
1048576     2           100.000%
2097152     0           100.000%
----

This tells us that:

* p95 < 1ms
* p99 < 2ms

There are 22,382,598 total data points.  Since I also have the file sorted by value, we can get more precise values for these percentiles.

p50 (median) = line 11191299
p95 = line 21263468
p99 = line 22158772
p995 = line 22270685
p999 = line 22360215

[source,text]
----
dap@lennier:~/nvmedb_data$ cat -n all-fsync-times-trimmed-sorted-by-value.out | awk '$1 == 11191299 || $1 == 21263468 || $1 == 22158772 || $1 == 22270685 || $1 == 22360215'
11191299        2020 Nov 18 23:45:34.039325144 fdsync       329 us
21263468        2020 Nov 19 03:53:10.466926990 fdsync       542 us
22158772        2020 Nov 19 00:04:58.955049856 fdsync      1598 us
22270685        2020 Nov 19 03:06:47.674328850 fdsync      2255 us
22360215        2020 Nov 19 03:07:46.387579487 fdsync      9669 us
dap@lennier:~/nvmedb_data$ 
----

To compute the average:

[source,text]
----
dap@lennier:~/nvmedb_data$ awk '{ s += $6 } END { print s }' all-fsync-times-trimmed-sorted-by-value.out
9327788734
----

Or summarized fsync latency across all fsyncs on all nodes:

[source,text]
----
avg  =  416 us
p50  =  329 us
p95  =  542 us
p99  = 1598 us
p995 = 2255 us
p999 = 9669 us
----

While looking around for expected fsync times on Linux, I found https://www.percona.com/blog/2018/02/08/fsync-performance-storage-devices/[this blog post from Percona].  I suspect this is what Keith from Cockroach Labs found, and it claims ZFS fsync takes on the order of 10ms, which is definitely wrong.  But that's the author's home server with 7200 RPM drives and no slog.

I've copied the raw fsync data to S3:

[source,text]
----
dap@lennier:~/nvmedb_data$ aws s3 ls s3://oxide-cockroachdb-exploration/data/2020-11-19/
2020-11-19 19:03:01   41153743 nvmedb1-fsync_times_trimmed.out.gz
2020-11-19 19:03:08   42268402 nvmedb2-fsync_times_trimmed.out.gz
2020-11-19 19:03:15   42493882 nvmedb3-fsync_times_trimmed.out.gz
2020-11-19 19:03:21   28035414 nvmedb4-fsync_times_trimmed.out.gz
2020-11-19 19:03:28   12608327 nvmedb5-fsync_times_trimmed.out.gz
----

=== Client-side data

I copied the summary files and kv-histograms files from the load generator zone to my main system.

I trimmed them using a similar process to what I did for the fsync files:

[source,text]
----
for file in kv-histograms-*; do awk '{print;} /04:00:/{ exit }' $file > $file.trimmed; done
----

Now I'm following similar steps to my last analysis:

[source,text]
----
chist print client-data/kv-histograms-*.trimmed > chist-all.out
...
dap@zathras cockroachdb-testing-round2 $ mkdir analysis/2020-11-18-nvme-expansion-contraction
dap@zathras cockroachdb-testing-round2 $ cd !$
cd analysis/2020-11-18-nvme-expansion-contraction
dap@zathras 2020-11-18-nvme-expansion-contraction $ ls
dap@zathras 2020-11-18-nvme-expansion-contraction $ ln -s ../../data/2020-11-18-nvme-expansion-contraction/client-data
dap@zathras 2020-11-18-nvme-expansion-contraction $ ls -l
total 0
lrwxr-xr-x  1 dap  staff  60 Nov 19 11:13 client-data@ -> ../../data/2020-11-18-nvme-expansion-contraction/client-data
dap@zathras 2020-11-18-nvme-expansion-contraction $ mv client-data/chist-all.out .
dap@zathras 2020-11-18-nvme-expansion-contraction $ awk 'NR == 1 || $2 == "write"' chist-all.out > chist-writes.out
dap@zathras 2020-11-18-nvme-expansion-contraction $ awk 'NR == 1 || $2 == "read"' chist-all.out > chist-reads.out
dap@zathras 2020-11-18-nvme-expansion-contraction $ awk '$3 != 3' chist-*.out
TIME                     OPNAM  # OPS/SEC p50(ms) p95(ms) p99(ms)    pMax
TIME                     OPNAM  # OPS/SEC p50(ms) p95(ms) p99(ms)    pMax
TIME                     OPNAM  # OPS/SEC p50(ms) p95(ms) p99(ms)    pMax
dap@zathras 2020-11-18-nvme-expansion-contraction $ ls -l
total 11992
-rw-r--r--  1 dap  staff  2986640 Nov 19 11:11 chist-all.out
-rw-r--r--  1 dap  staff  1493394 Nov 19 11:14 chist-reads.out
-rw-r--r--  1 dap  staff  1493320 Nov 19 11:13 chist-writes.out
lrwxr-xr-x  1 dap  staff       60 Nov 19 11:13 client-data@ -> ../../data/2020-11-18-nvme-expansion-contraction/client-data
dap@zathras 2020-11-18-nvme-expansion-contraction $ cat chist-reads.out | awk '$1 != "TIME"{ print $1,$4 }' > plot-data-reads-throughput.out
dap@zathras 2020-11-18-nvme-expansion-contraction $ cat chist-reads.out | awk '$1 != "TIME"{ print $1,$8 }' > plot-data-reads-pMax-latency.out
dap@zathras 2020-11-18-nvme-expansion-contraction $ cat chist-reads.out | awk '$1 != "TIME"{ print $1,$7 }' > plot-data-reads-p99-latency.out
dap@zathras 2020-11-18-nvme-expansion-contraction $ cat chist-writes.out | awk '$1 != "TIME"{ print $1,$4 }' > plot-data-writes-throughput.out
dap@zathras 2020-11-18-nvme-expansion-contraction $ cat chist-writes.out | awk '$1 != "TIME"{ print $1,$8 }' > plot-data-writes-pMax-latency.out
dap@zathras 2020-11-18-nvme-expansion-contraction $ cat chist-writes.out | awk '$1 != "TIME"{ print $1,$7 }' > plot-data-writes-p99-latency.out
dap@zathras 2020-11-18-nvme-expansion-contraction $ cp ../2020-11-02-nvme-expansion-contraction/gengraphs.gnuplot .
dap@zathras 2020-11-18-nvme-expansion-contraction $ vim gengraphs.gnuplot
dap@zathras 2020-11-18-nvme-expansion-contraction $
dap@zathras 2020-11-18-nvme-expansion-contraction $ gnuplot gengraphs.gnuplot 
dap@zathras 2020-11-18-nvme-expansion-contraction $ 
----

Overall, this does look a lot better than previous runs.  Reminder of key differences:

* i3.2xlarge instances (much more DRAM, VCPUs, maybe more dedicated I/O)
* CockroachDB v20.2.0 OSS build
* instrumented every fsync call on all nodes for a bunch of this testing
* I backed off the clients so that they're targeting a particular rate rather than going as fast as they can.

The latency and throughput are clearly degraded while this is going on, but the effect isn't nearly so significant, particularly for throughput.  There are lots more individual data points (seconds) in which throughput was a fair bit lower, but it's clear the majority of per-second throughput data points remain good.  There might be ways to improve this graph to make it clearer.

To clean up a bit, I'm going to remove the original out files and gzip all of the raw data files.

=== `cockroach debug zip`

For reference, I tried this out today:

[source,text]
----
root@nvmedb1:/cockroachdb# /cockroachdb/bin/cockroach debug zip --insecure --host 192.168.1.52 foo.zip
establishing RPC connection to 192.168.1.52:26257...
retrieving the node status to get the SQL address...
using SQL address: 192.168.1.52:26257
using SQL connection URL: postgresql://root@192.168.1.52:26257/system?application_name=%24+cockroach+zip&connect_timeout=15&sslmode=disable
writing foo.zip
requesting data for debug/events... writing: debug/events.json
requesting data for debug/rangelog... writing: debug/rangelog.json
requesting data for debug/settings... writing: debug/settings.json
requesting data for debug/reports/problemranges... writing: debug/reports/problemranges.json
retrieving SQL data for crdb_internal.cluster_queries... writing: debug/crdb_internal.cluster_queries.txt
retrieving SQL data for crdb_internal.cluster_sessions... writing: debug/crdb_internal.cluster_sessions.txt
retrieving SQL data for crdb_internal.cluster_settings... writing: debug/crdb_internal.cluster_settings.txt
retrieving SQL data for crdb_internal.cluster_transactions... writing: debug/crdb_internal.cluster_transactions.txt
retrieving SQL data for crdb_internal.jobs... writing: debug/crdb_internal.jobs.txt
retrieving SQL data for system.jobs... writing: debug/system.jobs.txt
retrieving SQL data for system.descriptor... writing: debug/system.descriptor.txt
retrieving SQL data for system.namespace... writing: debug/system.namespace.txt
retrieving SQL data for system.namespace2... writing: debug/system.namespace2.txt
retrieving SQL data for crdb_internal.kv_node_status... writing: debug/crdb_internal.kv_node_status.txt
retrieving SQL data for crdb_internal.kv_store_status... writing: debug/crdb_internal.kv_store_status.txt
retrieving SQL data for crdb_internal.schema_changes... writing: debug/crdb_internal.schema_changes.txt
retrieving SQL data for crdb_internal.partitions... writing: debug/crdb_internal.partitions.txt
retrieving SQL data for crdb_internal.zones... writing: debug/crdb_internal.zones.txt
requesting nodes... writing: debug/nodes.json
requesting liveness... writing: debug/liveness.json
requesting CPU profiles... ok
writing: debug/nodes/1/cpu.pprof
writing: debug/nodes/2/cpu.pprof
writing: debug/nodes/3/cpu.pprof
writing: debug/nodes/4/cpu.pprof
writing: debug/nodes/5/cpu.pprof
writing: debug/nodes/1/status.json
using SQL connection URL for node 1: postgresql://root@192.168.1.52:26257/system?application_name=%24+cockroach+zip&connect_timeout=15&sslmode=disable
retrieving SQL data for crdb_internal.feature_usage... writing: debug/nodes/1/crdb_internal.feature_usage.txt
retrieving SQL data for crdb_internal.gossip_alerts... writing: debug/nodes/1/crdb_internal.gossip_alerts.txt
retrieving SQL data for crdb_internal.gossip_liveness... writing: debug/nodes/1/crdb_internal.gossip_liveness.txt
retrieving SQL data for crdb_internal.gossip_network... writing: debug/nodes/1/crdb_internal.gossip_network.txt
retrieving SQL data for crdb_internal.gossip_nodes... writing: debug/nodes/1/crdb_internal.gossip_nodes.txt
retrieving SQL data for crdb_internal.leases... writing: debug/nodes/1/crdb_internal.leases.txt
retrieving SQL data for crdb_internal.node_build_info... writing: debug/nodes/1/crdb_internal.node_build_info.txt
retrieving SQL data for crdb_internal.node_metrics... writing: debug/nodes/1/crdb_internal.node_metrics.txt
retrieving SQL data for crdb_internal.node_queries... writing: debug/nodes/1/crdb_internal.node_queries.txt
retrieving SQL data for crdb_internal.node_runtime_info... writing: debug/nodes/1/crdb_internal.node_runtime_info.txt
retrieving SQL data for crdb_internal.node_sessions... writing: debug/nodes/1/crdb_internal.node_sessions.txt
retrieving SQL data for crdb_internal.node_statement_statistics... writing: debug/nodes/1/crdb_internal.node_statement_statistics.txt
retrieving SQL data for crdb_internal.node_transaction_statistics... writing: debug/nodes/1/crdb_internal.node_transaction_statistics.txt
retrieving SQL data for crdb_internal.node_transactions... writing: debug/nodes/1/crdb_internal.node_transactions.txt
retrieving SQL data for crdb_internal.node_txn_stats... writing: debug/nodes/1/crdb_internal.node_txn_stats.txt
requesting data for debug/nodes/1/details... writing: debug/nodes/1/details.json
requesting data for debug/nodes/1/gossip... writing: debug/nodes/1/gossip.json
requesting data for debug/nodes/1/enginestats... writing: debug/nodes/1/enginestats.json
requesting stacks for node 1... writing: debug/nodes/1/stacks.txt
requesting threads for node 1... writing: debug/nodes/1/threads.txt
requesting heap profile for node 1... writing: debug/nodes/1/heap.pprof
requesting heap files for node 1... 200 found
writing: debug/nodes/1/heapprof/memprof.2020-11-17T10_29_15.220.168543696.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T10_29_25.220.205207112.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T10_29_35.221.243794024.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T10_30_25.221.248857072.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T10_31_15.222.253543464.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T11_31_15.276.137566504.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T11_31_25.277.174090152.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T11_31_35.277.211567088.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T11_31_45.277.250901336.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T12_46_05.338.246233112.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T13_16_45.366.249351456.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T13_17_35.367.250943208.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T13_18_35.368.251243800.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T13_19_25.368.252408288.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T14_19_25.416.160689464.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T14_19_35.416.197900960.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T14_19_45.416.237943912.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T14_20_35.417.247808576.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T14_59_55.456.250446256.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T15_59_55.503.180615472.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T16_00_05.503.218254360.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T16_00_55.504.232290704.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T16_01_55.504.250954264.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T17_02_55.558.219896112.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T17_03_45.557.231357144.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T17_04_35.558.238952584.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T17_05_35.559.241830352.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T17_06_25.559.257870816.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T18_07_25.604.216007824.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T18_08_15.605.229433816.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T18_09_15.605.245173112.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T18_10_05.606.247596832.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T18_10_55.606.252433256.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T19_10_55.655.203473104.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T19_11_05.655.239886624.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T19_11_55.655.246041304.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T19_12_55.656.259984120.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T20_14_05.704.221122728.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T20_14_55.704.224888480.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T20_15_45.705.225667856.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T20_16_35.706.236777120.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T20_17_25.706.246236320.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T21_17_25.755.234167096.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T21_19_15.757.242806728.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T21_20_05.757.247140736.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T21_24_15.760.250923008.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T22_27_35.814.236579320.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T22_28_35.815.243602648.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T22_34_55.820.244225072.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T22_35_45.820.247032536.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T22_36_35.820.250914320.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T23_36_35.870.241070728.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-17T23_37_25.870.258754200.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T00_37_35.919.222646192.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T00_38_35.920.242985392.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T01_10_15.946.247421200.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T01_35_15.971.247799608.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T01_37_45.971.250047688.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T02_37_46.034.218741608.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T02_38_46.035.235350080.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T02_39_26.036.242580624.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T02_40_16.037.244313496.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T02_41_16.038.258471632.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T03_43_56.123.234322624.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T03_44_46.124.237381376.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T03_45_36.125.247996344.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T04_45_36.184.248778568.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T04_46_26.185.251361472.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T05_46_26.240.181818280.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T05_46_36.240.221138664.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T05_47_26.241.231290368.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T05_49_56.243.237966424.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T05_50_46.243.255995864.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T06_50_46.305.238960112.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T06_51_36.306.250677456.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T07_52_36.370.219652952.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T07_53_26.371.237088008.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T07_54_06.372.241496056.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T07_54_46.373.247387432.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T07_55_36.375.251965784.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T08_55_36.442.216375352.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T08_56_26.442.221100784.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T08_57_16.444.223071296.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T08_58_06.446.238608360.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T08_58_56.446.246815776.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T09_59_06.516.236117192.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T09_59_56.516.243893240.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T10_14_46.532.244686952.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T10_38_46.559.245852808.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T10_39_36.560.252839424.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T11_39_46.615.228352232.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T11_40_26.616.234217432.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T11_41_26.617.239648416.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T11_42_16.617.247142840.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T11_43_06.618.254509080.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T12_44_56.675.216495912.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T12_45_46.676.229956608.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T12_46_36.676.238693816.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T12_47_36.678.243186408.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T12_48_26.678.257895848.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T13_50_06.737.218442248.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T13_50_56.736.232997888.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T13_51_46.737.237605504.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T13_53_36.738.242782688.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T13_54_26.739.250046144.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T14_54_26.789.191904376.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T14_54_36.791.230010472.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T14_55_26.795.237719728.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T15_00_26.799.245190904.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T15_01_16.800.250184016.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T16_01_16.853.215186736.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T16_02_16.854.237092472.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T16_03_06.854.252663744.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T17_03_06.911.233036656.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T17_03_56.911.245766888.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T17_04_46.911.246135456.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T17_05_36.912.247537336.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T18_05_36.961.154267488.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T18_05_46.961.191247112.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T18_05_56.962.229796016.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T18_07_46.964.239882032.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T18_08_36.964.248380160.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T19_09_27.011.222484760.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T19_10_17.012.227896216.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T19_11_07.013.239387848.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T19_12_57.018.243050536.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T19_13_47.016.248708496.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T20_13_47.071.166023024.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T20_13_57.071.205170920.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T20_14_07.071.242954296.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T20_14_57.072.260407824.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T21_16_57.123.259894072.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T22_15_37.170.276586552.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T22_15_47.171.316075208.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T22_25_07.181.325953216.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T22_25_27.183.345302632.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T23_27_27.269.269037992.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T23_34_37.279.269375512.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T23_42_27.289.270359920.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-18T23_47_37.298.270814544.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T00_18_37.344.272570024.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T01_18_37.441.199601904.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T01_18_57.441.222806536.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T01_19_17.442.247336000.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T01_19_37.442.259773856.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T01_19_57.443.270980640.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T02_20_57.542.270802168.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T02_25_27.550.271154016.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T02_28_27.554.289790176.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T03_06_17.606.289796720.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T03_18_17.624.319369056.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T04_20_07.705.265145392.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T05_15_07.793.266203608.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T05_15_27.795.267429888.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T05_45_07.839.272236000.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T05_59_57.862.277966080.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T06_59_57.972.187092872.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T07_00_27.972.261638272.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T07_02_57.975.262650176.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T07_03_17.977.281388576.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T08_06_58.064.264739208.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T08_13_38.073.266205984.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T08_14_28.074.270470760.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T08_23_08.086.272480240.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T08_32_28.099.316869608.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T10_24_28.261.275381328.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T10_38_38.281.344000456.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T10_38_58.283.358974776.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T10_39_08.282.369247736.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T10_39_58.284.380133168.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T11_42_48.381.271800880.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T12_00_58.445.272624032.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T12_02_08.447.274577368.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T12_40_48.500.296930744.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T12_40_58.501.345329344.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T13_42_48.592.257623336.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T13_43_58.592.261288824.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T13_44_38.593.270295544.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T13_45_48.595.273835768.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T13_47_38.598.274716608.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T14_47_58.686.247551512.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T14_48_28.687.266161656.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T14_50_58.691.269057848.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T14_53_18.695.270803704.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T14_56_58.699.271226392.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T15_58_18.765.250323968.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T15_58_48.766.250350416.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T15_59_28.766.255020160.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T15_59_58.766.264369248.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T16_26_08.790.280481024.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T17_26_08.846.147326824.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T17_26_18.846.196561000.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T17_26_28.846.245195024.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T17_28_28.848.249362088.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T17_29_08.849.272126112.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T18_29_18.898.245313888.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T18_29_48.898.250010168.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T18_38_48.905.254117608.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T18_43_18.910.257208704.pprof
writing: debug/nodes/1/heapprof/memprof.2020-11-19T18_55_58.921.257314064.pprof
requesting goroutine files for node 1... 0 found
requesting log files list... 29 found
requesting log file cockroach.nvmedb1.cockroachdb.2020-11-10T13_51_04Z.001220.log... writing: debug/nodes/1/logs/cockroach.nvmedb1.cockroachdb.2020-11-10T13_51_04Z.001220.log
requesting log file cockroach.nvmedb1.cockroachdb.2020-11-10T16_05_55Z.001220.log... writing: debug/nodes/1/logs/cockroach.nvmedb1.cockroachdb.2020-11-10T16_05_55Z.001220.log
requesting log file cockroach.nvmedb1.cockroachdb.2020-11-10T16_35_29Z.003256.log... writing: debug/nodes/1/logs/cockroach.nvmedb1.cockroachdb.2020-11-10T16_35_29Z.003256.log
requesting log file cockroach.nvmedb1.cockroachdb.2020-11-10T23_13_54Z.007017.log... writing: debug/nodes/1/logs/cockroach.nvmedb1.cockroachdb.2020-11-10T23_13_54Z.007017.log
requesting log file cockroach.nvmedb1.cockroachdb.2020-11-11T21_53_36Z.013468.log... writing: debug/nodes/1/logs/cockroach.nvmedb1.cockroachdb.2020-11-11T21_53_36Z.013468.log
requesting log file cockroach.nvmedb1.cockroachdb.2020-11-13T01_30_37Z.013468.log... writing: debug/nodes/1/logs/cockroach.nvmedb1.cockroachdb.2020-11-13T01_30_37Z.013468.log
requesting log file cockroach.nvmedb1.cockroachdb.2020-11-14T05_08_07Z.013468.log... writing: debug/nodes/1/logs/cockroach.nvmedb1.cockroachdb.2020-11-14T05_08_07Z.013468.log
requesting log file cockroach.nvmedb1.cockroachdb.2020-11-15T08_45_36Z.013468.log... writing: debug/nodes/1/logs/cockroach.nvmedb1.cockroachdb.2020-11-15T08_45_36Z.013468.log
requesting log file cockroach.nvmedb1.cockroachdb.2020-11-16T12_22_26Z.013468.log... writing: debug/nodes/1/logs/cockroach.nvmedb1.cockroachdb.2020-11-16T12_22_26Z.013468.log
requesting log file cockroach.nvmedb1.cockroachdb.2020-11-17T15_57_25Z.013468.log... writing: debug/nodes/1/logs/cockroach.nvmedb1.cockroachdb.2020-11-17T15_57_25Z.013468.log
requesting log file cockroach.nvmedb1.cockroachdb.2020-11-18T19_33_25Z.013468.log... writing: debug/nodes/1/logs/cockroach.nvmedb1.cockroachdb.2020-11-18T19_33_25Z.013468.log
requesting log file cockroach.nvmedb1.cockroachdb.2020-11-19T13_42_18Z.013468.log... writing: debug/nodes/1/logs/cockroach.nvmedb1.cockroachdb.2020-11-19T13_42_18Z.013468.log
requesting log file cockroach-stderr.nvmedb1.cockroachdb.2020-11-10T01_00_49Z.001220.log... writing: debug/nodes/1/logs/cockroach-stderr.nvmedb1.cockroachdb.2020-11-10T01_00_49Z.001220.log
requesting log file cockroach-stderr.nvmedb1.cockroachdb.2020-11-10T16_35_29Z.003256.log... writing: debug/nodes/1/logs/cockroach-stderr.nvmedb1.cockroachdb.2020-11-10T16_35_29Z.003256.log
requesting log file cockroach-stderr.nvmedb1.cockroachdb.2020-11-10T23_13_54Z.007017.log... writing: debug/nodes/1/logs/cockroach-stderr.nvmedb1.cockroachdb.2020-11-10T23_13_54Z.007017.log
requesting log file cockroach-stderr.nvmedb1.cockroachdb.2020-11-11T21_53_36Z.013468.log... writing: debug/nodes/1/logs/cockroach-stderr.nvmedb1.cockroachdb.2020-11-11T21_53_36Z.013468.log
requesting log file cockroach-pebble.nvmedb1.cockroachdb.2020-11-10T06_30_28Z.001220.log... writing: debug/nodes/1/logs/cockroach-pebble.nvmedb1.cockroachdb.2020-11-10T06_30_28Z.001220.log
requesting log file cockroach-pebble.nvmedb1.cockroachdb.2020-11-10T07_26_07Z.001220.log... writing: debug/nodes/1/logs/cockroach-pebble.nvmedb1.cockroachdb.2020-11-10T07_26_07Z.001220.log
requesting log file cockroach-pebble.nvmedb1.cockroachdb.2020-11-10T10_11_37Z.001220.log... writing: debug/nodes/1/logs/cockroach-pebble.nvmedb1.cockroachdb.2020-11-10T10_11_37Z.001220.log
requesting log file cockroach-pebble.nvmedb1.cockroachdb.2020-11-10T11_01_15Z.001220.log... writing: debug/nodes/1/logs/cockroach-pebble.nvmedb1.cockroachdb.2020-11-10T11_01_15Z.001220.log
requesting log file cockroach-pebble.nvmedb1.cockroachdb.2020-11-10T11_47_05Z.001220.log... writing: debug/nodes/1/logs/cockroach-pebble.nvmedb1.cockroachdb.2020-11-10T11_47_05Z.001220.log
requesting log file cockroach-pebble.nvmedb1.cockroachdb.2020-11-10T12_34_36Z.001220.log... writing: debug/nodes/1/logs/cockroach-pebble.nvmedb1.cockroachdb.2020-11-10T12_34_36Z.001220.log
requesting log file cockroach-pebble.nvmedb1.cockroachdb.2020-11-10T13_21_04Z.001220.log... writing: debug/nodes/1/logs/cockroach-pebble.nvmedb1.cockroachdb.2020-11-10T13_21_04Z.001220.log
requesting log file cockroach-pebble.nvmedb1.cockroachdb.2020-11-10T14_08_28Z.001220.log... writing: debug/nodes/1/logs/cockroach-pebble.nvmedb1.cockroachdb.2020-11-10T14_08_28Z.001220.log
requesting log file cockroach-pebble.nvmedb1.cockroachdb.2020-11-10T14_57_00Z.001220.log... writing: debug/nodes/1/logs/cockroach-pebble.nvmedb1.cockroachdb.2020-11-10T14_57_00Z.001220.log
requesting log file cockroach-pebble.nvmedb1.cockroachdb.2020-11-10T15_44_31Z.001220.log... writing: debug/nodes/1/logs/cockroach-pebble.nvmedb1.cockroachdb.2020-11-10T15_44_31Z.001220.log
requesting log file cockroach-pebble.nvmedb1.cockroachdb.2020-11-10T16_35_29Z.003256.log... writing: debug/nodes/1/logs/cockroach-pebble.nvmedb1.cockroachdb.2020-11-10T16_35_29Z.003256.log
requesting log file cockroach-pebble.nvmedb1.cockroachdb.2020-11-10T23_13_54Z.007017.log... writing: debug/nodes/1/logs/cockroach-pebble.nvmedb1.cockroachdb.2020-11-10T23_13_54Z.007017.log
requesting log file cockroach-pebble.nvmedb1.cockroachdb.2020-11-11T21_53_36Z.013468.log... writing: debug/nodes/1/logs/cockroach-pebble.nvmedb1.cockroachdb.2020-11-11T21_53_36Z.013468.log
requesting ranges... 299 found
writing: debug/nodes/1/ranges/1.json
writing: debug/nodes/1/ranges/2.json
writing: debug/nodes/1/ranges/3.json
writing: debug/nodes/1/ranges/4.json
writing: debug/nodes/1/ranges/5.json
writing: debug/nodes/1/ranges/6.json
writing: debug/nodes/1/ranges/7.json
writing: debug/nodes/1/ranges/8.json
writing: debug/nodes/1/ranges/9.json
writing: debug/nodes/1/ranges/10.json
writing: debug/nodes/1/ranges/11.json
writing: debug/nodes/1/ranges/12.json
writing: debug/nodes/1/ranges/13.json
writing: debug/nodes/1/ranges/14.json
writing: debug/nodes/1/ranges/15.json
writing: debug/nodes/1/ranges/16.json
writing: debug/nodes/1/ranges/17.json
writing: debug/nodes/1/ranges/18.json
writing: debug/nodes/1/ranges/19.json
writing: debug/nodes/1/ranges/20.json
writing: debug/nodes/1/ranges/21.json
writing: debug/nodes/1/ranges/22.json
writing: debug/nodes/1/ranges/23.json
writing: debug/nodes/1/ranges/24.json
writing: debug/nodes/1/ranges/25.json
writing: debug/nodes/1/ranges/26.json
writing: debug/nodes/1/ranges/27.json
writing: debug/nodes/1/ranges/28.json
writing: debug/nodes/1/ranges/29.json
writing: debug/nodes/1/ranges/30.json
writing: debug/nodes/1/ranges/31.json
writing: debug/nodes/1/ranges/32.json
writing: debug/nodes/1/ranges/33.json
writing: debug/nodes/1/ranges/34.json
writing: debug/nodes/1/ranges/35.json
writing: debug/nodes/1/ranges/36.json
writing: debug/nodes/1/ranges/37.json
writing: debug/nodes/1/ranges/38.json
writing: debug/nodes/1/ranges/39.json
writing: debug/nodes/1/ranges/40.json
writing: debug/nodes/1/ranges/41.json
writing: debug/nodes/1/ranges/42.json
writing: debug/nodes/1/ranges/43.json
writing: debug/nodes/1/ranges/44.json
writing: debug/nodes/1/ranges/45.json
writing: debug/nodes/1/ranges/46.json
writing: debug/nodes/1/ranges/47.json
writing: debug/nodes/1/ranges/48.json
writing: debug/nodes/1/ranges/49.json
writing: debug/nodes/1/ranges/50.json
writing: debug/nodes/1/ranges/51.json
writing: debug/nodes/1/ranges/52.json
writing: debug/nodes/1/ranges/53.json
writing: debug/nodes/1/ranges/54.json
writing: debug/nodes/1/ranges/55.json
writing: debug/nodes/1/ranges/56.json
writing: debug/nodes/1/ranges/57.json
writing: debug/nodes/1/ranges/58.json
writing: debug/nodes/1/ranges/60.json
writing: debug/nodes/1/ranges/61.json
writing: debug/nodes/1/ranges/62.json
writing: debug/nodes/1/ranges/63.json
writing: debug/nodes/1/ranges/64.json
writing: debug/nodes/1/ranges/65.json
writing: debug/nodes/1/ranges/66.json
writing: debug/nodes/1/ranges/67.json
writing: debug/nodes/1/ranges/68.json
writing: debug/nodes/1/ranges/69.json
writing: debug/nodes/1/ranges/70.json
writing: debug/nodes/1/ranges/71.json
writing: debug/nodes/1/ranges/72.json
writing: debug/nodes/1/ranges/73.json
writing: debug/nodes/1/ranges/74.json
writing: debug/nodes/1/ranges/75.json
writing: debug/nodes/1/ranges/76.json
writing: debug/nodes/1/ranges/77.json
writing: debug/nodes/1/ranges/78.json
writing: debug/nodes/1/ranges/79.json
writing: debug/nodes/1/ranges/80.json
writing: debug/nodes/1/ranges/81.json
writing: debug/nodes/1/ranges/82.json
writing: debug/nodes/1/ranges/83.json
writing: debug/nodes/1/ranges/84.json
writing: debug/nodes/1/ranges/85.json
writing: debug/nodes/1/ranges/86.json
writing: debug/nodes/1/ranges/87.json
writing: debug/nodes/1/ranges/88.json
writing: debug/nodes/1/ranges/89.json
writing: debug/nodes/1/ranges/90.json
writing: debug/nodes/1/ranges/92.json
writing: debug/nodes/1/ranges/94.json
writing: debug/nodes/1/ranges/95.json
writing: debug/nodes/1/ranges/96.json
writing: debug/nodes/1/ranges/97.json
writing: debug/nodes/1/ranges/98.json
writing: debug/nodes/1/ranges/99.json
writing: debug/nodes/1/ranges/100.json
writing: debug/nodes/1/ranges/101.json
writing: debug/nodes/1/ranges/102.json
writing: debug/nodes/1/ranges/103.json
writing: debug/nodes/1/ranges/104.json
writing: debug/nodes/1/ranges/105.json
writing: debug/nodes/1/ranges/106.json
writing: debug/nodes/1/ranges/107.json
writing: debug/nodes/1/ranges/108.json
writing: debug/nodes/1/ranges/109.json
writing: debug/nodes/1/ranges/110.json
writing: debug/nodes/1/ranges/111.json
writing: debug/nodes/1/ranges/112.json
writing: debug/nodes/1/ranges/113.json
writing: debug/nodes/1/ranges/114.json
writing: debug/nodes/1/ranges/115.json
writing: debug/nodes/1/ranges/116.json
writing: debug/nodes/1/ranges/117.json
writing: debug/nodes/1/ranges/118.json
writing: debug/nodes/1/ranges/119.json
writing: debug/nodes/1/ranges/120.json
writing: debug/nodes/1/ranges/121.json
writing: debug/nodes/1/ranges/122.json
writing: debug/nodes/1/ranges/123.json
writing: debug/nodes/1/ranges/124.json
writing: debug/nodes/1/ranges/125.json
writing: debug/nodes/1/ranges/126.json
writing: debug/nodes/1/ranges/127.json
writing: debug/nodes/1/ranges/128.json
writing: debug/nodes/1/ranges/129.json
writing: debug/nodes/1/ranges/130.json
writing: debug/nodes/1/ranges/131.json
writing: debug/nodes/1/ranges/132.json
writing: debug/nodes/1/ranges/133.json
writing: debug/nodes/1/ranges/134.json
writing: debug/nodes/1/ranges/135.json
writing: debug/nodes/1/ranges/136.json
writing: debug/nodes/1/ranges/137.json
writing: debug/nodes/1/ranges/138.json
writing: debug/nodes/1/ranges/139.json
writing: debug/nodes/1/ranges/140.json
writing: debug/nodes/1/ranges/141.json
writing: debug/nodes/1/ranges/142.json
writing: debug/nodes/1/ranges/143.json
writing: debug/nodes/1/ranges/144.json
writing: debug/nodes/1/ranges/145.json
writing: debug/nodes/1/ranges/146.json
writing: debug/nodes/1/ranges/147.json
writing: debug/nodes/1/ranges/148.json
writing: debug/nodes/1/ranges/149.json
writing: debug/nodes/1/ranges/150.json
writing: debug/nodes/1/ranges/151.json
writing: debug/nodes/1/ranges/153.json
writing: debug/nodes/1/ranges/154.json
writing: debug/nodes/1/ranges/155.json
writing: debug/nodes/1/ranges/156.json
writing: debug/nodes/1/ranges/157.json
writing: debug/nodes/1/ranges/158.json
writing: debug/nodes/1/ranges/159.json
writing: debug/nodes/1/ranges/160.json
writing: debug/nodes/1/ranges/161.json
writing: debug/nodes/1/ranges/162.json
writing: debug/nodes/1/ranges/163.json
writing: debug/nodes/1/ranges/164.json
writing: debug/nodes/1/ranges/165.json
writing: debug/nodes/1/ranges/166.json
writing: debug/nodes/1/ranges/167.json
writing: debug/nodes/1/ranges/168.json
writing: debug/nodes/1/ranges/169.json
writing: debug/nodes/1/ranges/170.json
writing: debug/nodes/1/ranges/171.json
writing: debug/nodes/1/ranges/172.json
writing: debug/nodes/1/ranges/173.json
writing: debug/nodes/1/ranges/174.json
writing: debug/nodes/1/ranges/175.json
writing: debug/nodes/1/ranges/176.json
writing: debug/nodes/1/ranges/177.json
writing: debug/nodes/1/ranges/178.json
writing: debug/nodes/1/ranges/179.json
writing: debug/nodes/1/ranges/180.json
writing: debug/nodes/1/ranges/181.json
writing: debug/nodes/1/ranges/182.json
writing: debug/nodes/1/ranges/183.json
writing: debug/nodes/1/ranges/184.json
writing: debug/nodes/1/ranges/185.json
writing: debug/nodes/1/ranges/186.json
writing: debug/nodes/1/ranges/187.json
writing: debug/nodes/1/ranges/188.json
writing: debug/nodes/1/ranges/189.json
writing: debug/nodes/1/ranges/190.json
writing: debug/nodes/1/ranges/191.json
writing: debug/nodes/1/ranges/192.json
writing: debug/nodes/1/ranges/193.json
writing: debug/nodes/1/ranges/194.json
writing: debug/nodes/1/ranges/195.json
writing: debug/nodes/1/ranges/196.json
writing: debug/nodes/1/ranges/197.json
writing: debug/nodes/1/ranges/198.json
writing: debug/nodes/1/ranges/199.json
writing: debug/nodes/1/ranges/200.json
writing: debug/nodes/1/ranges/201.json
writing: debug/nodes/1/ranges/202.json
writing: debug/nodes/1/ranges/203.json
writing: debug/nodes/1/ranges/204.json
writing: debug/nodes/1/ranges/205.json
writing: debug/nodes/1/ranges/206.json
writing: debug/nodes/1/ranges/207.json
writing: debug/nodes/1/ranges/208.json
writing: debug/nodes/1/ranges/209.json
writing: debug/nodes/1/ranges/210.json
writing: debug/nodes/1/ranges/211.json
writing: debug/nodes/1/ranges/212.json
writing: debug/nodes/1/ranges/213.json
writing: debug/nodes/1/ranges/214.json
writing: debug/nodes/1/ranges/215.json
writing: debug/nodes/1/ranges/216.json
writing: debug/nodes/1/ranges/217.json
writing: debug/nodes/1/ranges/218.json
writing: debug/nodes/1/ranges/219.json
writing: debug/nodes/1/ranges/220.json
writing: debug/nodes/1/ranges/221.json
writing: debug/nodes/1/ranges/222.json
writing: debug/nodes/1/ranges/223.json
writing: debug/nodes/1/ranges/224.json
writing: debug/nodes/1/ranges/225.json
writing: debug/nodes/1/ranges/226.json
writing: debug/nodes/1/ranges/227.json
writing: debug/nodes/1/ranges/228.json
writing: debug/nodes/1/ranges/229.json
writing: debug/nodes/1/ranges/230.json
writing: debug/nodes/1/ranges/231.json
writing: debug/nodes/1/ranges/232.json
writing: debug/nodes/1/ranges/233.json
writing: debug/nodes/1/ranges/234.json
writing: debug/nodes/1/ranges/235.json
writing: debug/nodes/1/ranges/236.json
writing: debug/nodes/1/ranges/237.json
writing: debug/nodes/1/ranges/238.json
writing: debug/nodes/1/ranges/239.json
writing: debug/nodes/1/ranges/240.json
writing: debug/nodes/1/ranges/241.json
writing: debug/nodes/1/ranges/242.json
writing: debug/nodes/1/ranges/243.json
writing: debug/nodes/1/ranges/244.json
writing: debug/nodes/1/ranges/245.json
writing: debug/nodes/1/ranges/246.json
writing: debug/nodes/1/ranges/247.json
writing: debug/nodes/1/ranges/248.json
writing: debug/nodes/1/ranges/249.json
writing: debug/nodes/1/ranges/250.json
writing: debug/nodes/1/ranges/251.json
writing: debug/nodes/1/ranges/252.json
writing: debug/nodes/1/ranges/253.json
writing: debug/nodes/1/ranges/254.json
writing: debug/nodes/1/ranges/255.json
writing: debug/nodes/1/ranges/256.json
writing: debug/nodes/1/ranges/257.json
writing: debug/nodes/1/ranges/258.json
writing: debug/nodes/1/ranges/259.json
writing: debug/nodes/1/ranges/260.json
writing: debug/nodes/1/ranges/261.json
writing: debug/nodes/1/ranges/262.json
writing: debug/nodes/1/ranges/263.json
writing: debug/nodes/1/ranges/264.json
writing: debug/nodes/1/ranges/265.json
writing: debug/nodes/1/ranges/266.json
writing: debug/nodes/1/ranges/267.json
writing: debug/nodes/1/ranges/268.json
writing: debug/nodes/1/ranges/269.json
writing: debug/nodes/1/ranges/270.json
writing: debug/nodes/1/ranges/271.json
writing: debug/nodes/1/ranges/272.json
writing: debug/nodes/1/ranges/276.json
writing: debug/nodes/1/ranges/277.json
writing: debug/nodes/1/ranges/278.json
writing: debug/nodes/1/ranges/279.json
writing: debug/nodes/1/ranges/280.json
writing: debug/nodes/1/ranges/281.json
writing: debug/nodes/1/ranges/282.json
writing: debug/nodes/1/ranges/286.json
writing: debug/nodes/1/ranges/287.json
writing: debug/nodes/1/ranges/288.json
writing: debug/nodes/1/ranges/289.json
writing: debug/nodes/1/ranges/290.json
writing: debug/nodes/1/ranges/291.json
writing: debug/nodes/1/ranges/292.json
writing: debug/nodes/1/ranges/293.json
writing: debug/nodes/1/ranges/294.json
writing: debug/nodes/1/ranges/295.json
writing: debug/nodes/1/ranges/296.json
writing: debug/nodes/1/ranges/297.json
writing: debug/nodes/1/ranges/298.json
writing: debug/nodes/1/ranges/299.json
writing: debug/nodes/1/ranges/320.json
writing: debug/nodes/1/ranges/321.json
writing: debug/nodes/1/ranges/324.json
writing: debug/nodes/1/ranges/325.json
writing: debug/nodes/1/ranges/356.json
writing: debug/nodes/1/ranges/358.json
writing: debug/nodes/1/ranges/361.json
writing: debug/nodes/1/ranges/362.json
writing: debug/nodes/1/ranges/363.json
writing: debug/nodes/1/ranges/376.json
writing: debug/nodes/2/status.json
using SQL connection URL for node 2: postgresql://root@192.168.1.43:26257/system?application_name=%24+cockroach+zip&connect_timeout=15&sslmode=disable
retrieving SQL data for crdb_internal.feature_usage... writing: debug/nodes/2/crdb_internal.feature_usage.txt
retrieving SQL data for crdb_internal.gossip_alerts... writing: debug/nodes/2/crdb_internal.gossip_alerts.txt
retrieving SQL data for crdb_internal.gossip_liveness... writing: debug/nodes/2/crdb_internal.gossip_liveness.txt
retrieving SQL data for crdb_internal.gossip_network... writing: debug/nodes/2/crdb_internal.gossip_network.txt
retrieving SQL data for crdb_internal.gossip_nodes... writing: debug/nodes/2/crdb_internal.gossip_nodes.txt
retrieving SQL data for crdb_internal.leases... writing: debug/nodes/2/crdb_internal.leases.txt
retrieving SQL data for crdb_internal.node_build_info... writing: debug/nodes/2/crdb_internal.node_build_info.txt
retrieving SQL data for crdb_internal.node_metrics... writing: debug/nodes/2/crdb_internal.node_metrics.txt
retrieving SQL data for crdb_internal.node_queries... writing: debug/nodes/2/crdb_internal.node_queries.txt
retrieving SQL data for crdb_internal.node_runtime_info... writing: debug/nodes/2/crdb_internal.node_runtime_info.txt
retrieving SQL data for crdb_internal.node_sessions... writing: debug/nodes/2/crdb_internal.node_sessions.txt
retrieving SQL data for crdb_internal.node_statement_statistics... writing: debug/nodes/2/crdb_internal.node_statement_statistics.txt
retrieving SQL data for crdb_internal.node_transaction_statistics... writing: debug/nodes/2/crdb_internal.node_transaction_statistics.txt
retrieving SQL data for crdb_internal.node_transactions... writing: debug/nodes/2/crdb_internal.node_transactions.txt
retrieving SQL data for crdb_internal.node_txn_stats... writing: debug/nodes/2/crdb_internal.node_txn_stats.txt
requesting data for debug/nodes/2/details... writing: debug/nodes/2/details.json
requesting data for debug/nodes/2/gossip... writing: debug/nodes/2/gossip.json
requesting data for debug/nodes/2/enginestats... writing: debug/nodes/2/enginestats.json
requesting stacks for node 2... writing: debug/nodes/2/stacks.txt
requesting threads for node 2... writing: debug/nodes/2/threads.txt
requesting heap profile for node 2... writing: debug/nodes/2/heap.pprof
requesting heap files for node 2... 250 found
writing: debug/nodes/2/heapprof/memprof.2020-11-16T14_13_19.251.246436096.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-16T15_15_49.150.229827456.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-16T15_16_49.148.231314256.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-16T15_17_49.146.235568464.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-16T15_18_59.144.244826808.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-16T15_29_49.126.247329664.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-16T16_33_19.027.233505960.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-16T16_36_39.022.234201112.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-16T16_41_09.015.237928680.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-16T16_42_09.014.245857184.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-16T17_39_08.920.246345968.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-16T18_41_28.868.241706736.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-16T18_45_58.861.242256200.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-16T18_46_58.859.245015712.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-16T18_47_58.857.245706376.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-16T19_10_08.821.246026376.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-16T20_10_18.720.159254712.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-16T20_10_28.720.192618136.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-16T20_10_38.719.224461360.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-16T20_11_48.717.238745816.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-16T20_12_48.715.245446688.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-16T21_12_58.614.237860744.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-16T21_13_58.612.242784864.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-16T21_24_08.595.242920472.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-16T21_25_08.593.246538256.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-16T22_27_28.486.232925184.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-16T22_30_48.481.233249792.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-16T22_31_48.478.233561856.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-16T22_32_58.476.241577424.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-16T22_33_58.476.245923984.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-16T23_34_08.375.236739864.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-16T23_35_08.374.241212288.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-16T23_36_18.373.244762448.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-16T23_37_18.371.247261792.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T00_37_38.271.204973480.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T00_37_48.270.237534288.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T00_38_48.269.244158696.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T00_55_08.241.244243536.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T00_56_08.239.248520376.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T01_56_38.143.211491312.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T01_57_38.142.219196320.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T01_58_38.140.221902024.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T01_59_48.139.236888552.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T02_00_48.137.250775960.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T03_01_28.039.236173256.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T03_02_28.038.242328512.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T03_07_58.030.244564456.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T03_08_58.028.245987184.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T03_10_08.026.250084328.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T04_13_47.933.239484512.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T04_13_57.930.242555072.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T04_23_57.914.246086768.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T05_18_47.833.246087848.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T05_19_47.831.247664000.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T06_19_57.739.233041392.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T06_22_37.735.239836912.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T06_23_37.733.246525352.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T07_23_47.636.204308192.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T07_23_57.636.236970592.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T07_24_57.634.245347048.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T08_20_27.558.245795840.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T08_21_27.557.251956112.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T09_22_57.500.221520040.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T09_23_57.497.222313480.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T09_25_07.495.239163552.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T09_26_07.493.247164616.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T09_27_17.491.257809704.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T10_27_27.407.245465896.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T10_58_07.358.245478464.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T11_07_07.347.245872752.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T11_08_07.345.246870448.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T12_16_37.236.241745200.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T12_17_37.234.244974392.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T12_22_07.227.245285704.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T12_23_07.225.248162416.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T12_23_37.228.259421496.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T13_23_47.130.241720344.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T13_24_47.129.247326928.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T14_25_07.056.225413376.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T14_26_07.055.226042016.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T14_26_57.054.240552224.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T14_27_57.052.244624752.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T15_11_27.013.246234560.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T16_21_36.900.237463600.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T16_22_36.898.241400216.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T16_24_46.894.244755832.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T16_25_46.893.246747656.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T17_04_06.829.247104272.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T18_04_26.728.237869184.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T18_05_26.726.243361928.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T18_11_06.717.244126040.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T18_21_26.700.244622848.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T18_22_26.698.247865888.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T19_24_56.598.222164704.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T19_26_06.596.235905424.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T19_27_06.595.242975824.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T19_51_46.557.243549176.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T19_52_46.555.247919984.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T20_53_06.459.165132200.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T20_53_16.459.196186248.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T20_53_26.459.228973336.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T20_54_26.457.235321912.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T20_55_26.455.245628616.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T21_55_56.356.215428808.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T21_56_26.356.227268296.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T21_57_16.354.228917424.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T21_58_26.353.238163296.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T21_59_26.352.245498840.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T22_59_36.254.233762352.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T23_00_36.253.238944920.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T23_03_56.248.239673160.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T23_04_56.247.247140456.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-17T23_40_16.191.247576176.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T00_40_36.112.233030120.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T00_41_36.113.233800536.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T00_43_46.108.235613336.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T00_46_26.104.238654752.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T00_47_26.102.245574632.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T01_48_36.073.232537624.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T01_49_26.073.237847824.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T01_50_26.071.243033992.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T02_33_06.013.243280416.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T02_34_06.013.251216984.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T03_37_25.914.231540672.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T03_38_25.912.232528016.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T03_39_25.911.236585192.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T03_40_25.910.238937712.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T03_41_25.908.250753856.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T04_41_45.818.225117920.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T04_42_45.816.228090696.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T04_43_35.815.230823232.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T04_44_35.813.245196408.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T04_45_35.812.253257544.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T05_50_45.721.227145112.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T05_51_55.719.242995448.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T06_01_25.706.243470784.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T06_04_35.701.244494096.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T06_05_35.700.248139560.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T07_05_45.607.195495240.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T07_05_55.607.229207600.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T07_06_55.606.243393000.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T07_30_15.576.247763992.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T07_31_15.574.252549176.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T08_32_55.494.242309976.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T08_47_05.476.244524416.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T08_54_35.465.245129896.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T09_05_25.449.246095768.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T09_06_25.448.249916408.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T10_08_45.357.238191592.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T10_09_45.355.244235032.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T10_48_35.303.245118968.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T10_52_25.298.246909640.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T10_53_25.297.254486160.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T11_54_35.204.215728672.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T11_55_35.202.221674392.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T11_56_35.200.224160728.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T11_57_45.199.241523976.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T11_58_45.197.247331688.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T12_58_55.102.225808536.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T13_00_05.100.242584144.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T13_01_05.099.248222096.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T14_01_25.003.211532848.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T14_01_35.001.242697624.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T14_02_35.000.243954160.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T14_10_14.987.245784584.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T14_11_14.986.249076376.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T15_11_34.888.245287552.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T15_12_34.886.246876712.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T15_24_54.868.247905624.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T15_25_54.865.248222920.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T15_27_04.863.257672496.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T16_28_14.772.226749152.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T16_30_24.768.234961880.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T16_31_24.767.241020216.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T16_32_34.765.244431568.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T16_33_34.763.247056488.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T17_35_04.669.230231648.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T17_37_54.667.232049272.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T17_38_54.666.237523576.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T17_42_14.664.240427288.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T17_43_14.664.245889864.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T18_45_54.621.238081120.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T18_48_54.620.238528840.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T18_49_54.619.245480544.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T18_54_24.616.246018216.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T18_55_24.615.253171104.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T21_26_04.368.247517008.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T21_40_04.343.248529072.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T22_25_04.261.251682120.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T22_25_14.262.307898032.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T22_25_34.261.325624840.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T23_25_44.203.225944048.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T23_26_24.203.263929472.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T23_27_14.203.264201200.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T23_27_54.202.264350576.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-18T23_43_24.183.283640176.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T00_43_34.127.258796560.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T00_47_24.124.266248688.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T00_49_34.123.266274456.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T00_53_24.120.266651048.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T02_21_24.028.270747672.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T03_04_03.981.273180360.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T03_04_13.983.277828872.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T03_05_23.981.278852984.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T03_23_53.959.289899904.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T04_24_03.896.255306512.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T04_33_33.883.258944808.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T04_45_13.870.268214792.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T05_03_43.852.287020120.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T06_11_03.777.257672224.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T06_14_23.774.260580504.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T06_30_03.753.261181664.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T06_34_03.748.262694000.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T06_44_23.735.270653088.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T07_44_33.666.156404368.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T07_44_53.666.262349984.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T08_13_43.632.266050040.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T08_54_43.590.281458376.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T09_28_13.545.287309984.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T10_28_33.475.261173504.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T10_29_23.473.263759328.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T10_35_13.468.265247856.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T10_38_13.466.338061440.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T10_38_33.467.352333392.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T11_42_13.394.262355592.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T11_43_03.392.264428784.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T11_54_13.448.266399496.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T12_41_03.384.341002352.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T12_41_13.383.347486488.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T13_42_53.322.252391336.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T13_43_43.321.260340016.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T13_49_33.314.264581648.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T13_53_43.307.266944944.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T14_18_13.279.277623184.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T15_18_23.214.261867216.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T15_18_53.213.264239616.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T16_19_13.117.239896152.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T16_19_43.117.250194128.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T16_24_53.108.250325280.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T16_35_53.090.250778488.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T16_36_23.089.254543864.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T17_36_32.981.142574616.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T17_36_42.980.195744656.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T17_36_52.980.248340288.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T17_37_22.979.262279528.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T18_37_32.886.198183136.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T18_38_12.885.235988896.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T18_38_42.884.244940456.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T18_40_22.882.246133320.pprof
writing: debug/nodes/2/heapprof/memprof.2020-11-19T18_40_52.881.250477472.pprof
requesting goroutine files for node 2... 1 found
writing: debug/nodes/2/goroutines/goroutine_dump.2020-11-10T01_21_36.702.double_since_last_dump.000001506.txt.gz
requesting log files list... 31 found
requesting log file cockroach.nvmedb3.cockroachdb.2020-11-10T01_01_07Z.001214.log... writing: debug/nodes/2/logs/cockroach.nvmedb3.cockroachdb.2020-11-10T01_01_07Z.001214.log
requesting log file cockroach.nvmedb3.cockroachdb.2020-11-10T16_35_25Z.009471.log... writing: debug/nodes/2/logs/cockroach.nvmedb3.cockroachdb.2020-11-10T16_35_25Z.009471.log
requesting log file cockroach.nvmedb3.cockroachdb.2020-11-10T23_21_08Z.016275.log... writing: debug/nodes/2/logs/cockroach.nvmedb3.cockroachdb.2020-11-10T23_21_08Z.016275.log
requesting log file cockroach.nvmedb3.cockroachdb.2020-11-10T23_21_38Z.016315.log... writing: debug/nodes/2/logs/cockroach.nvmedb3.cockroachdb.2020-11-10T23_21_38Z.016315.log
requesting log file cockroach.nvmedb3.cockroachdb.2020-11-11T21_53_47Z.004971.log... writing: debug/nodes/2/logs/cockroach.nvmedb3.cockroachdb.2020-11-11T21_53_47Z.004971.log
requesting log file cockroach.nvmedb3.cockroachdb.2020-11-13T01_47_46Z.004971.log... writing: debug/nodes/2/logs/cockroach.nvmedb3.cockroachdb.2020-11-13T01_47_46Z.004971.log
requesting log file cockroach.nvmedb3.cockroachdb.2020-11-14T05_39_24Z.004971.log... writing: debug/nodes/2/logs/cockroach.nvmedb3.cockroachdb.2020-11-14T05_39_24Z.004971.log
requesting log file cockroach.nvmedb3.cockroachdb.2020-11-15T09_29_51Z.004971.log... writing: debug/nodes/2/logs/cockroach.nvmedb3.cockroachdb.2020-11-15T09_29_51Z.004971.log
requesting log file cockroach.nvmedb3.cockroachdb.2020-11-16T13_19_39Z.004971.log... writing: debug/nodes/2/logs/cockroach.nvmedb3.cockroachdb.2020-11-16T13_19_39Z.004971.log
requesting log file cockroach.nvmedb3.cockroachdb.2020-11-17T17_09_16Z.004971.log... writing: debug/nodes/2/logs/cockroach.nvmedb3.cockroachdb.2020-11-17T17_09_16Z.004971.log
requesting log file cockroach.nvmedb3.cockroachdb.2020-11-18T20_58_54Z.004971.log... writing: debug/nodes/2/logs/cockroach.nvmedb3.cockroachdb.2020-11-18T20_58_54Z.004971.log
requesting log file cockroach.nvmedb3.cockroachdb.2020-11-19T16_33_03Z.004971.log... writing: debug/nodes/2/logs/cockroach.nvmedb3.cockroachdb.2020-11-19T16_33_03Z.004971.log
requesting log file cockroach-stderr.nvmedb3.cockroachdb.2020-11-10T01_01_07Z.001214.log... writing: debug/nodes/2/logs/cockroach-stderr.nvmedb3.cockroachdb.2020-11-10T01_01_07Z.001214.log
requesting log file cockroach-stderr.nvmedb3.cockroachdb.2020-11-10T16_35_25Z.009471.log... writing: debug/nodes/2/logs/cockroach-stderr.nvmedb3.cockroachdb.2020-11-10T16_35_25Z.009471.log
requesting log file cockroach-stderr.nvmedb3.cockroachdb.2020-11-10T23_21_08Z.016275.log... writing: debug/nodes/2/logs/cockroach-stderr.nvmedb3.cockroachdb.2020-11-10T23_21_08Z.016275.log
requesting log file cockroach-stderr.nvmedb3.cockroachdb.2020-11-10T23_21_38Z.016315.log... writing: debug/nodes/2/logs/cockroach-stderr.nvmedb3.cockroachdb.2020-11-10T23_21_38Z.016315.log
requesting log file cockroach-stderr.nvmedb3.cockroachdb.2020-11-11T21_53_47Z.004971.log... writing: debug/nodes/2/logs/cockroach-stderr.nvmedb3.cockroachdb.2020-11-11T21_53_47Z.004971.log
requesting log file cockroach-pebble.nvmedb3.cockroachdb.2020-11-10T07_30_48Z.001214.log... writing: debug/nodes/2/logs/cockroach-pebble.nvmedb3.cockroachdb.2020-11-10T07_30_48Z.001214.log
requesting log file cockroach-pebble.nvmedb3.cockroachdb.2020-11-10T10_11_59Z.001214.log... writing: debug/nodes/2/logs/cockroach-pebble.nvmedb3.cockroachdb.2020-11-10T10_11_59Z.001214.log
requesting log file cockroach-pebble.nvmedb3.cockroachdb.2020-11-10T10_59_13Z.001214.log... writing: debug/nodes/2/logs/cockroach-pebble.nvmedb3.cockroachdb.2020-11-10T10_59_13Z.001214.log
requesting log file cockroach-pebble.nvmedb3.cockroachdb.2020-11-10T11_43_14Z.001214.log... writing: debug/nodes/2/logs/cockroach-pebble.nvmedb3.cockroachdb.2020-11-10T11_43_14Z.001214.log
requesting log file cockroach-pebble.nvmedb3.cockroachdb.2020-11-10T12_28_51Z.001214.log... writing: debug/nodes/2/logs/cockroach-pebble.nvmedb3.cockroachdb.2020-11-10T12_28_51Z.001214.log
requesting log file cockroach-pebble.nvmedb3.cockroachdb.2020-11-10T13_13_57Z.001214.log... writing: debug/nodes/2/logs/cockroach-pebble.nvmedb3.cockroachdb.2020-11-10T13_13_57Z.001214.log
requesting log file cockroach-pebble.nvmedb3.cockroachdb.2020-11-10T14_00_06Z.001214.log... writing: debug/nodes/2/logs/cockroach-pebble.nvmedb3.cockroachdb.2020-11-10T14_00_06Z.001214.log
requesting log file cockroach-pebble.nvmedb3.cockroachdb.2020-11-10T14_47_57Z.001214.log... writing: debug/nodes/2/logs/cockroach-pebble.nvmedb3.cockroachdb.2020-11-10T14_47_57Z.001214.log
requesting log file cockroach-pebble.nvmedb3.cockroachdb.2020-11-10T15_35_10Z.001214.log... writing: debug/nodes/2/logs/cockroach-pebble.nvmedb3.cockroachdb.2020-11-10T15_35_10Z.001214.log
requesting log file cockroach-pebble.nvmedb3.cockroachdb.2020-11-10T16_26_31Z.001214.log... writing: debug/nodes/2/logs/cockroach-pebble.nvmedb3.cockroachdb.2020-11-10T16_26_31Z.001214.log
requesting log file cockroach-pebble.nvmedb3.cockroachdb.2020-11-10T16_35_25Z.009471.log... writing: debug/nodes/2/logs/cockroach-pebble.nvmedb3.cockroachdb.2020-11-10T16_35_25Z.009471.log
requesting log file cockroach-pebble.nvmedb3.cockroachdb.2020-11-10T23_21_08Z.016275.log... writing: debug/nodes/2/logs/cockroach-pebble.nvmedb3.cockroachdb.2020-11-10T23_21_08Z.016275.log
requesting log file cockroach-pebble.nvmedb3.cockroachdb.2020-11-10T23_21_38Z.016315.log... writing: debug/nodes/2/logs/cockroach-pebble.nvmedb3.cockroachdb.2020-11-10T23_21_38Z.016315.log
requesting log file cockroach-pebble.nvmedb3.cockroachdb.2020-11-11T21_53_47Z.004971.log... writing: debug/nodes/2/logs/cockroach-pebble.nvmedb3.cockroachdb.2020-11-11T21_53_47Z.004971.log
requesting ranges... 299 found
writing: debug/nodes/2/ranges/1.json
writing: debug/nodes/2/ranges/2.json
writing: debug/nodes/2/ranges/3.json
writing: debug/nodes/2/ranges/4.json
writing: debug/nodes/2/ranges/5.json
writing: debug/nodes/2/ranges/6.json
writing: debug/nodes/2/ranges/7.json
writing: debug/nodes/2/ranges/8.json
writing: debug/nodes/2/ranges/9.json
writing: debug/nodes/2/ranges/10.json
writing: debug/nodes/2/ranges/11.json
writing: debug/nodes/2/ranges/12.json
writing: debug/nodes/2/ranges/13.json
writing: debug/nodes/2/ranges/14.json
writing: debug/nodes/2/ranges/15.json
writing: debug/nodes/2/ranges/16.json
writing: debug/nodes/2/ranges/17.json
writing: debug/nodes/2/ranges/18.json
writing: debug/nodes/2/ranges/19.json
writing: debug/nodes/2/ranges/20.json
writing: debug/nodes/2/ranges/21.json
writing: debug/nodes/2/ranges/22.json
writing: debug/nodes/2/ranges/23.json
writing: debug/nodes/2/ranges/24.json
writing: debug/nodes/2/ranges/25.json
writing: debug/nodes/2/ranges/26.json
writing: debug/nodes/2/ranges/27.json
writing: debug/nodes/2/ranges/28.json
writing: debug/nodes/2/ranges/29.json
writing: debug/nodes/2/ranges/30.json
writing: debug/nodes/2/ranges/31.json
writing: debug/nodes/2/ranges/32.json
writing: debug/nodes/2/ranges/33.json
writing: debug/nodes/2/ranges/34.json
writing: debug/nodes/2/ranges/35.json
writing: debug/nodes/2/ranges/36.json
writing: debug/nodes/2/ranges/37.json
writing: debug/nodes/2/ranges/38.json
writing: debug/nodes/2/ranges/39.json
writing: debug/nodes/2/ranges/40.json
writing: debug/nodes/2/ranges/41.json
writing: debug/nodes/2/ranges/42.json
writing: debug/nodes/2/ranges/43.json
writing: debug/nodes/2/ranges/44.json
writing: debug/nodes/2/ranges/45.json
writing: debug/nodes/2/ranges/46.json
writing: debug/nodes/2/ranges/47.json
writing: debug/nodes/2/ranges/48.json
writing: debug/nodes/2/ranges/49.json
writing: debug/nodes/2/ranges/50.json
writing: debug/nodes/2/ranges/51.json
writing: debug/nodes/2/ranges/52.json
writing: debug/nodes/2/ranges/53.json
writing: debug/nodes/2/ranges/54.json
writing: debug/nodes/2/ranges/55.json
writing: debug/nodes/2/ranges/56.json
writing: debug/nodes/2/ranges/57.json
writing: debug/nodes/2/ranges/58.json
writing: debug/nodes/2/ranges/60.json
writing: debug/nodes/2/ranges/61.json
writing: debug/nodes/2/ranges/62.json
writing: debug/nodes/2/ranges/63.json
writing: debug/nodes/2/ranges/64.json
writing: debug/nodes/2/ranges/65.json
writing: debug/nodes/2/ranges/66.json
writing: debug/nodes/2/ranges/67.json
writing: debug/nodes/2/ranges/68.json
writing: debug/nodes/2/ranges/69.json
writing: debug/nodes/2/ranges/70.json
writing: debug/nodes/2/ranges/71.json
writing: debug/nodes/2/ranges/72.json
writing: debug/nodes/2/ranges/73.json
writing: debug/nodes/2/ranges/74.json
writing: debug/nodes/2/ranges/75.json
writing: debug/nodes/2/ranges/76.json
writing: debug/nodes/2/ranges/77.json
writing: debug/nodes/2/ranges/78.json
writing: debug/nodes/2/ranges/79.json
writing: debug/nodes/2/ranges/80.json
writing: debug/nodes/2/ranges/81.json
writing: debug/nodes/2/ranges/82.json
writing: debug/nodes/2/ranges/83.json
writing: debug/nodes/2/ranges/84.json
writing: debug/nodes/2/ranges/85.json
writing: debug/nodes/2/ranges/86.json
writing: debug/nodes/2/ranges/87.json
writing: debug/nodes/2/ranges/88.json
writing: debug/nodes/2/ranges/89.json
writing: debug/nodes/2/ranges/90.json
writing: debug/nodes/2/ranges/92.json
writing: debug/nodes/2/ranges/94.json
writing: debug/nodes/2/ranges/95.json
writing: debug/nodes/2/ranges/96.json
writing: debug/nodes/2/ranges/97.json
writing: debug/nodes/2/ranges/98.json
writing: debug/nodes/2/ranges/99.json
writing: debug/nodes/2/ranges/100.json
writing: debug/nodes/2/ranges/101.json
writing: debug/nodes/2/ranges/102.json
writing: debug/nodes/2/ranges/103.json
writing: debug/nodes/2/ranges/104.json
writing: debug/nodes/2/ranges/105.json
writing: debug/nodes/2/ranges/106.json
writing: debug/nodes/2/ranges/107.json
writing: debug/nodes/2/ranges/108.json
writing: debug/nodes/2/ranges/109.json
writing: debug/nodes/2/ranges/110.json
writing: debug/nodes/2/ranges/111.json
writing: debug/nodes/2/ranges/112.json
writing: debug/nodes/2/ranges/113.json
writing: debug/nodes/2/ranges/114.json
writing: debug/nodes/2/ranges/115.json
writing: debug/nodes/2/ranges/116.json
writing: debug/nodes/2/ranges/117.json
writing: debug/nodes/2/ranges/118.json
writing: debug/nodes/2/ranges/119.json
writing: debug/nodes/2/ranges/120.json
writing: debug/nodes/2/ranges/121.json
writing: debug/nodes/2/ranges/122.json
writing: debug/nodes/2/ranges/123.json
writing: debug/nodes/2/ranges/124.json
writing: debug/nodes/2/ranges/125.json
writing: debug/nodes/2/ranges/126.json
writing: debug/nodes/2/ranges/127.json
writing: debug/nodes/2/ranges/128.json
writing: debug/nodes/2/ranges/129.json
writing: debug/nodes/2/ranges/130.json
writing: debug/nodes/2/ranges/131.json
writing: debug/nodes/2/ranges/132.json
writing: debug/nodes/2/ranges/133.json
writing: debug/nodes/2/ranges/134.json
writing: debug/nodes/2/ranges/135.json
writing: debug/nodes/2/ranges/136.json
writing: debug/nodes/2/ranges/137.json
writing: debug/nodes/2/ranges/138.json
writing: debug/nodes/2/ranges/139.json
writing: debug/nodes/2/ranges/140.json
writing: debug/nodes/2/ranges/141.json
writing: debug/nodes/2/ranges/142.json
writing: debug/nodes/2/ranges/143.json
writing: debug/nodes/2/ranges/144.json
writing: debug/nodes/2/ranges/145.json
writing: debug/nodes/2/ranges/146.json
writing: debug/nodes/2/ranges/147.json
writing: debug/nodes/2/ranges/148.json
writing: debug/nodes/2/ranges/149.json
writing: debug/nodes/2/ranges/150.json
writing: debug/nodes/2/ranges/151.json
writing: debug/nodes/2/ranges/153.json
writing: debug/nodes/2/ranges/154.json
writing: debug/nodes/2/ranges/155.json
writing: debug/nodes/2/ranges/156.json
writing: debug/nodes/2/ranges/157.json
writing: debug/nodes/2/ranges/158.json
writing: debug/nodes/2/ranges/159.json
writing: debug/nodes/2/ranges/160.json
writing: debug/nodes/2/ranges/161.json
writing: debug/nodes/2/ranges/162.json
writing: debug/nodes/2/ranges/163.json
writing: debug/nodes/2/ranges/164.json
writing: debug/nodes/2/ranges/165.json
writing: debug/nodes/2/ranges/166.json
writing: debug/nodes/2/ranges/167.json
writing: debug/nodes/2/ranges/168.json
writing: debug/nodes/2/ranges/169.json
writing: debug/nodes/2/ranges/170.json
writing: debug/nodes/2/ranges/171.json
writing: debug/nodes/2/ranges/172.json
writing: debug/nodes/2/ranges/173.json
writing: debug/nodes/2/ranges/174.json
writing: debug/nodes/2/ranges/175.json
writing: debug/nodes/2/ranges/176.json
writing: debug/nodes/2/ranges/177.json
writing: debug/nodes/2/ranges/178.json
writing: debug/nodes/2/ranges/179.json
writing: debug/nodes/2/ranges/180.json
writing: debug/nodes/2/ranges/181.json
writing: debug/nodes/2/ranges/182.json
writing: debug/nodes/2/ranges/183.json
writing: debug/nodes/2/ranges/184.json
writing: debug/nodes/2/ranges/185.json
writing: debug/nodes/2/ranges/186.json
writing: debug/nodes/2/ranges/187.json
writing: debug/nodes/2/ranges/188.json
writing: debug/nodes/2/ranges/189.json
writing: debug/nodes/2/ranges/190.json
writing: debug/nodes/2/ranges/191.json
writing: debug/nodes/2/ranges/192.json
writing: debug/nodes/2/ranges/193.json
writing: debug/nodes/2/ranges/194.json
writing: debug/nodes/2/ranges/195.json
writing: debug/nodes/2/ranges/196.json
writing: debug/nodes/2/ranges/197.json
writing: debug/nodes/2/ranges/198.json
writing: debug/nodes/2/ranges/199.json
writing: debug/nodes/2/ranges/200.json
writing: debug/nodes/2/ranges/201.json
writing: debug/nodes/2/ranges/202.json
writing: debug/nodes/2/ranges/203.json
writing: debug/nodes/2/ranges/204.json
writing: debug/nodes/2/ranges/205.json
writing: debug/nodes/2/ranges/206.json
writing: debug/nodes/2/ranges/207.json
writing: debug/nodes/2/ranges/208.json
writing: debug/nodes/2/ranges/209.json
writing: debug/nodes/2/ranges/210.json
writing: debug/nodes/2/ranges/211.json
writing: debug/nodes/2/ranges/212.json
writing: debug/nodes/2/ranges/213.json
writing: debug/nodes/2/ranges/214.json
writing: debug/nodes/2/ranges/215.json
writing: debug/nodes/2/ranges/216.json
writing: debug/nodes/2/ranges/217.json
writing: debug/nodes/2/ranges/218.json
writing: debug/nodes/2/ranges/219.json
writing: debug/nodes/2/ranges/220.json
writing: debug/nodes/2/ranges/221.json
writing: debug/nodes/2/ranges/222.json
writing: debug/nodes/2/ranges/223.json
writing: debug/nodes/2/ranges/224.json
writing: debug/nodes/2/ranges/225.json
writing: debug/nodes/2/ranges/226.json
writing: debug/nodes/2/ranges/227.json
writing: debug/nodes/2/ranges/228.json
writing: debug/nodes/2/ranges/229.json
writing: debug/nodes/2/ranges/230.json
writing: debug/nodes/2/ranges/231.json
writing: debug/nodes/2/ranges/232.json
writing: debug/nodes/2/ranges/233.json
writing: debug/nodes/2/ranges/234.json
writing: debug/nodes/2/ranges/235.json
writing: debug/nodes/2/ranges/236.json
writing: debug/nodes/2/ranges/237.json
writing: debug/nodes/2/ranges/238.json
writing: debug/nodes/2/ranges/239.json
writing: debug/nodes/2/ranges/240.json
writing: debug/nodes/2/ranges/241.json
writing: debug/nodes/2/ranges/242.json
writing: debug/nodes/2/ranges/243.json
writing: debug/nodes/2/ranges/244.json
writing: debug/nodes/2/ranges/245.json
writing: debug/nodes/2/ranges/246.json
writing: debug/nodes/2/ranges/247.json
writing: debug/nodes/2/ranges/248.json
writing: debug/nodes/2/ranges/249.json
writing: debug/nodes/2/ranges/250.json
writing: debug/nodes/2/ranges/251.json
writing: debug/nodes/2/ranges/252.json
writing: debug/nodes/2/ranges/253.json
writing: debug/nodes/2/ranges/254.json
writing: debug/nodes/2/ranges/255.json
writing: debug/nodes/2/ranges/256.json
writing: debug/nodes/2/ranges/257.json
writing: debug/nodes/2/ranges/258.json
writing: debug/nodes/2/ranges/259.json
writing: debug/nodes/2/ranges/260.json
writing: debug/nodes/2/ranges/261.json
writing: debug/nodes/2/ranges/262.json
writing: debug/nodes/2/ranges/263.json
writing: debug/nodes/2/ranges/264.json
writing: debug/nodes/2/ranges/265.json
writing: debug/nodes/2/ranges/266.json
writing: debug/nodes/2/ranges/267.json
writing: debug/nodes/2/ranges/268.json
writing: debug/nodes/2/ranges/269.json
writing: debug/nodes/2/ranges/270.json
writing: debug/nodes/2/ranges/271.json
writing: debug/nodes/2/ranges/272.json
writing: debug/nodes/2/ranges/276.json
writing: debug/nodes/2/ranges/277.json
writing: debug/nodes/2/ranges/278.json
writing: debug/nodes/2/ranges/279.json
writing: debug/nodes/2/ranges/280.json
writing: debug/nodes/2/ranges/281.json
writing: debug/nodes/2/ranges/282.json
writing: debug/nodes/2/ranges/286.json
writing: debug/nodes/2/ranges/287.json
writing: debug/nodes/2/ranges/288.json
writing: debug/nodes/2/ranges/289.json
writing: debug/nodes/2/ranges/290.json
writing: debug/nodes/2/ranges/291.json
writing: debug/nodes/2/ranges/292.json
writing: debug/nodes/2/ranges/293.json
writing: debug/nodes/2/ranges/294.json
writing: debug/nodes/2/ranges/295.json
writing: debug/nodes/2/ranges/296.json
writing: debug/nodes/2/ranges/297.json
writing: debug/nodes/2/ranges/298.json
writing: debug/nodes/2/ranges/299.json
writing: debug/nodes/2/ranges/320.json
writing: debug/nodes/2/ranges/321.json
writing: debug/nodes/2/ranges/324.json
writing: debug/nodes/2/ranges/325.json
writing: debug/nodes/2/ranges/356.json
writing: debug/nodes/2/ranges/358.json
writing: debug/nodes/2/ranges/361.json
writing: debug/nodes/2/ranges/362.json
writing: debug/nodes/2/ranges/363.json
writing: debug/nodes/2/ranges/376.json
writing: debug/nodes/3/status.json
using SQL connection URL for node 3: postgresql://root@192.168.1.159:26257/system?application_name=%24+cockroach+zip&connect_timeout=15&sslmode=disable
retrieving SQL data for crdb_internal.feature_usage... writing: debug/nodes/3/crdb_internal.feature_usage.txt
retrieving SQL data for crdb_internal.gossip_alerts... writing: debug/nodes/3/crdb_internal.gossip_alerts.txt
retrieving SQL data for crdb_internal.gossip_liveness... writing: debug/nodes/3/crdb_internal.gossip_liveness.txt
retrieving SQL data for crdb_internal.gossip_network... writing: debug/nodes/3/crdb_internal.gossip_network.txt
retrieving SQL data for crdb_internal.gossip_nodes... writing: debug/nodes/3/crdb_internal.gossip_nodes.txt
retrieving SQL data for crdb_internal.leases... writing: debug/nodes/3/crdb_internal.leases.txt
retrieving SQL data for crdb_internal.node_build_info... writing: debug/nodes/3/crdb_internal.node_build_info.txt
retrieving SQL data for crdb_internal.node_metrics... writing: debug/nodes/3/crdb_internal.node_metrics.txt
retrieving SQL data for crdb_internal.node_queries... writing: debug/nodes/3/crdb_internal.node_queries.txt
retrieving SQL data for crdb_internal.node_runtime_info... writing: debug/nodes/3/crdb_internal.node_runtime_info.txt
retrieving SQL data for crdb_internal.node_sessions... writing: debug/nodes/3/crdb_internal.node_sessions.txt
retrieving SQL data for crdb_internal.node_statement_statistics... writing: debug/nodes/3/crdb_internal.node_statement_statistics.txt
retrieving SQL data for crdb_internal.node_transaction_statistics... writing: debug/nodes/3/crdb_internal.node_transaction_statistics.txt
retrieving SQL data for crdb_internal.node_transactions... writing: debug/nodes/3/crdb_internal.node_transactions.txt
retrieving SQL data for crdb_internal.node_txn_stats... writing: debug/nodes/3/crdb_internal.node_txn_stats.txt
requesting data for debug/nodes/3/details... writing: debug/nodes/3/details.json
requesting data for debug/nodes/3/gossip... writing: debug/nodes/3/gossip.json
requesting data for debug/nodes/3/enginestats... writing: debug/nodes/3/enginestats.json
requesting stacks for node 3... writing: debug/nodes/3/stacks.txt
requesting threads for node 3... writing: debug/nodes/3/threads.txt
requesting heap profile for node 3... writing: debug/nodes/3/heap.pprof
requesting heap files for node 3... 221 found
writing: debug/nodes/3/heapprof/memprof.2020-11-17T01_56_45.501.245931040.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T01_57_35.501.247937480.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T03_00_05.527.230777904.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T03_00_55.528.240935320.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T03_01_45.528.244925640.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T03_17_25.534.244962584.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T03_18_15.534.251561744.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T04_20_45.556.232208288.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T04_22_35.557.241553880.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T04_23_25.557.248514496.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T04_24_15.557.250808080.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T04_55_25.573.266423288.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T05_56_25.602.238231392.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T05_57_05.603.238367016.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T06_01_25.606.239139272.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T06_02_15.606.249058360.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T06_56_25.639.249089032.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T07_56_25.687.234558264.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T07_57_15.687.241338024.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T07_58_05.687.245586480.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T08_58_55.723.247547400.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T08_59_45.724.247795584.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T09_20_25.735.248505968.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T09_23_05.738.250475784.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T09_23_55.738.253600920.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T10_23_55.769.152356904.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T10_24_05.769.189135784.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T10_24_45.769.192089864.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T10_24_55.770.228852744.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T10_25_45.770.251521528.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T11_25_45.797.160308616.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T11_25_55.797.197770368.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T11_26_05.797.234691312.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T11_26_55.798.248825080.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T12_30_15.816.238300912.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T12_33_35.816.238401136.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T12_34_25.816.250875488.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T13_16_55.829.251647128.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T13_17_45.828.254009856.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T14_17_45.848.200826584.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T14_17_55.848.238694232.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T14_18_45.849.250811704.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T15_20_35.873.229857872.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T15_21_25.873.235135528.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T15_22_15.873.243678984.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T15_37_35.878.244311200.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T15_38_25.879.253250968.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T16_38_45.902.232629504.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T16_42_15.903.241892112.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T16_43_05.904.245177168.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T16_44_55.905.245229352.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T16_45_45.905.247476736.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T17_46_35.924.233251432.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T17_47_25.925.244385512.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T17_54_35.927.246661280.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T17_55_25.926.247944256.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T17_56_15.926.251081496.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T18_58_15.947.226770880.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T19_00_05.948.237598168.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T19_04_25.949.240026504.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T19_05_15.949.246203736.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T19_06_05.950.249156840.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T20_06_05.967.229388944.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T20_06_55.967.242299464.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T20_07_45.969.246483680.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T20_46_45.979.246921464.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T20_47_35.978.248537584.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T21_47_35.998.193644112.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T21_47_45.998.232734376.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T21_48_35.999.243266024.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T21_59_26.005.244284472.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T22_00_16.007.250191376.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T23_00_16.033.166320552.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T23_00_26.033.203896192.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T23_00_36.033.241027056.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T23_01_26.033.245792752.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-17T23_02_16.033.251194304.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T00_03_56.064.231978120.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T00_04_46.063.237507064.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T00_05_36.063.243611312.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T00_07_26.065.249687504.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T00_08_16.066.250755664.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T01_10_16.079.231148496.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T01_11_06.080.236045552.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T01_15_36.080.239959880.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T01_16_26.082.247864280.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T01_17_16.080.251888672.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T02_17_16.109.236670752.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T02_18_06.109.246984128.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T02_33_16.120.248073128.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T02_34_06.122.255490536.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T03_35_16.143.234565728.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T03_36_06.142.245359784.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T03_58_06.150.246585168.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T03_58_56.151.252620096.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T03_59_46.151.257096544.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T04_59_46.183.198410888.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T04_59_56.183.234898800.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T05_00_46.183.247174808.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T05_38_46.201.247963008.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T05_39_36.201.250668208.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T06_45_16.237.243494264.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T06_50_26.240.246008928.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T07_29_16.258.246917936.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T07_30_06.260.256244920.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T07_30_56.260.258347824.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T08_31_06.294.183456928.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T08_31_16.294.220703952.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T08_32_06.295.226364872.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T08_32_56.294.248907392.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T08_59_56.310.248914952.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T10_01_56.354.237801648.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T10_02_46.354.244971544.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T10_16_56.362.245241160.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T10_17_46.363.249328928.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T10_18_36.364.256916000.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T11_21_26.397.231388136.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T11_22_16.397.233576120.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T11_24_16.400.236858864.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T11_25_06.399.247999216.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T11_25_56.400.248778680.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T12_26_16.416.233078256.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T12_27_06.415.243554016.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T12_43_56.420.244479568.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T12_44_46.420.250525248.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T12_45_36.420.254368288.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T13_45_36.438.163356824.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T13_45_46.438.205583160.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T13_45_56.438.243699712.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T13_46_46.439.251467328.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T14_46_46.461.220380792.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T14_47_36.462.220755000.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T14_47_56.469.239005488.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T14_50_36.468.246507904.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T14_51_26.468.249512424.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T15_51_26.487.183533600.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T15_51_36.487.222373864.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T15_52_26.487.229215784.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T15_53_16.486.236580320.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T15_54_06.487.251446232.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T16_56_56.505.229469152.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T16_57_46.505.239922016.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T16_58_36.506.249411208.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T17_10_26.509.250236568.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T17_23_46.513.253006280.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T18_23_46.531.237592432.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T18_24_36.532.249033744.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T18_25_26.532.254176992.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T19_25_36.553.181168408.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T19_25_46.553.219165488.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T19_26_36.553.221922984.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T19_27_26.553.233546152.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T19_28_16.553.245029304.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T20_29_16.573.214365744.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T20_29_36.573.223293568.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T20_29_46.573.230621096.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T20_31_36.575.242943112.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T20_32_26.575.254402872.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T21_32_46.597.240015184.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T21_33_36.597.250103296.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T22_24_16.610.266543048.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T22_24_36.611.290490128.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T22_45_56.632.304589664.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T23_45_56.693.209567400.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T23_46_06.694.253307976.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T23_47_36.696.260490928.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-18T23_51_36.701.265786704.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T00_14_06.728.271096584.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T01_53_26.832.260770248.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T02_18_26.860.263447432.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T02_24_36.866.276885656.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T03_10_36.914.277459016.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T03_20_16.922.281622880.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T04_20_16.981.244082624.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T04_20_56.982.258990720.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T04_25_36.986.260382992.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T04_26_16.987.270987376.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T05_31_07.060.261992048.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T05_40_07.069.262761816.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T05_40_47.072.263721456.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T06_24_27.113.264483400.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T06_36_57.124.301768920.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T07_38_27.182.244673352.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T07_39_37.183.251988376.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T07_41_57.185.261381792.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T07_43_47.189.265417080.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T08_43_37.248.267028864.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T09_53_17.317.264821088.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T09_57_37.321.266608968.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T10_12_57.337.277283224.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T10_38_27.355.288255208.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T10_39_07.357.347760544.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T11_47_17.419.266754560.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T11_49_37.420.273702704.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T12_40_57.467.289226816.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T12_41_27.467.293325304.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T12_42_37.467.304356904.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T13_42_37.520.256524616.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T13_44_17.521.261589472.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T13_44_57.521.265580896.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T13_52_27.530.286673592.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T14_52_27.596.163358080.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T14_52_47.596.235382384.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T14_53_17.597.260817064.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T14_58_57.603.262654656.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T14_59_27.603.280523440.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T15_59_37.635.230278192.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T16_00_17.635.232379120.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T16_00_57.635.238710344.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T16_01_37.636.260340448.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T16_02_17.636.262281056.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T17_04_17.665.226870200.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T17_04_57.666.242070840.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T17_05_37.667.254577808.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T17_12_07.669.255847904.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T17_12_47.669.267502888.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T18_14_27.694.252358456.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T18_17_47.690.255042416.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T18_18_27.690.257903600.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T18_19_07.691.260949632.pprof
writing: debug/nodes/3/heapprof/memprof.2020-11-19T18_19_47.690.263476640.pprof
requesting goroutine files for node 3... 0 found
requesting log files list... 27 found
requesting log file cockroach.nvmedb2.cockroachdb.2020-11-10T01_00_59Z.001213.log... writing: debug/nodes/3/logs/cockroach.nvmedb2.cockroachdb.2020-11-10T01_00_59Z.001213.log
requesting log file cockroach.nvmedb2.cockroachdb.2020-11-10T16_35_32Z.001747.log... writing: debug/nodes/3/logs/cockroach.nvmedb2.cockroachdb.2020-11-10T16_35_32Z.001747.log
requesting log file cockroach.nvmedb2.cockroachdb.2020-11-10T23_17_53Z.003371.log... writing: debug/nodes/3/logs/cockroach.nvmedb2.cockroachdb.2020-11-10T23_17_53Z.003371.log
requesting log file cockroach.nvmedb2.cockroachdb.2020-11-11T21_53_41Z.009990.log... writing: debug/nodes/3/logs/cockroach.nvmedb2.cockroachdb.2020-11-11T21_53_41Z.009990.log
requesting log file cockroach.nvmedb2.cockroachdb.2020-11-13T02_02_02Z.009990.log... writing: debug/nodes/3/logs/cockroach.nvmedb2.cockroachdb.2020-11-13T02_02_02Z.009990.log
requesting log file cockroach.nvmedb2.cockroachdb.2020-11-14T06_10_39Z.009990.log... writing: debug/nodes/3/logs/cockroach.nvmedb2.cockroachdb.2020-11-14T06_10_39Z.009990.log
requesting log file cockroach.nvmedb2.cockroachdb.2020-11-15T10_18_34Z.009990.log... writing: debug/nodes/3/logs/cockroach.nvmedb2.cockroachdb.2020-11-15T10_18_34Z.009990.log
requesting log file cockroach.nvmedb2.cockroachdb.2020-11-16T14_25_45Z.009990.log... writing: debug/nodes/3/logs/cockroach.nvmedb2.cockroachdb.2020-11-16T14_25_45Z.009990.log
requesting log file cockroach.nvmedb2.cockroachdb.2020-11-17T18_32_55Z.009990.log... writing: debug/nodes/3/logs/cockroach.nvmedb2.cockroachdb.2020-11-17T18_32_55Z.009990.log
requesting log file cockroach.nvmedb2.cockroachdb.2020-11-18T22_39_46Z.009990.log... writing: debug/nodes/3/logs/cockroach.nvmedb2.cockroachdb.2020-11-18T22_39_46Z.009990.log
requesting log file cockroach-stderr.nvmedb2.cockroachdb.2020-11-10T01_00_59Z.001213.log... writing: debug/nodes/3/logs/cockroach-stderr.nvmedb2.cockroachdb.2020-11-10T01_00_59Z.001213.log
requesting log file cockroach-stderr.nvmedb2.cockroachdb.2020-11-10T16_35_32Z.001747.log... writing: debug/nodes/3/logs/cockroach-stderr.nvmedb2.cockroachdb.2020-11-10T16_35_32Z.001747.log
requesting log file cockroach-stderr.nvmedb2.cockroachdb.2020-11-10T23_17_53Z.003371.log... writing: debug/nodes/3/logs/cockroach-stderr.nvmedb2.cockroachdb.2020-11-10T23_17_53Z.003371.log
requesting log file cockroach-stderr.nvmedb2.cockroachdb.2020-11-11T21_53_41Z.009990.log... writing: debug/nodes/3/logs/cockroach-stderr.nvmedb2.cockroachdb.2020-11-11T21_53_41Z.009990.log
requesting log file cockroach-pebble.nvmedb2.cockroachdb.2020-11-10T07_03_16Z.001213.log... writing: debug/nodes/3/logs/cockroach-pebble.nvmedb2.cockroachdb.2020-11-10T07_03_16Z.001213.log
requesting log file cockroach-pebble.nvmedb2.cockroachdb.2020-11-10T09_59_57Z.001213.log... writing: debug/nodes/3/logs/cockroach-pebble.nvmedb2.cockroachdb.2020-11-10T09_59_57Z.001213.log
requesting log file cockroach-pebble.nvmedb2.cockroachdb.2020-11-10T10_48_33Z.001213.log... writing: debug/nodes/3/logs/cockroach-pebble.nvmedb2.cockroachdb.2020-11-10T10_48_33Z.001213.log
requesting log file cockroach-pebble.nvmedb2.cockroachdb.2020-11-10T11_32_52Z.001213.log... writing: debug/nodes/3/logs/cockroach-pebble.nvmedb2.cockroachdb.2020-11-10T11_32_52Z.001213.log
requesting log file cockroach-pebble.nvmedb2.cockroachdb.2020-11-10T12_18_49Z.001213.log... writing: debug/nodes/3/logs/cockroach-pebble.nvmedb2.cockroachdb.2020-11-10T12_18_49Z.001213.log
requesting log file cockroach-pebble.nvmedb2.cockroachdb.2020-11-10T13_03_32Z.001213.log... writing: debug/nodes/3/logs/cockroach-pebble.nvmedb2.cockroachdb.2020-11-10T13_03_32Z.001213.log
requesting log file cockroach-pebble.nvmedb2.cockroachdb.2020-11-10T13_49_11Z.001213.log... writing: debug/nodes/3/logs/cockroach-pebble.nvmedb2.cockroachdb.2020-11-10T13_49_11Z.001213.log
requesting log file cockroach-pebble.nvmedb2.cockroachdb.2020-11-10T14_36_42Z.001213.log... writing: debug/nodes/3/logs/cockroach-pebble.nvmedb2.cockroachdb.2020-11-10T14_36_42Z.001213.log
requesting log file cockroach-pebble.nvmedb2.cockroachdb.2020-11-10T15_24_07Z.001213.log... writing: debug/nodes/3/logs/cockroach-pebble.nvmedb2.cockroachdb.2020-11-10T15_24_07Z.001213.log
requesting log file cockroach-pebble.nvmedb2.cockroachdb.2020-11-10T16_15_22Z.001213.log... writing: debug/nodes/3/logs/cockroach-pebble.nvmedb2.cockroachdb.2020-11-10T16_15_22Z.001213.log
requesting log file cockroach-pebble.nvmedb2.cockroachdb.2020-11-10T16_35_32Z.001747.log... writing: debug/nodes/3/logs/cockroach-pebble.nvmedb2.cockroachdb.2020-11-10T16_35_32Z.001747.log
requesting log file cockroach-pebble.nvmedb2.cockroachdb.2020-11-10T23_17_53Z.003371.log... writing: debug/nodes/3/logs/cockroach-pebble.nvmedb2.cockroachdb.2020-11-10T23_17_53Z.003371.log
requesting log file cockroach-pebble.nvmedb2.cockroachdb.2020-11-11T21_53_41Z.009990.log... writing: debug/nodes/3/logs/cockroach-pebble.nvmedb2.cockroachdb.2020-11-11T21_53_41Z.009990.log
requesting ranges... 299 found
writing: debug/nodes/3/ranges/1.json
writing: debug/nodes/3/ranges/2.json
writing: debug/nodes/3/ranges/3.json
writing: debug/nodes/3/ranges/4.json
writing: debug/nodes/3/ranges/5.json
writing: debug/nodes/3/ranges/6.json
writing: debug/nodes/3/ranges/7.json
writing: debug/nodes/3/ranges/8.json
writing: debug/nodes/3/ranges/9.json
writing: debug/nodes/3/ranges/10.json
writing: debug/nodes/3/ranges/11.json
writing: debug/nodes/3/ranges/12.json
writing: debug/nodes/3/ranges/13.json
writing: debug/nodes/3/ranges/14.json
writing: debug/nodes/3/ranges/15.json
writing: debug/nodes/3/ranges/16.json
writing: debug/nodes/3/ranges/17.json
writing: debug/nodes/3/ranges/18.json
writing: debug/nodes/3/ranges/19.json
writing: debug/nodes/3/ranges/20.json
writing: debug/nodes/3/ranges/21.json
writing: debug/nodes/3/ranges/22.json
writing: debug/nodes/3/ranges/23.json
writing: debug/nodes/3/ranges/24.json
writing: debug/nodes/3/ranges/25.json
writing: debug/nodes/3/ranges/26.json
writing: debug/nodes/3/ranges/27.json
writing: debug/nodes/3/ranges/28.json
writing: debug/nodes/3/ranges/29.json
writing: debug/nodes/3/ranges/30.json
writing: debug/nodes/3/ranges/31.json
writing: debug/nodes/3/ranges/32.json
writing: debug/nodes/3/ranges/33.json
writing: debug/nodes/3/ranges/34.json
writing: debug/nodes/3/ranges/35.json
writing: debug/nodes/3/ranges/36.json
writing: debug/nodes/3/ranges/37.json
writing: debug/nodes/3/ranges/38.json
writing: debug/nodes/3/ranges/39.json
writing: debug/nodes/3/ranges/40.json
writing: debug/nodes/3/ranges/41.json
writing: debug/nodes/3/ranges/42.json
writing: debug/nodes/3/ranges/43.json
writing: debug/nodes/3/ranges/44.json
writing: debug/nodes/3/ranges/45.json
writing: debug/nodes/3/ranges/46.json
writing: debug/nodes/3/ranges/47.json
writing: debug/nodes/3/ranges/48.json
writing: debug/nodes/3/ranges/49.json
writing: debug/nodes/3/ranges/50.json
writing: debug/nodes/3/ranges/51.json
writing: debug/nodes/3/ranges/52.json
writing: debug/nodes/3/ranges/53.json
writing: debug/nodes/3/ranges/54.json
writing: debug/nodes/3/ranges/55.json
writing: debug/nodes/3/ranges/56.json
writing: debug/nodes/3/ranges/57.json
writing: debug/nodes/3/ranges/58.json
writing: debug/nodes/3/ranges/60.json
writing: debug/nodes/3/ranges/61.json
writing: debug/nodes/3/ranges/62.json
writing: debug/nodes/3/ranges/63.json
writing: debug/nodes/3/ranges/64.json
writing: debug/nodes/3/ranges/65.json
writing: debug/nodes/3/ranges/66.json
writing: debug/nodes/3/ranges/67.json
writing: debug/nodes/3/ranges/68.json
writing: debug/nodes/3/ranges/69.json
writing: debug/nodes/3/ranges/70.json
writing: debug/nodes/3/ranges/71.json
writing: debug/nodes/3/ranges/72.json
writing: debug/nodes/3/ranges/73.json
writing: debug/nodes/3/ranges/74.json
writing: debug/nodes/3/ranges/75.json
writing: debug/nodes/3/ranges/76.json
writing: debug/nodes/3/ranges/77.json
writing: debug/nodes/3/ranges/78.json
writing: debug/nodes/3/ranges/79.json
writing: debug/nodes/3/ranges/80.json
writing: debug/nodes/3/ranges/81.json
writing: debug/nodes/3/ranges/82.json
writing: debug/nodes/3/ranges/83.json
writing: debug/nodes/3/ranges/84.json
writing: debug/nodes/3/ranges/85.json
writing: debug/nodes/3/ranges/86.json
writing: debug/nodes/3/ranges/87.json
writing: debug/nodes/3/ranges/88.json
writing: debug/nodes/3/ranges/89.json
writing: debug/nodes/3/ranges/90.json
writing: debug/nodes/3/ranges/92.json
writing: debug/nodes/3/ranges/94.json
writing: debug/nodes/3/ranges/95.json
writing: debug/nodes/3/ranges/96.json
writing: debug/nodes/3/ranges/97.json
writing: debug/nodes/3/ranges/98.json
writing: debug/nodes/3/ranges/99.json
writing: debug/nodes/3/ranges/100.json
writing: debug/nodes/3/ranges/101.json
writing: debug/nodes/3/ranges/102.json
writing: debug/nodes/3/ranges/103.json
writing: debug/nodes/3/ranges/104.json
writing: debug/nodes/3/ranges/105.json
writing: debug/nodes/3/ranges/106.json
writing: debug/nodes/3/ranges/107.json
writing: debug/nodes/3/ranges/108.json
writing: debug/nodes/3/ranges/109.json
writing: debug/nodes/3/ranges/110.json
writing: debug/nodes/3/ranges/111.json
writing: debug/nodes/3/ranges/112.json
writing: debug/nodes/3/ranges/113.json
writing: debug/nodes/3/ranges/114.json
writing: debug/nodes/3/ranges/115.json
writing: debug/nodes/3/ranges/116.json
writing: debug/nodes/3/ranges/117.json
writing: debug/nodes/3/ranges/118.json
writing: debug/nodes/3/ranges/119.json
writing: debug/nodes/3/ranges/120.json
writing: debug/nodes/3/ranges/121.json
writing: debug/nodes/3/ranges/122.json
writing: debug/nodes/3/ranges/123.json
writing: debug/nodes/3/ranges/124.json
writing: debug/nodes/3/ranges/125.json
writing: debug/nodes/3/ranges/126.json
writing: debug/nodes/3/ranges/127.json
writing: debug/nodes/3/ranges/128.json
writing: debug/nodes/3/ranges/129.json
writing: debug/nodes/3/ranges/130.json
writing: debug/nodes/3/ranges/131.json
writing: debug/nodes/3/ranges/132.json
writing: debug/nodes/3/ranges/133.json
writing: debug/nodes/3/ranges/134.json
writing: debug/nodes/3/ranges/135.json
writing: debug/nodes/3/ranges/136.json
writing: debug/nodes/3/ranges/137.json
writing: debug/nodes/3/ranges/138.json
writing: debug/nodes/3/ranges/139.json
writing: debug/nodes/3/ranges/140.json
writing: debug/nodes/3/ranges/141.json
writing: debug/nodes/3/ranges/142.json
writing: debug/nodes/3/ranges/143.json
writing: debug/nodes/3/ranges/144.json
writing: debug/nodes/3/ranges/145.json
writing: debug/nodes/3/ranges/146.json
writing: debug/nodes/3/ranges/147.json
writing: debug/nodes/3/ranges/148.json
writing: debug/nodes/3/ranges/149.json
writing: debug/nodes/3/ranges/150.json
writing: debug/nodes/3/ranges/151.json
writing: debug/nodes/3/ranges/153.json
writing: debug/nodes/3/ranges/154.json
writing: debug/nodes/3/ranges/155.json
writing: debug/nodes/3/ranges/156.json
writing: debug/nodes/3/ranges/157.json
writing: debug/nodes/3/ranges/158.json
writing: debug/nodes/3/ranges/159.json
writing: debug/nodes/3/ranges/160.json
writing: debug/nodes/3/ranges/161.json
writing: debug/nodes/3/ranges/162.json
writing: debug/nodes/3/ranges/163.json
writing: debug/nodes/3/ranges/164.json
writing: debug/nodes/3/ranges/165.json
writing: debug/nodes/3/ranges/166.json
writing: debug/nodes/3/ranges/167.json
writing: debug/nodes/3/ranges/168.json
writing: debug/nodes/3/ranges/169.json
writing: debug/nodes/3/ranges/170.json
writing: debug/nodes/3/ranges/171.json
writing: debug/nodes/3/ranges/172.json
writing: debug/nodes/3/ranges/173.json
writing: debug/nodes/3/ranges/174.json
writing: debug/nodes/3/ranges/175.json
writing: debug/nodes/3/ranges/176.json
writing: debug/nodes/3/ranges/177.json
writing: debug/nodes/3/ranges/178.json
writing: debug/nodes/3/ranges/179.json
writing: debug/nodes/3/ranges/180.json
writing: debug/nodes/3/ranges/181.json
writing: debug/nodes/3/ranges/182.json
writing: debug/nodes/3/ranges/183.json
writing: debug/nodes/3/ranges/184.json
writing: debug/nodes/3/ranges/185.json
writing: debug/nodes/3/ranges/186.json
writing: debug/nodes/3/ranges/187.json
writing: debug/nodes/3/ranges/188.json
writing: debug/nodes/3/ranges/189.json
writing: debug/nodes/3/ranges/190.json
writing: debug/nodes/3/ranges/191.json
writing: debug/nodes/3/ranges/192.json
writing: debug/nodes/3/ranges/193.json
writing: debug/nodes/3/ranges/194.json
writing: debug/nodes/3/ranges/195.json
writing: debug/nodes/3/ranges/196.json
writing: debug/nodes/3/ranges/197.json
writing: debug/nodes/3/ranges/198.json
writing: debug/nodes/3/ranges/199.json
writing: debug/nodes/3/ranges/200.json
writing: debug/nodes/3/ranges/201.json
writing: debug/nodes/3/ranges/202.json
writing: debug/nodes/3/ranges/203.json
writing: debug/nodes/3/ranges/204.json
writing: debug/nodes/3/ranges/205.json
writing: debug/nodes/3/ranges/206.json
writing: debug/nodes/3/ranges/207.json
writing: debug/nodes/3/ranges/208.json
writing: debug/nodes/3/ranges/209.json
writing: debug/nodes/3/ranges/210.json
writing: debug/nodes/3/ranges/211.json
writing: debug/nodes/3/ranges/212.json
writing: debug/nodes/3/ranges/213.json
writing: debug/nodes/3/ranges/214.json
writing: debug/nodes/3/ranges/215.json
writing: debug/nodes/3/ranges/216.json
writing: debug/nodes/3/ranges/217.json
writing: debug/nodes/3/ranges/218.json
writing: debug/nodes/3/ranges/219.json
writing: debug/nodes/3/ranges/220.json
writing: debug/nodes/3/ranges/221.json
writing: debug/nodes/3/ranges/222.json
writing: debug/nodes/3/ranges/223.json
writing: debug/nodes/3/ranges/224.json
writing: debug/nodes/3/ranges/225.json
writing: debug/nodes/3/ranges/226.json
writing: debug/nodes/3/ranges/227.json
writing: debug/nodes/3/ranges/228.json
writing: debug/nodes/3/ranges/229.json
writing: debug/nodes/3/ranges/230.json
writing: debug/nodes/3/ranges/231.json
writing: debug/nodes/3/ranges/232.json
writing: debug/nodes/3/ranges/233.json
writing: debug/nodes/3/ranges/234.json
writing: debug/nodes/3/ranges/235.json
writing: debug/nodes/3/ranges/236.json
writing: debug/nodes/3/ranges/237.json
writing: debug/nodes/3/ranges/238.json
writing: debug/nodes/3/ranges/239.json
writing: debug/nodes/3/ranges/240.json
writing: debug/nodes/3/ranges/241.json
writing: debug/nodes/3/ranges/242.json
writing: debug/nodes/3/ranges/243.json
writing: debug/nodes/3/ranges/244.json
writing: debug/nodes/3/ranges/245.json
writing: debug/nodes/3/ranges/246.json
writing: debug/nodes/3/ranges/247.json
writing: debug/nodes/3/ranges/248.json
writing: debug/nodes/3/ranges/249.json
writing: debug/nodes/3/ranges/250.json
writing: debug/nodes/3/ranges/251.json
writing: debug/nodes/3/ranges/252.json
writing: debug/nodes/3/ranges/253.json
writing: debug/nodes/3/ranges/254.json
writing: debug/nodes/3/ranges/255.json
writing: debug/nodes/3/ranges/256.json
writing: debug/nodes/3/ranges/257.json
writing: debug/nodes/3/ranges/258.json
writing: debug/nodes/3/ranges/259.json
writing: debug/nodes/3/ranges/260.json
writing: debug/nodes/3/ranges/261.json
writing: debug/nodes/3/ranges/262.json
writing: debug/nodes/3/ranges/263.json
writing: debug/nodes/3/ranges/264.json
writing: debug/nodes/3/ranges/265.json
writing: debug/nodes/3/ranges/266.json
writing: debug/nodes/3/ranges/267.json
writing: debug/nodes/3/ranges/268.json
writing: debug/nodes/3/ranges/269.json
writing: debug/nodes/3/ranges/270.json
writing: debug/nodes/3/ranges/271.json
writing: debug/nodes/3/ranges/272.json
writing: debug/nodes/3/ranges/276.json
writing: debug/nodes/3/ranges/277.json
writing: debug/nodes/3/ranges/278.json
writing: debug/nodes/3/ranges/279.json
writing: debug/nodes/3/ranges/280.json
writing: debug/nodes/3/ranges/281.json
writing: debug/nodes/3/ranges/282.json
writing: debug/nodes/3/ranges/286.json
writing: debug/nodes/3/ranges/287.json
writing: debug/nodes/3/ranges/288.json
writing: debug/nodes/3/ranges/289.json
writing: debug/nodes/3/ranges/290.json
writing: debug/nodes/3/ranges/291.json
writing: debug/nodes/3/ranges/292.json
writing: debug/nodes/3/ranges/293.json
writing: debug/nodes/3/ranges/294.json
writing: debug/nodes/3/ranges/295.json
writing: debug/nodes/3/ranges/296.json
writing: debug/nodes/3/ranges/297.json
writing: debug/nodes/3/ranges/298.json
writing: debug/nodes/3/ranges/299.json
writing: debug/nodes/3/ranges/320.json
writing: debug/nodes/3/ranges/321.json
writing: debug/nodes/3/ranges/324.json
writing: debug/nodes/3/ranges/325.json
writing: debug/nodes/3/ranges/356.json
writing: debug/nodes/3/ranges/358.json
writing: debug/nodes/3/ranges/361.json
writing: debug/nodes/3/ranges/362.json
writing: debug/nodes/3/ranges/363.json
writing: debug/nodes/3/ranges/376.json
writing: debug/nodes/4/status.json
using SQL connection URL for node 4: postgresql://root@192.168.1.169:26257/system?application_name=%24+cockroach+zip&connect_timeout=15&sslmode=disable
retrieving SQL data for crdb_internal.feature_usage... writing: debug/nodes/4/crdb_internal.feature_usage.txt
retrieving SQL data for crdb_internal.gossip_alerts... writing: debug/nodes/4/crdb_internal.gossip_alerts.txt
retrieving SQL data for crdb_internal.gossip_liveness... writing: debug/nodes/4/crdb_internal.gossip_liveness.txt
retrieving SQL data for crdb_internal.gossip_network... writing: debug/nodes/4/crdb_internal.gossip_network.txt
retrieving SQL data for crdb_internal.gossip_nodes... writing: debug/nodes/4/crdb_internal.gossip_nodes.txt
retrieving SQL data for crdb_internal.leases... writing: debug/nodes/4/crdb_internal.leases.txt
retrieving SQL data for crdb_internal.node_build_info... writing: debug/nodes/4/crdb_internal.node_build_info.txt
retrieving SQL data for crdb_internal.node_metrics... writing: debug/nodes/4/crdb_internal.node_metrics.txt
retrieving SQL data for crdb_internal.node_queries... writing: debug/nodes/4/crdb_internal.node_queries.txt
retrieving SQL data for crdb_internal.node_runtime_info... writing: debug/nodes/4/crdb_internal.node_runtime_info.txt
retrieving SQL data for crdb_internal.node_sessions... writing: debug/nodes/4/crdb_internal.node_sessions.txt
retrieving SQL data for crdb_internal.node_statement_statistics... writing: debug/nodes/4/crdb_internal.node_statement_statistics.txt
retrieving SQL data for crdb_internal.node_transaction_statistics... writing: debug/nodes/4/crdb_internal.node_transaction_statistics.txt
retrieving SQL data for crdb_internal.node_transactions... writing: debug/nodes/4/crdb_internal.node_transactions.txt
retrieving SQL data for crdb_internal.node_txn_stats... writing: debug/nodes/4/crdb_internal.node_txn_stats.txt
requesting data for debug/nodes/4/details... writing: debug/nodes/4/details.json
requesting data for debug/nodes/4/gossip... writing: debug/nodes/4/gossip.json
requesting data for debug/nodes/4/enginestats... writing: debug/nodes/4/enginestats.json
requesting stacks for node 4... writing: debug/nodes/4/stacks.txt
requesting threads for node 4... writing: debug/nodes/4/threads.txt
requesting heap profile for node 4... writing: debug/nodes/4/heap.pprof
requesting heap files for node 4... 57 found
writing: debug/nodes/4/heapprof/memprof.2020-11-19T00_08_24.554.222760568.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T00_14_14.537.223296208.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T00_20_14.517.225311520.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T00_26_14.499.231749880.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T00_32_54.480.232004856.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T02_19_14.158.231271232.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T02_20_14.154.233099000.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T02_25_34.136.238075320.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T02_31_14.124.240761192.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T02_35_54.111.245591264.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T03_42_53.885.199809616.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T04_06_23.801.200761216.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T04_15_53.766.201343664.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T04_58_23.614.201955912.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T05_20_13.536.202011296.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T06_20_23.320.152118720.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T06_20_33.318.180566936.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T06_21_23.315.181164016.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T06_22_23.312.197385224.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T06_23_13.309.201053840.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T07_26_13.076.196959712.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T07_27_03.073.198723400.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T07_38_23.031.199969016.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T07_39_13.028.202176784.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T07_51_32.983.203897992.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T08_51_52.763.189763856.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T08_53_42.757.194634136.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T08_54_32.753.198912872.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T08_57_22.743.201094592.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T08_58_12.741.202074208.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T09_58_22.518.173437360.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T09_58_32.516.200221216.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T09_59_22.513.201723520.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T10_59_32.290.179242272.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T11_00_22.288.181491792.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T11_01_22.283.196469480.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T11_02_12.281.202103752.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T11_23_02.205.202367288.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T12_24_51.981.181386976.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T12_25_51.977.196620896.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T12_26_41.974.198567720.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T12_29_31.964.201638176.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T12_30_21.961.205110968.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T13_49_41.669.199503312.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T13_50_31.665.201469480.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T13_58_01.639.202071216.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T14_09_11.597.202552856.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T14_10_01.594.203204440.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T15_26_11.314.201687472.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T15_27_01.312.202249080.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T15_48_41.235.202402936.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T15_49_31.231.204167760.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T16_34_01.068.206825264.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T17_34_10.850.194364296.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T17_35_00.847.199846288.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T17_39_40.830.202908192.pprof
writing: debug/nodes/4/heapprof/memprof.2020-11-19T18_18_00.691.204370024.pprof
requesting goroutine files for node 4... 0 found
requesting log files list... 3 found
requesting log file cockroach.nvmedb4.cockroachdb.2020-11-18T22_45_04Z.001219.log... writing: debug/nodes/4/logs/cockroach.nvmedb4.cockroachdb.2020-11-18T22_45_04Z.001219.log
requesting log file cockroach-stderr.nvmedb4.cockroachdb.2020-11-18T22_45_04Z.001219.log... writing: debug/nodes/4/logs/cockroach-stderr.nvmedb4.cockroachdb.2020-11-18T22_45_04Z.001219.log
requesting log file cockroach-pebble.nvmedb4.cockroachdb.2020-11-18T22_45_04Z.001219.log... writing: debug/nodes/4/logs/cockroach-pebble.nvmedb4.cockroachdb.2020-11-18T22_45_04Z.001219.log
requesting ranges... 0 found
writing: debug/nodes/5/status.json
using SQL connection URL for node 5: postgresql://root@192.168.1.55:26257/system?application_name=%24+cockroach+zip&connect_timeout=15&sslmode=disable
retrieving SQL data for crdb_internal.feature_usage... writing: debug/nodes/5/crdb_internal.feature_usage.txt
retrieving SQL data for crdb_internal.gossip_alerts... writing: debug/nodes/5/crdb_internal.gossip_alerts.txt
retrieving SQL data for crdb_internal.gossip_liveness... writing: debug/nodes/5/crdb_internal.gossip_liveness.txt
retrieving SQL data for crdb_internal.gossip_network... writing: debug/nodes/5/crdb_internal.gossip_network.txt
retrieving SQL data for crdb_internal.gossip_nodes... writing: debug/nodes/5/crdb_internal.gossip_nodes.txt
retrieving SQL data for crdb_internal.leases... writing: debug/nodes/5/crdb_internal.leases.txt
retrieving SQL data for crdb_internal.node_build_info... writing: debug/nodes/5/crdb_internal.node_build_info.txt
retrieving SQL data for crdb_internal.node_metrics... writing: debug/nodes/5/crdb_internal.node_metrics.txt
retrieving SQL data for crdb_internal.node_queries... writing: debug/nodes/5/crdb_internal.node_queries.txt
retrieving SQL data for crdb_internal.node_runtime_info... writing: debug/nodes/5/crdb_internal.node_runtime_info.txt
retrieving SQL data for crdb_internal.node_sessions... writing: debug/nodes/5/crdb_internal.node_sessions.txt
retrieving SQL data for crdb_internal.node_statement_statistics... writing: debug/nodes/5/crdb_internal.node_statement_statistics.txt
retrieving SQL data for crdb_internal.node_transaction_statistics... writing: debug/nodes/5/crdb_internal.node_transaction_statistics.txt
retrieving SQL data for crdb_internal.node_transactions... writing: debug/nodes/5/crdb_internal.node_transactions.txt
retrieving SQL data for crdb_internal.node_txn_stats... writing: debug/nodes/5/crdb_internal.node_txn_stats.txt
requesting data for debug/nodes/5/details... writing: debug/nodes/5/details.json
requesting data for debug/nodes/5/gossip... writing: debug/nodes/5/gossip.json
requesting data for debug/nodes/5/enginestats... writing: debug/nodes/5/enginestats.json
requesting stacks for node 5... writing: debug/nodes/5/stacks.txt
requesting threads for node 5... writing: debug/nodes/5/threads.txt
requesting heap profile for node 5... writing: debug/nodes/5/heap.pprof
requesting heap files for node 5... 51 found
writing: debug/nodes/5/heapprof/memprof.2020-11-19T01_11_50.968.191992808.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T01_13_10.967.201102424.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T01_18_30.964.205317200.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T01_41_20.954.205336632.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T01_52_30.950.205948744.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T02_52_40.911.171107192.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T02_53_40.909.180422944.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T02_54_40.908.188984888.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T03_18_20.886.189246744.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T03_19_20.885.196259000.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T04_19_30.831.158626992.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T04_19_40.831.183654480.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T04_24_30.826.183734696.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T04_25_30.825.195382880.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T05_25_40.772.164301800.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T05_25_50.772.188957832.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T05_34_40.762.192046704.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T05_57_30.740.192758896.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T06_57_40.686.158801536.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T06_57_50.686.183000720.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T06_58_50.685.190542200.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T07_10_30.676.191089064.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T08_10_40.619.173768240.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T08_11_40.618.184192968.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T08_17_30.614.186227736.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T08_18_30.611.197301648.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T09_59_50.519.189893168.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T10_10_30.508.190243592.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T10_19_20.501.190652464.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T10_33_10.488.191471328.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T10_56_50.467.193811896.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T11_57_10.413.185114512.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T12_05_00.406.190467416.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T12_29_40.385.191760680.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T13_27_10.335.192841728.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T13_49_30.314.193282800.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T14_51_30.260.188423072.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T14_56_20.255.190313272.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T15_09_50.243.190536496.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T15_10_40.243.191023880.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T15_11_40.243.197411672.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T16_17_40.180.188886728.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T16_21_30.178.189011048.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T16_22_20.176.190865392.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T16_36_10.163.191231216.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T16_45_00.155.193982728.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T17_45_10.100.189797192.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T17_50_00.095.190428224.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T17_55_50.089.191109528.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T18_17_20.071.191676912.pprof
writing: debug/nodes/5/heapprof/memprof.2020-11-19T18_41_50.050.193000784.pprof
requesting goroutine files for node 5... 0 found
requesting log files list... 3 found
requesting log file cockroach.nvmedb5.cockroachdb.2020-11-19T00_03_20Z.001246.log... writing: debug/nodes/5/logs/cockroach.nvmedb5.cockroachdb.2020-11-19T00_03_20Z.001246.log
requesting log file cockroach-stderr.nvmedb5.cockroachdb.2020-11-19T00_03_20Z.001246.log... writing: debug/nodes/5/logs/cockroach-stderr.nvmedb5.cockroachdb.2020-11-19T00_03_20Z.001246.log
requesting log file cockroach-pebble.nvmedb5.cockroachdb.2020-11-19T00_03_20Z.001246.log... writing: debug/nodes/5/logs/cockroach-pebble.nvmedb5.cockroachdb.2020-11-19T00_03_20Z.001246.log
requesting ranges... 0 found
requesting list of SQL databases... 4 found
requesting database details for defaultdb... writing: debug/schema/defaultdb@details.json
0 tables found
requesting database details for kv... writing: debug/schema/kv@details.json
1 tables found
requesting table details for kv.public.kv... writing: debug/schema/kv/public_kv.json
requesting database details for postgres... writing: debug/schema/postgres@details.json
0 tables found
requesting database details for system... writing: debug/schema/system@details.json
29 tables found
requesting table details for system.public.namespace... writing: debug/schema/system/public_namespace.json
requesting table details for system.public.descriptor... writing: debug/schema/system/public_descriptor.json
requesting table details for system.public.users... writing: debug/schema/system/public_users.json
requesting table details for system.public.zones... writing: debug/schema/system/public_zones.json
requesting table details for system.public.settings... writing: debug/schema/system/public_settings.json
requesting table details for system.public.tenants... writing: debug/schema/system/public_tenants.json
requesting table details for system.public.lease... writing: debug/schema/system/public_lease.json
requesting table details for system.public.eventlog... writing: debug/schema/system/public_eventlog.json
requesting table details for system.public.rangelog... writing: debug/schema/system/public_rangelog.json
requesting table details for system.public.ui... writing: debug/schema/system/public_ui.json
requesting table details for system.public.jobs... writing: debug/schema/system/public_jobs.json
requesting table details for system.public.web_sessions... writing: debug/schema/system/public_web_sessions.json
requesting table details for system.public.table_statistics... writing: debug/schema/system/public_table_statistics.json
requesting table details for system.public.locations... writing: debug/schema/system/public_locations.json
requesting table details for system.public.role_members... writing: debug/schema/system/public_role_members.json
requesting table details for system.public.comments... writing: debug/schema/system/public_comments.json
requesting table details for system.public.replication_constraint_stats... writing: debug/schema/system/public_replication_constraint_stats.json
requesting table details for system.public.replication_critical_localities... writing: debug/schema/system/public_replication_critical_localities.json
requesting table details for system.public.replication_stats... writing: debug/schema/system/public_replication_stats.json
requesting table details for system.public.reports_meta... writing: debug/schema/system/public_reports_meta.json
requesting table details for system.public.namespace2... writing: debug/schema/system/public_namespace2.json
requesting table details for system.public.protected_ts_meta... writing: debug/schema/system/public_protected_ts_meta.json
requesting table details for system.public.protected_ts_records... writing: debug/schema/system/public_protected_ts_records.json
requesting table details for system.public.role_options... writing: debug/schema/system/public_role_options.json
requesting table details for system.public.statement_bundle_chunks... writing: debug/schema/system/public_statement_bundle_chunks.json
requesting table details for system.public.statement_diagnostics_requests... writing: debug/schema/system/public_statement_diagnostics_requests.json
requesting table details for system.public.statement_diagnostics... writing: debug/schema/system/public_statement_diagnostics.json
requesting table details for system.public.scheduled_jobs... writing: debug/schema/system/public_scheduled_jobs.json
requesting table details for system.public.sqlliveness... writing: debug/schema/system/public_sqlliveness.json
writing: debug/pprof-summary.sh
root@nvmedb1:/cockroachdb# echo $?
0
----

That seems to have worked.  I have not checked the zip file contents yet.

=== Analyzing rolling upgrade data

This testing was done on 11/11.  Timeline:

[source,text]
----
2020-11-10T16:40:09Z workload started
2020-11-10T23:13:24Z upgrade node 1 started (finished at 23:13:55Z)
2020-11-10T23:15:54Z upgrade node 2 started (finished at 23:17:54Z)*
2020-11-10T23:19:26Z upgrade node 3 started (finished at 23:21:38Z)*
----

I think it would be fair to look at just 23:00 to 23:30Z.  Filtering at the load generator side:

[source,text]
----
grep T23:[012] kv-histograms-159-2020-11-10T16\:40\:09Z.out > kv-histograms-159-trimmed.out
grep T23:[012] kv-histograms-43-2020-11-10T16\:40\:09Z.out > kv-histograms-43-trimmed.out
grep T23:[012] kv-histograms-52-2020-11-10T16\:40\:09Z.out> kv-histograms-52-trimmed.out
----

Note too that n1 = .52, n2 = .43, and n3 = .159.

I copied these files locally and did a similar analysis as for other client data sets, but I changed it because we're not so worried about overall throughput and latency but errors and latency, so I thought it would be helpful to plot the different clients' activities together.

I will plot:

* a graph of latency:
** series 1: pMax for client pointed at n1
** series 2: pMax for client pointed at n2
** series 3: pMax for client pointed at n3
* a graph of throughput:
** series 1: throughput for client pointed at n1
** series 2: throughput for client pointed at n2
** series 3: throughput for client pointed at n3
* a graph of errors (not sure how to produce this usefully yet)
** series 1: errors for client pointed at n1
** series 2: errors for client pointed at n2
** series 3: errors for client pointed at n3

[source,text]
----
dap@zathras cockroachdb-testing-round2 $ mkdir analysis/2020-11-11-rolling-upgrade
dap@zathras cockroachdb-testing-round2 $ cd !$
cd analysis/2020-11-11-rolling-upgrade
dap@zathras 2020-11-11-rolling-upgrade $ ln -s ../../data/2020-11-11-rolling-upgrade/client-data
dap@zathras 2020-11-11-rolling-upgrade $ ~/oxide/cockroachdb-testing/tools/chist/target/debug/chist print client-data/kv-histograms-52-trimmed.out > chist-n1.out
dap@zathras 2020-11-11-rolling-upgrade $ ~/oxide/cockroachdb-testing/tools/chist/target/debug/chist print client-data/kv-histograms-43-trimmed.out > chist-n2.out
dap@zathras 2020-11-11-rolling-upgrade $ ~/oxide/cockroachdb-testing/tools/chist/target/debug/chist print client-data/kv-histograms-159-trimmed.out > chist-n3.out
dap@zathras 2020-11-11-rolling-upgrade $ ls -l
total 1584
-rw-r--r--  1 dap  staff  266474 Nov 19 14:20 chist-n1.out
-rw-r--r--  1 dap  staff  266474 Nov 19 14:20 chist-n2.out
-rw-r--r--  1 dap  staff  266474 Nov 19 14:21 chist-n3.out
lrwxr-xr-x  1 dap  staff      49 Nov 19 14:16 client-data@ -> ../../data/2020-11-11-rolling-upgrade/client-data
dap@zathras 2020-11-11-rolling-upgrade $ awk 'NR == 1 || $2 == "write"' chist-n1.out > chist-n1-writes.out
dap@zathras 2020-11-11-rolling-upgrade $ awk 'NR == 1 || $2 == "read"' chist-n1.out > chist-n1-reads.out
dap@zathras 2020-11-11-rolling-upgrade $ awk 'NR == 1 || $2 == "write"' chist-n2.out > chist-n2-writes.out
dap@zathras 2020-11-11-rolling-upgrade $ awk 'NR == 1 || $2 == "write"' chist-n3.out > chist-n3-writes.out
dap@zathras 2020-11-11-rolling-upgrade $ awk 'NR == 1 || $2 == "read"' chist-n2.out > chist-n2-reads.out
dap@zathras 2020-11-11-rolling-upgrade $ awk 'NR == 1 || $2 == "read"' chist-n3.out > chist-n3-reads.out
dap@zathras 2020-11-11-rolling-upgrade $ join plot-n1-read-throughput.out plot-n1-write-throughput.out > plot-n1-throughput.out
dap@zathras 2020-11-11-rolling-upgrade $ join plot-n2-read-throughput.out plot-n2-write-throughput.out > plot-n2-throughput.out
dap@zathras 2020-11-11-rolling-upgrade $ join plot-n3-read-throughput.out plot-n3-write-throughput.out > plot-n3-throughput.out
dap@zathras 2020-11-11-rolling-upgrade $ awk '$1 != "TIME"{ print $1, $7 }' chist-n1-reads.out > plot-n1-read-p99.out
dap@zathras 2020-11-11-rolling-upgrade $ awk '$1 != "TIME"{ print $1, $7 }' chist-n2-reads.out > plot-n2-read-p99.out
dap@zathras 2020-11-11-rolling-upgrade $ awk '$1 != "TIME"{ print $1, $7 }' chist-n3-reads.out > plot-n3-read-p99.out
dap@zathras 2020-11-11-rolling-upgrade $ awk '$1 != "TIME"{ print $1, $7 }' chist-n3-writes.out > plot-n3-write-p99.out
dap@zathras 2020-11-11-rolling-upgrade $ awk '$1 != "TIME"{ print $1, $7 }' chist-n2-writes.out > plot-n2-write-p99.out
dap@zathras 2020-11-11-rolling-upgrade $ awk '$1 != "TIME"{ print $1, $7 }' chist-n1-writes.out > plot-n1-write-p99.out
dap@zathras 2020-11-11-rolling-upgrade $ awk '$1 != "TIME"{ print $1, $8 }' chist-n1-reads.out > plot-n1-read-pMax.out
dap@zathras 2020-11-11-rolling-upgrade $ awk '$1 != "TIME"{ print $1, $8 }' chist-n2-reads.out > plot-n2-read-pMax.out
dap@zathras 2020-11-11-rolling-upgrade $ awk '$1 != "TIME"{ print $1, $8 }' chist-n3-reads.out > plot-n3-read-pMax.out
dap@zathras 2020-11-11-rolling-upgrade $ awk '$1 != "TIME"{ print $1, $8 }' chist-n3-writes.out > plot-n3-write-pMax.out
dap@zathras 2020-11-11-rolling-upgrade $ awk '$1 != "TIME"{ print $1, $8 }' chist-n2-writes.out > plot-n2-write-pMax.out
dap@zathras 2020-11-11-rolling-upgrade $ awk '$1 != "TIME"{ print $1, $8 }' chist-n1-writes.out > plot-n1-write-pMax.out
dap@zathras 2020-11-11-rolling-upgrade $
----

IMPORTANT: I learned while writing this up that the bash prompt for nvmedb2 and nvmedb3 (and the AWS instance labels) are inconsistent with the CockroachDB node names -- they're backwards (n3 and n2, respectively).  What a pain.

== 2020-11-20

=== "debug zip"

I'm poking through the debug zip that I created the other day.  There's good stuff in here.  I see:

* settings.json: list of all cluster config settings
* events.json: appears to be an event log (e.g., "decommissioning")
* rangelog.json: appears to be a log of changes to ranges
* nodes.json: includes detailed information about each node, including metrics, uptime, buildinformation, etc.
* running queries of various kinds, including internal
* list of replication zones
* for each node: lots of internal details, including stacks.txt, command-line arguments, environment variables, log files
* for each node, for each range: information about the range, including byte counts, replicas, etc.
* schema details
* report on "problem ranges"

Data missing:

* nodes/*/threads.txt: "thread stacks only available on Linux/Glibc"
(stacks.txt is populated)

This is the only file I've found that contains a string like this.  I'm not sure about the pprof files below though.

Not sure:

* cpu.pprof, heap.pprof: not sure what file format this is
* there are a bunch of "memprof" files for each node with different timestamps a few minutes apart


=== Schema upgrade

https://www.cockroachlabs.com/docs/stable/online-schema-changes.html[Docs] are here.

Ideas:

* create a new default NULL column
* rename the column
* drop the column
* create a new column with a non-default value

Initial state:

[source,text]
----
root@192.168.1.163:26257/kv> \d kv;
  column_name | data_type | is_nullable | column_default | generation_expression |  indices  | is_hidden
--------------+-----------+-------------+----------------+-----------------------+-----------+------------
  k           | INT8      |    false    | NULL           |                       | {primary} |   false
  v           | BYTES     |    false    | NULL           |                       | {}        |   false
(2 rows)
----

So maybe:

[source,text]
----
ALTER TABLE kv ADD COLUMN d1 INT8 DEFAULT NULL;
ALTER TABLE kv ADD COLUMN d2 INT8 NOT NULL DEFAULT 3;
ALTER TABLE kv RENAME COLUMN d2 TO d3;
ALTER TABLE kv DROP COLUMN d1;
ALTER TABLE kv DROP COLUMN d3;
----

First, I'm going to start a light workload like I've been doing:

[source,text]
----
date="$(date +%FT%TZ)"
for node in 52 43 159; do
	nohup cockroach workload run kv --histograms kv-histograms-$node-$date.out --concurrency 4 --max-rate=333 --display-every=1s --read-percent 80 --tolerate-errors postgresql://root@192.168.1.$node:26257/kv?sslmode=disable > loadgen-summary-$node-$date.out 2>&1 &
done
----

Workload started at 19:26:23Z.

[source,text]
----
root@192.168.1.163:26257/kv> SELECT NOW(); ALTER TABLE kv ADD COLUMN d1 INT8 DEFAULT NULL; SELECT NOW();
                now
------------------------------------
  2020-11-20 19:56:09.510504+00:00
(1 row)

Note: timings for multiple statements on a single line are not supported. See https://go.crdb.dev/issue-v/48180/v20.2.

                now
------------------------------------
  2020-11-20 19:56:12.894271+00:00
(1 row)

Note: timings for multiple statements on a single line are not supported. See https://go.crdb.dev/issue-v/48180/v20.2.

root@192.168.1.163:26257/kv> SELECT NOW();  ALTER TABLE kv ADD COLUMN d2 INT8 NOT NULL DEFAULT 3;  SELECT NOW();
ERROR: driver: bad connection
warning: connection lost!
opening new connection: all session settings will be lost
root@192.168.1.163:26257/kv> 
----

add d1: started 19:56:09Z, job succeeded in 2 seconds.
add d2: started 19:58:23Z, was "Waiting in DELETE-ONLY" for a while.  SQL session bombed out.  Switched to "Populating schema", which looks like a potentially very long-running backfill operation.  By 20:02Z, it reported 2.7% complete with "02:12:08 remaining" (presumably 2 hours 12 minutes)

I wonder if the decommissioned-but-not-yet-removed nodes n4 and n5 are making this slow.

Note: at this time, the "kv" database is 273.2 GiB in size with 260 total ranges.  I think that's smaller than before.

This job eventually completed:

[source,text]
----
root@192.168.1.163:26257/defaultdb> show jobs;
        job_id       |   job_type    |                          description                           | statement | user_name |  status   | running_status |             created              |             started              |             finished             |             modified             | fraction_completed | error | coordinator_id
---------------------+---------------+----------------------------------------------------------------+-----------+-----------+-----------+----------------+----------------------------------+----------------------------------+----------------------------------+----------------------------------+--------------------+-------+-----------------
  608933977813516289 | SCHEMA CHANGE | ALTER TABLE kv.public.kv ADD COLUMN d2 INT8 NOT NULL DEFAULT 3 |           | root      | succeeded | NULL           | 2020-11-20 19:58:22.396793+00:00 | 2020-11-20 19:58:23.262529+00:00 | 2020-11-20 21:04:02.599603+00:00 | 2020-11-20 21:04:02.58695+00:00  |                  1 |       |           NULL
  608933542348455937 | SCHEMA CHANGE | ALTER TABLE kv.public.kv ADD COLUMN d1 INT8 DEFAULT NULL       |           | root      | succeeded | NULL           | 2020-11-20 19:56:09.510659+00:00 | 2020-11-20 19:56:09.844324+00:00 | 2020-11-20 19:56:12.018731+00:00 | 2020-11-20 19:56:11.997991+00:00 |                  1 |       |           NULL
(2 rows)

Time: 9ms total (execution 8ms / network 1ms)
----

It looks like it took about 1h6m.

Latency was very badly affected, though.

I'm going to try a few more items:

[source,text]
----
root@192.168.1.163:26257/kv> ALTER TABLE kv RENAME COLUMN d2 TO d3; SELECT NOW();
                now
------------------------------------
  2020-11-20 22:03:05.527899+00:00
(1 row)

Note: timings for multiple statements on a single line are not supported. See https://go.crdb.dev/issue-v/48180/v20.2.

root@192.168.1.163:26257/kv> show jobs;
        job_id       |   job_type    |                          description                           | statement | user_name |  status   | running_status |             created              |             started              |             finished             |             modified             | fraction_completed | error | coordinator_id
---------------------+---------------+----------------------------------------------------------------+-----------+-----------+-----------+----------------+----------------------------------+----------------------------------+----------------------------------+----------------------------------+--------------------+-------+-----------------
  608958498022752257 | SCHEMA CHANGE | ALTER TABLE kv.public.kv RENAME COLUMN d2 TO d3                |           | root      | succeeded | NULL           | 2020-11-20 22:03:05.380101+00:00 | 2020-11-20 22:03:05.456246+00:00 | 2020-11-20 22:03:05.486898+00:00 | 2020-11-20 22:03:05.477285+00:00 |                  1 |       |           NULL
  608933977813516289 | SCHEMA CHANGE | ALTER TABLE kv.public.kv ADD COLUMN d2 INT8 NOT NULL DEFAULT 3 |           | root      | succeeded | NULL           | 2020-11-20 19:58:22.396793+00:00 | 2020-11-20 19:58:23.262529+00:00 | 2020-11-20 21:04:02.599603+00:00 | 2020-11-20 21:04:02.58695+00:00  |                  1 |       |           NULL
  608933542348455937 | SCHEMA CHANGE | ALTER TABLE kv.public.kv ADD COLUMN d1 INT8 DEFAULT NULL       |           | root      | succeeded | NULL           | 2020-11-20 19:56:09.510659+00:00 | 2020-11-20 19:56:09.844324+00:00 | 2020-11-20 19:56:12.018731+00:00 | 2020-11-20 19:56:11.997991+00:00 |                  1 |       |           NULL
(3 rows)

Time: 9ms total (execution 8ms / network 1ms)
----

That completed very quickly.  Now I'm going to drop "d1", which I expect should be fast.  I was wrong about that.  It looks like it's going to take a while.

[source,text]
----
root@192.168.1.163:26257/kv> ALTER TABLE kv DROP COLUMN d1;
ERROR: rejected (sql_safe_updates = true): ALTER TABLE DROP COLUMN will remove all data in that column
SQLSTATE: 01000
root@192.168.1.163:26257/kv> set sql_safe_updates = false;
invalid syntax: statement ignored: at or near "safe_updates": syntax error
SQLSTATE: 42601
DETAIL: source SQL:
safe_updates = false
^
root@192.168.1.163:26257/kv> set sql_safe_updates = false;
SET

Time: 0ms total (execution 0ms / network 0ms)

root@192.168.1.163:26257/kv> select NOW(); ALTER TABLE kv DROP COLUMN d1;
ERROR: driver: bad connection
warning: connection lost!
opening new connection: all session settings will be lost
root@192.168.1.163:26257/kv> 
----

This ultimately took 58m57s.

At this point, it looks like this:

[source,text]
----
root@192.168.1.163:26257/kv> \d kv;
  column_name | data_type | is_nullable | column_default | generation_expression |  indices  | is_hidden
--------------+-----------+-------------+----------------+-----------------------+-----------+------------
  k           | INT8      |    false    | NULL           |                       | {primary} |   false
  v           | BYTES     |    false    | NULL           |                       | {}        |   false
  d3          | INT8      |    false    | 3:::INT8       |                       | {}        |   false
(3 rows)
----

Finally, I'm going to try the last one, which I also expect to take a while:

[source,text]
----
root@192.168.1.163:26257/kv> ALTER TABLE kv DROP COLUMN d3;
ERROR: driver: bad connection
warning: connection lost!
opening new connection: all session settings will be lost
root@192.168.1.163:26257/kv> 
----

There are lots of open issues around the latency impact:

* https://github.com/cockroachdb/cockroach/issues/36850
* https://github.com/cockroachdb/cockroach/issues/34868
* https://github.com/cockroachdb/cockroach/issues/47989
* https://github.com/cockroachdb/cockroach/issues/36430
* https://github.com/cockroachdb/cockroach/issues/47215

This finally finished around 00:57:49Z (91m total).

Final output:

[source,text]
----
root@192.168.1.163:26257/kv> show jobs;
        job_id       |   job_type    |                          description                           | statement | user_name |  status   | running_status |             created              |             started              |             finished             |             modified             | fraction_completed | error | coordinator_id
---------------------+---------------+----------------------------------------------------------------+-----------+-----------+-----------+----------------+----------------------------------+----------------------------------+----------------------------------+----------------------------------+--------------------+-------+-----------------
  608974938327416833 | SCHEMA CHANGE | ALTER TABLE kv.public.kv DROP COLUMN d3                        |           | root      | succeeded | NULL           | 2020-11-20 23:26:42.550792+00:00 | 2020-11-20 23:26:42.637955+00:00 | 2020-11-21 00:57:49.484005+00:00 | 2020-11-21 00:57:49.473637+00:00 |                  1 |       |           NULL
  608958717186801665 | SCHEMA CHANGE | ALTER TABLE kv.public.kv DROP COLUMN d1                        |           | root      | succeeded | NULL           | 2020-11-20 22:04:12.247699+00:00 | 2020-11-20 22:04:12.336029+00:00 | 2020-11-20 23:03:09.523536+00:00 | 2020-11-20 23:03:09.513231+00:00 |                  1 |       |           NULL
  608958498022752257 | SCHEMA CHANGE | ALTER TABLE kv.public.kv RENAME COLUMN d2 TO d3                |           | root      | succeeded | NULL           | 2020-11-20 22:03:05.380101+00:00 | 2020-11-20 22:03:05.456246+00:00 | 2020-11-20 22:03:05.486898+00:00 | 2020-11-20 22:03:05.477285+00:00 |                  1 |       |           NULL
  608933977813516289 | SCHEMA CHANGE | ALTER TABLE kv.public.kv ADD COLUMN d2 INT8 NOT NULL DEFAULT 3 |           | root      | succeeded | NULL           | 2020-11-20 19:58:22.396793+00:00 | 2020-11-20 19:58:23.262529+00:00 | 2020-11-20 21:04:02.599603+00:00 | 2020-11-20 21:04:02.58695+00:00  |                  1 |       |           NULL
  608933542348455937 | SCHEMA CHANGE | ALTER TABLE kv.public.kv ADD COLUMN d1 INT8 DEFAULT NULL       |           | root      | succeeded | NULL           | 2020-11-20 19:56:09.510659+00:00 | 2020-11-20 19:56:09.844324+00:00 | 2020-11-20 19:56:12.018731+00:00 | 2020-11-20 19:56:11.997991+00:00 |                  1 |       |           NULL
(5 rows)

Time: 10ms total (execution 9ms / network 1ms)

root@192.168.1.163:26257/kv> \d kv
  column_name | data_type | is_nullable | column_default | generation_expression |  indices  | is_hidden
--------------+-----------+-------------+----------------+-----------------------+-----------+------------
  k           | INT8      |    false    | NULL           |                       | {primary} |   false
  v           | BYTES     |    false    | NULL           |                       | {}        |   false
(2 rows)

Time: 44ms total (execution 44ms / network 1ms)

root@192.168.1.163:26257/kv> select NOW();
                now
-----------------------------------
  2020-11-21 01:10:41.32235+00:00
(1 row)

Time: 1ms total (execution 0ms / network 0ms)

----

Shut down load generators at 2020-11-21T01:11Z

== 2020-11-23

=== Update proto area

I need to update my raw proto tarball for the changes I made a few weeks ago.  First, I'm going to compare the old one to the current contents of the "vminit" directory.

[source,text]
----
dap@zathras cockroachdb-testing-round2 $ mkdir old_proto
dap@zathras cockroachdb-testing-round2 $ cd old_proto
dap@zathras old_proto $ tar xzf ../proto-raw.tgz
dap@zathras old_proto $ pwd
/Users/dap/oxide/cockroachdb-testing-round2/old_proto
dap@zathras old_proto $ ls
total 0
drwxr-xr-x  6 dap  staff  192 Nov 23 13:42 vminit/
dap@zathras old_proto $ ls vminit/
total 13992
drwxr-xr-x  4 dap  staff      128 Nov 23 13:42 cockroachdb/
drwxr-xr-x  3 dap  staff       96 Nov 23 13:42 common/
-rwxr-xr-x  1 dap  staff  7160924 Sep  2 15:37 fetcher.gz*
drwxr-xr-x  3 dap  staff       96 Nov 23 13:42 mon/
dap@zathras old_proto $ ls ../vminit/
total 567248
-rw-r--r--  1 dap  staff        357 Oct 28 15:25 GNUmakefile
drwxr-xr-x  4 dap  staff        128 Oct 28 15:26 cockroachdb/
drwxr-xr-x  3 dap  staff         96 Oct 28 15:25 common/
-rwxr-xr-x  1 dap  staff    7160924 Sep  2 15:37 fetcher.gz*
drwxr-xr-x  3 dap  staff         96 Oct 28 15:25 mon/
-rw-r--r--  1 dap  staff  180481661 Nov  9 16:45 vminit-cockroachdb.tgz
-rw-r--r--  1 dap  staff   13412672 Oct 28 15:32 vminit-common.tgz
-rw-r--r--  1 dap  staff   84643107 Oct 28 15:33 vminit-mon.tgz
-rw-r--r--  1 dap  staff       5269 Oct 28 15:25 vminit.sh
dap@zathras old_proto $ 
cd ..
dap@zathras cockroachdb-testing-round2 $ diff -r vminit/ old_proto/vminit > proto_diff.out
Only in vminit/: GNUmakefile
Binary files vminit/cockroachdb/cockroachdb/bin/cockroach and old_proto/vminit/cockroachdb/cockroachdb/bin/cockroach differ
Only in vminit/cockroachdb/cockroachdb/bin: cockroach-v20.2.0
Only in vminit/cockroachdb/cockroachdb/bin: cockroach-v20.2.0-rc.3
Only in vminit/cockroachdb/cockroachdb/bin: cockroach-v20.2.0-rc.4
Only in vminit/cockroachdb/cockroachdb/bin: configure_cluster
Only in vminit/cockroachdb/cockroachdb/bin: configure_haproxy
Only in vminit/cockroachdb/cockroachdb: etc
Binary files vminit/cockroachdb/cockroachdb/lib/libgcc_s.so.1 and old_proto/vminit/cockroachdb/cockroachdb/lib/libgcc_s.so.1 differ
Binary files vminit/cockroachdb/cockroachdb/lib/libstdc++.so.6 and old_proto/vminit/cockroachdb/cockroachdb/lib/libstdc++.so.6 differ
Only in vminit/cockroachdb/cockroachdb: smf
Only in vminit/common/opt/oxide: etc
Only in vminit/common/opt/oxide: smf
Only in vminit/mon/mon/grafana: grafana.ini
Only in vminit/mon/mon/grafana: provisioning
Only in vminit/mon/mon/grafana: stock_dashboards
Only in vminit/mon/mon/prometheus: prometheus.yml
Only in vminit/mon/mon/prometheus: rules
Only in vminit/mon/mon: smf
Only in vminit/: vminit-cockroachdb.tgz
Only in vminit/: vminit-common.tgz
Only in vminit/: vminit-mon.tgz
Only in vminit/: vminit.sh
----

Files to be added or updated:

[source,text]
----
vminit/cockroachdb/cockroachdb/bin/cockroach
vminit/cockroachdb/cockroachdb/bin/cockroach-v20.2.0
vminit/cockroachdb/cockroachdb/bin/cockroach-v20.2.0-rc.3
vminit/cockroachdb/cockroachdb/bin/cockroach-v20.2.0-rc.4
vminit/cockroachdb/cockroachdb/lib/libgcc_s.so.1
vminit/cockroachdb/cockroachdb/lib/libstdc++.so.6
----

The rest are in Git in this repo.

This exactly matches what I'd expect.  So now to create the _new_ proto tarball, I'm going to start with the list of files from the _old_ one, add the six files above, sort and dedup, and use that list of files.

[source,text]
----
$ tar tzf proto-raw.tgz > proto_files.txt
$ vim proto_files.txt 
$ wc -l proto_files.txt 
    2812 proto_files.txt
$ sort -u proto_files.txt > proto_files_sorted.txt
$ wc -l proto_files_sorted.txt 
    2809 proto_files_sorted.txt
$ mv proto_files_sorted.txt proto_files.txt 
overwrite proto_files.txt? (y/n [n]) y
$ tar czf proto-raw-new.tgz -n -T proto_files.txt 
$
----

Now, let's compare them:

[source,text]
----
dap@zathras cockroachdb-testing-round2 $ tar tzf proto-raw.tgz > proto_files_1.txt
dap@zathras cockroachdb-testing-round2 $ tar tzf proto-raw-new.tgz > proto_files_2.txt
dap@zathras cockroachdb-testing-round2 $ wc -l proto_files_{1,2}.txt
    2806 proto_files_1.txt
    2809 proto_files_2.txt
    5615 total
dap@zathras cockroachdb-testing-round2 $ sort proto_files_1.txt > proto_files_1_sorted.txt
dap@zathras cockroachdb-testing-round2 $ sort proto_files_2.txt > proto_files_2_sorted.txt
dap@zathras cockroachdb-testing-round2 $ diff proto_files_{1,2}_sorted.txt
1a2,4
> vminit/cockroachdb/cockroachdb/bin/cockroach-v20.2.0
> vminit/cockroachdb/cockroachdb/bin/cockroach-v20.2.0-rc.3
> vminit/cockroachdb/cockroachdb/bin/cockroach-v20.2.0-rc.4
dap@zathras cockroachdb-testing-round2 $ 
----

So far, so good.  Let's be really sure.  I'll create a new clone of this repo, unpack the new proto tarball, and recursively diff its "vminit" directory tree with this clone's:

[source,text]
----
dap@zathras cockroachdb-testing-round2 $ cd ../
dap@zathras oxide $ git clone cockroachdb-testing-round2/ cockroachdb-testing-new-proto
Cloning into 'cockroachdb-testing-new-proto'...
done.
Updating files: 100% (399/399), done.
dap@zathras oxide $ cd !$
cd cockroachdb-testing-new-proto
dap@zathras cockroachdb-testing-new-proto $ tar xzf ../cockroachdb-testing-round2/proto-raw-new.tgz
dap@zathras cockroachdb-testing-new-proto $ diff -r vminit/ ../cockroachdb-testing-round2/vminit
Only in ../cockroachdb-testing-round2/vminit: vminit-cockroachdb.tgz
Only in ../cockroachdb-testing-round2/vminit: vminit-common.tgz
Only in ../cockroachdb-testing-round2/vminit: vminit-mon.tgz
dap@zathras cockroachdb-testing-new-proto $
----

Perfect!  Now, to finalize the changes.  In the S3 bucket, I'm going to copy the current proto-raw.tgz to an archived version based on its current mtime, which is from 2020-10-16.  Then I'm going to copy my new tarball to an archived copy based on today's date.  Then I'll copy that to "proto-raw.tgz" to represent the current one.

[source,text]
----
dap@zathras cockroachdb-testing-round2 $ aws s3 ls oxide-cockroachdb-exploration
                           PRE data/
2020-11-13 15:39:43 87348554912 cockroach-dump-kv-2020-11-11.sql
2020-08-27 15:28:39   87156784 cockroachdb.tar.gz
2020-10-16 16:39:48  198929508 proto-raw.tgz
2020-09-14 16:00:45   93765576 vminit-cockroachdb.tgz
2020-09-10 16:09:46   13412713 vminit-common.tgz
2020-09-08 16:10:44   84117103 vminit-mon.tgz
dap@zathras cockroachdb-testing-round2 $ aws s3 cp s3://oxide-cockroachdb-exploration/proto-raw.tgz s3://oxide-cockroachdb-exploration/proto-raw-2020-10-16.tgz
copy: s3://oxide-cockroachdb-exploration/proto-raw.tgz to s3://oxide-cockroachdb-exploration/proto-raw-2020-10-16.tgz
dap@zathras cockroachdb-testing-round2 $ aws s3 cp proto-raw-new.tgz s3://oxide-cockroachdb-exploration/proto-raw-2020-11-23.tgz
upload: ./proto-raw-new.tgz to s3://oxide-cockroachdb-exploration/proto-raw-2020-11-23.tgz
dap@zathras cockroachdb-testing-round2 $ aws s3 ls oxide-cockroachdb-exploration
                           PRE data/
2020-11-13 15:39:43 87348554912 cockroach-dump-kv-2020-11-11.sql
2020-08-27 15:28:39   87156784 cockroachdb.tar.gz
2020-11-23 14:03:34  198929508 proto-raw-2020-10-16.tgz
2020-11-23 14:04:01  369013041 proto-raw-2020-11-23.tgz
2020-10-16 16:39:48  198929508 proto-raw.tgz
2020-09-14 16:00:45   93765576 vminit-cockroachdb.tgz
2020-09-10 16:09:46   13412713 vminit-common.tgz
2020-09-08 16:10:44   84117103 vminit-mon.tgz
dap@zathras cockroachdb-testing-round2 $ aws s3 cp s3://oxide-cockroachdb-exploration/proto-raw-2020-11-23.tgz s3://oxide-cockroachdb-exploration/proto-raw.tgz
copy: s3://oxide-cockroachdb-exploration/proto-raw-2020-11-23.tgz to s3://oxide-cockroachdb-exploration/proto-raw.tgz
dap@zathras cockroachdb-testing-round2 $ aws s3 ls oxide-cockroachdb-exploration
                           PRE data/
2020-11-13 15:39:43 87348554912 cockroach-dump-kv-2020-11-11.sql
2020-08-27 15:28:39   87156784 cockroachdb.tar.gz
2020-11-23 14:03:34  198929508 proto-raw-2020-10-16.tgz
2020-11-23 14:04:01  369013041 proto-raw-2020-11-23.tgz
2020-11-23 14:52:20  369013041 proto-raw.tgz
2020-09-14 16:00:45   93765576 vminit-cockroachdb.tgz
2020-09-10 16:09:46   13412713 vminit-common.tgz
2020-09-08 16:10:44   84117103 vminit-mon.tgz
----




=== Summarizing data from schema changes

Timeline (from above):

[source,text]
----
CREATED              FINISHED             ELAPSED   SQL
2020-11-20T19:56:09Z 2020-11-20T19:56:12Z        3s ALTER TABLE kv.public.kv ADD COLUMN d1 INT8 DEFAULT NULL
2020-11-20T19:58:22Z 2020-11-20T21:04:02Z  1h05m40s ALTER TABLE kv.public.kv ADD COLUMN d2 INT8 NOT NULL DEFAULT 3
2020-11-20T22:03:05Z 2020-11-20T22:03:05Z        0s ALTER TABLE kv.public.kv RENAME COLUMN d2 TO d3
2020-11-20T22:04:12Z 2020-11-20T23:03:09Z    58m57s ALTER TABLE kv.public.kv DROP COLUMN d1
2020-11-20T23:26:42Z 2020-11-21T00:57:49Z  1h31m07s ALTER TABLE kv.public.kv DROP COLUMN d3
----

The graphs that I want for this are probably similar to the graphs for the expansion/contraction events rather than, say, rolling upgrade.  I don't need data broken out by client.

I did the usual setup of directory structure and sequence of `chist` and `awk` commands, based on the sequence from the 2020-11-18-nvme-expansion-contraction analysis.  I gzip'd the data files before adding them to Git because they're a bit large.

=== Testing a restore

I tested a restore using the database dump I created earlier with `cockroach backup`:

[source,text]
----
cockroachdb@nvmedb1:~$ /cockroachdb/bin/cockroach sql --insecure --host 192.168.1.52 -e 'CREATE DATABASE newkv';
CREATE DATABASE

Time: 23ms

cockroachdb@nvmedb1:~$ cd /cockroachdb/backups/
cockroachdb@nvmedb1:/cockroachdb/backups$  time /cockroachdb/bin/cockroach sql --insecure --host 192.168.1.52 --database newkv < cockroach-dump.out > restore.out 2>restore.err &
[1] 2903

----

I'm waiting for this to complete.  Recall that the backup is 87348554912 bytes (81 GiB) and contains an estimated 43M records (43762680, to be precise).

It's going pretty slow: 8 minutes in and we're about 2% done.  The output file is going to be about 43762680/100+1 lines long, or 437628 lines long.  In 568s (9m28s), it was 11966 lines long, or 2.7%.  That suggests a total time of 21037 seconds, or 5h51m, for an end time of 7:17PM PT.

There seems to be plenty of resources available -- this is not very efficient.

At the end, we'll want to `select count(*) from kv` in the "newkv" database and confirm that it matches 43762680.

=== TODO next

* update testing details and RFD 110 for the test restore that's in progress
