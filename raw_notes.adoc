// Include a Table of Contents on the left hand side.
:toc: left
// ":icons: font" is needed for adminition and callout icons.
:icons: font

= Raw notes

== Up to 2020-08-25

* basic research: AWS, Terraform, CockroachDB deployment, OmniOS
** find / build a simple load generator. (will use "cockroach workload" for now)
** calculate how much it will cost to run a small cluster on AWS for a while.
*** three instances
*** Prometheus
*** Grafana
*** 1-3 load generators
* Successfully used Terraform to provision a 3-node CockroachDB cluster on AWS, using OmniOS and https://sysmgr.org/~jclulow/tmp/cockroach.tar.gz[Joshua's CRDB binaries].  You need to manually run `cockroach init --insecure --host ...` on one of them to get the cluster to come up.

== 2020-08-26

* fixed bugs in Terraform config
** cockroachdb SMF service was disabled on reboot (was using `svcadm enable -t`)
** `terraform apply` could fail if the VPC subnet wound up in us-west-2d because our instance types aren't supported there
** it would be convenient if the instance names didn't have spaces
** it would be convenient if there were a single tag for all of our instances so
we could select them without relying on my specific key
* successful cold start
* lots of NTP issues: see GitHub issue #1.  These appear to be mitigated.

== 2020-08-27

Summary of the day:

* Ran into a lot of issues with NTP.  Installed Chrony.  The issues appear
  resolved.
* Got workloads running.  Exercised a bunch of the options for duration, ramp-up time, percent reads, etc.

Details follow.

* Three databases, 1 load generator.  Each load generator can only be pointed at one database, so this shouldn't be too heavy for the whole cluster, but let's see what happens.
* I'm going to start with the "kv" worklaod.

 /cockroachdb/bin/cockroach workload init kv postgres://root@192.168.1.152:26257?sslmode=disable
/cockroachdb/bin/cockroach workload run kv --histograms kv-histograms-$(date +%FT%TZ).out postgres://root@192.168.1.152:26257?sslmode=disable

Things to play with:

--ramp
--max-rate
--max-ops
--read-percent
--tolerate-errors

I let that run for about 25-30 minutes.  End of the run:

[source,text]
----
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
 1661.0s        0          830.6          802.5      4.7      6.3      9.4     24.1 write
 1662.0s        0          827.3          802.5      4.7      6.3      9.4     16.8 write
 1663.0s        0          820.8          802.5      4.7      6.6     12.1     17.8 write
 1664.0s        0          808.1          802.5      4.7      6.6     11.5     16.8 write
 1665.0s        0          789.3          802.5      5.0      7.1      9.4     16.3 write
 1666.0s        0          764.4          802.5      5.0      7.3     11.0     16.8 write
 1667.0s        0          806.0          802.5      5.0      6.8      8.9     15.7 write
 1668.0s        0          803.0          802.5      4.7      6.6     11.0     23.1 write
 1669.0s        0          787.9          802.5      5.0      6.8      8.4     18.9 write
 1670.0s        0          809.2          802.5      5.0      6.8      9.4     12.1 write
 1671.0s        0          799.8          802.5      5.0      7.1      9.4     15.7 write
 1672.0s        0          838.8          802.5      4.7      6.3     11.0     19.9 write
 1673.0s        0          840.4          802.5      4.5      6.3     11.0     16.3 write
 1674.0s        0          806.9          802.5      4.7      7.3      9.4     14.7 write
^CHighest sequence written: 1343922. Can be passed as --write-seq=R1343922 to the next run.

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
 1674.6s        0        1343922          802.5      5.0      4.7      6.8     11.5     65.0  write

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__result
 1674.6s        0        1343922          802.5      5.0      4.7      6.8     11.5     65.0
----

This created kv-histograms-2020-08-27T17:29:16Z.out.

I'm going to try it again for a few minutes to see if the initial spike in latency is one-time or not.

[source,text]
----
$ /cockroachdb/bin/cockroach workload run kv --histograms kv-histograms-$(date +%FT%TZ).out --duration 5m postgres://root@192.168.1.152:26257?sslmode=disable 

...

Highest sequence written: 239288. Can be passed as --write-seq=R239288 to the next run.

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
  300.0s        0         239284          797.6      5.0      4.7      6.8     12.6    125.8  write

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__result
  300.0s        0         239284          797.6      5.0      4.7      6.8     12.6    125.8
----

This created kv-histograms-2020-08-27T17:59:04Z.out.

The latency spike up front happened again.

Let's try out the --max-rate option to place a cap at 500 operations.  (I accidentally used --max-ops first, which exited quickly!)

cockroachdb@ip-192-168-1-192:~$ /cockroachdb/bin/cockroach workload run kv --histograms kv-histograms-$(date +%FT%TZ).out --max-rate 500 postgres://root@192.168.1.152:26257?sslmode=disable 

That seemed to work reasonably well.  There are a ton of metrics in the Admin UI dashboard!

[source,text]
----
_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
  362.6s        0         178192          491.4      4.0      3.7      5.8     12.1     88.1  write

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__result
  362.6s        0         178192          491.4      4.0      3.7      5.8     12.1     88.1  
----

This created kv-histograms-2020-08-27T18:08:37Z.out.

Let's try `--ramp`.  I used 30s first, but that's too fast to really see the effect.  I'm going to try this again with 5m.

[source,text]
----
cockroachdb@ip-192-168-1-192:~$ /cockroachdb/bin/cockroach workload run kv --histograms kv-histograms-$(date +%FT%TZ).out --ramp=5m --max-rate 700 postgres://root@192.168.1.152:26257?sslmode=disable 
...
^CHighest sequence written: 588373. Can be passed as --write-seq=R588373 to the next run.

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
  655.4s        0         435950          665.2      4.7      4.5      6.6     11.5     56.6  write

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__result
  655.4s        0         435950          665.2      4.7      4.5      6.6     11.5     56.6
----

This created kv-histograms-2020-08-27T18:18:53Z.out.  That seemed to do what I expected -- ramped up over several minutes and capped around 700.

The histogram file looks to be per-second histograms.

I want to throw some reads into the mix, but one of the nodes has become "suspect" because its clock is too far off.  I'm starting to get:

[source,text]
----
W200827 18:41:29.280504 1064 kv/kvserver/replica_range_lease.go:555  [n2,s2,r10/
3:/Table/1{4-5}] can't determine lease status of (n2,s2):3 due to node liveness
error: node not in the liveness table
(1) attached stack trace
  | github.com/cockroachdb/cockroach/pkg/kv/kvserver.init
  |     /ws/cockroach/gopath/src/github.com/cockroachdb/cockroach/pkg/kv/kvserve
r/node_liveness.go:44
  | runtime.doInit
  |     /opt/go/1.14.4/src/runtime/proc.go:5420
  | runtime.doInit
  |     /opt/go/1.14.4/src/runtime/proc.go:5415
  | runtime.doInit
  |     /opt/go/1.14.4/src/runtime/proc.go:5415
  | runtime.doInit
  |     /opt/go/1.14.4/src/runtime/proc.go:5415
  | runtime.doInit
  |     /opt/go/1.14.4/src/runtime/proc.go:5415
  | runtime.main
  |     /opt/go/1.14.4/src/runtime/proc.go:190
  | runtime.goexit
  |     /opt/go/1.14.4/src/runtime/asm_amd64.s:1373
----

Two of them have gone into maintenance now.

Several hours later: I've built and deployed chrony to these boxes to see if
this goes better.  Let's go ahead and run that mixed workload I wanted to do
next.

[source,text]
----
$ /cockroachdb/bin/cockroach workload run kv --histograms kv-histograms-$(date +%FT%TZ).out --read-percent=30 --ramp=5m postgres://root@192.168.1.152:26257?sslmode=disable 
...
^CNumber of reads that didn't return any results: 2.
Highest sequence written: 2550079. Can be passed as --write-seq=R2550079 to the next run.

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
 3465.5s        0        1028361          296.7      2.0      1.9      3.0      5.0     67.1  read

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
 3465.5s        0        2395944          691.4      4.9      4.7      6.8     11.0    201.3  write

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__result
 3465.5s        0        3424305          988.1      4.0      4.5      6.6     10.0    201.3
----

I let this run for about an hour.  This created kv-histograms-2020-08-27T22:54:19Z.out.  Note that this file has two lines per second -- one for reads and ones for writes.

The clocks are consistently within 1ms of each other now (!).  This workload is running well.

At about 2020-08-27T23:16Z, I activated statement diagnostics for the UPSERT that this thing is running to see what it does.  This produced a bundle that was 23 bytes (0 bytes downloaded, for some reason).  This looks like this bug fixed in https://www.cockroachlabs.com/docs/releases/v20.2.0-alpha.3.html[v20.2.0-alpha.3]:

> Fixed a bug causing the raw trace file collected inside a statement diagnostics bundle to be sometimes empty when the cluster setting sql.trace.txn.enable_threshold was in use. #50914

although in our case `sql.trace.txn.enable_threshold` is 0 (disabled).  Maybe not the same issue.

== 2020-08-31

Went through:

* https://www.cockroachlabs.com/docs/v20.1/learn-cockroachdb-sql.html[Learn CockroachDB SQL] (this was just basic SQL)
** https://www.cockroachlabs.com/docs/v20.1/developer-guide-overview.html[Developer Guide]
** Skipped exercises under https://www.cockroachlabs.com/docs/v20.1/deploy-a-test-cluster.html[Test deployment] -- these were too basic or exercised K8s behavior.
** Skimmed the https://www.cockroachlabs.com/docs/v20.1/performance.html[Performance Guide]
** https://www.cockroachlabs.com/docs/v20.1/monitoring-and-alerting.html[Prometheus stuff]
** Skimmed https://www.cockroachlabs.com/docs/v20.1/configure-replication-zones.html[Replication Zones]
** https://www.cockroachlabs.com/docs/v20.1/manage-long-running-queries.html[Long-running queries]
** Read through https://www.cockroachlabs.com/docs/v20.1/remove-nodes.html[Decommision nodes]
** Read through https://www.cockroachlabs.com/docs/v20.1/disaster-recovery.html[disaster recovery]
** Skimmed through https://www.cockroachlabs.com/docs/v20.1/troubleshooting-overview.html[Troubleshooting section]

Exercised replication + rebalancing tutorial:

* Started with a cluster with 65 ranges: internal data + some poking around with the "movr" dataset.
* That's 65 ranges with replication factor 3 divided across 3 nodes = 65 replicas per node (confirmed).
* Started a fourth node: expect ~48 replicas per node (65 ranges times replication factor 3 divided by 4 nodes)
* Final state: between 46 - 50 replicas per node.  Stopped slightly before I expected, but well within reasonable.

Now I want to decommission that fourth node.

```
/cockroachdb/bin/cockroach node decommission 4 --insecure --host 192.168.1.46
...
  id | is_live | replicas | is_decommissioning |   membership   | is_draining
-----+---------+----------+--------------------+----------------+--------------
   4 |  true   |        0 |        true        | decommissioned |    false
(1 row)

No more data reported on target nodes. Please verify cluster health before removing the nodes.
```

For good measure, I drained it before disabling it:

```
root@ip-192-168-1-46:~# /cockroachdb/bin/cockroach node drain --insecure --host 192.168.1.46
node is draining... remaining: 1
node is draining... remaining: 0 (complete)
ok
root@ip-192-168-1-46:~# svcadm disable -s cockroachdb
root@ip-192-168-1-46:~#
```

Then I removed it with Terraform.  (Fortunately, just decrementing the count of db nodes caused Terraform to want to destroy this one and not some other one.)

After a few minutes, the UI reports the node as decommissioned.

---

I'm now switching over to fleshing out more of the deployment: Prometheus + Grafana for better situational awareness, plus haproxy so I can do more interesting load testing like shutting off individual nodes.

---

Prometheus:
* building from scratch for illumos
** need: golang, nodejs, yarn
*** added OmniOSce "extra" publisher
*** installed golang 1.14 (plus add path)
*** installed nodejs 12
*** used `npm install -g yarn` (plus add path)
*** needed to install gnu-tar and put that onto PATH before tar
*** needed to set TMPDIR=/var/tmp because /tmp isn't big enough.
*** needed to build `promu` first because the build doesn't have a binary for that but doesn't handle that case.  See https://elatov.github.io/2020/04/monitoring-other-targets-with-prometheus/#compiling-node_exporter-on-omnios[here].  Worked around as described there, by pulling `promu` source.
*** also needed to apply patch below to client_unix.go.

[source,text]
----
diff --git a/vendor/github.com/docker/docker/client/client_unix.go b/vendor/github.com/docker/docker/client/client_unix.go
index 178ff6740..69fb1b48f 100644
--- a/vendor/github.com/docker/docker/client/client_unix.go
+++ b/vendor/github.com/docker/docker/client/client_unix.go
@@ -1,4 +1,4 @@
-// +build linux freebsd openbsd netbsd darwin dragonfly
+// +build linux freebsd openbsd netbsd darwin dragonfly illumos

 package client // import "github.com/docker/docker/client"
----

Grafana: huge pain, but ultimately:
* need at least 8G of memory (!)
* install yarn, node, go, etc.
* git clone
* git checkout # tag you want
* `rm -rf packages/grafana-e2e`
* `yarn install --pure-lockfile` or whatever
* `yarn start` or whatever (might be able to use `go run build.go build-frontend` instead)
* `go run build.go build`
* `go run build.go pkg-archive`
* (appeared to be missing `make build` (for `make build-js`) there?)

== 2020-09-01

* Set up elastic IP for my dev zone.  This looks like about $44/year if my instance were off the whole year, which seems reasonable.
* Set up manual deployment of Prometheus and Grafana in "mon" VM
** use user called "mon" for Prometheus and Grafana
** /export/home/mon/{bin,etc,grafana,var/prometheus/data}

So it will look like:

[source,text]
----
/export/home/mon/bin/prometheus
/export/home/mon/etc/prometheus.yml
/export/home/mon/var/prometheus/data/...
/export/home/mon/grafana/
----

(note: I changed this on 9/2 to separate Prometheus and Grafana into their own directories because they seem more oriented around that approach and it's not clear there's much value in following the traditional system package manager layout here.)

prometheus to be invoked as: prometheus --storage.tsdb.path=... --config.file=... &
refresh: kill -HUP?

NOTE: cockroachdb on one node went into maintenance on boot again because of clock issues.  This time, chrony had definitely finished starting before cockroachdb went into maintenance.  Is this going to be a serious problem?

I did eventually get Prometheus set up pulling from CockroachDB.

I tried running Grafana, but found that my build was busted in a way that only fails when you go to configure a data source in the web UI.

Finally got that fixed and updated instructions above.

== 2020-09-02 and 2020-09-03

Working to automate the deployment of Prometheus and Grafana to a dev zone.  This included a bunch of changes:

* refactored "vminit" directory and created a janky build that creates a "common" tarball for chrony and role-specific tarballs for the database/loadgen and monitoring VMs.
** refactored directory structure of "mon" VM from what's above
** built "fetcher" command to fetch asset from S3
** updated Terraform to configure IAM to support this
** updated vminit.sh to use "fetcher" and reflect the rest of these changes
* incorporated Prometheus
** with config to automatically discover EC2 instances in this project
** with config to scrape Grafana too
** updated Terraform to configure IAM to support this
* incorporated Grafana
** including our Prometheus data source
** including stock Prometheus, Grafana, and CockroachDB dashboards.  This involved manually fixing them to remove DS_PROMETHEUS/DS_NAME inputs -- see the README in that directory.
* various improvements:
** more useful hostnames for VMs (though this is not currently persistent)
** created "env.sh" file with various useful aliases

== 2020-09-04

* added Prometheus node_exporter (see [prometheus/node_exporter#1836](https://github.com/prometheus/node_exporter/issues/1836))
* built out a Grafana dashboard to show key metrics.  Discovered [prometheus/node_exporter#1837](https://github.com/prometheus/node_exporter/issues/1837).

Still, I think I'm just about ready to do some more serious testing.

== 2020-09-08

Summary:

* Switched to Joshua's OmniOS image running his metadata agent: AMI
  `ami-012f34b61b75182e8`.
* Updated Terraform config to deploy much larger root disks.
* Spent some time automating disk and zpool expansion to match provisioned size before realizing that Josh's image already does this.
* Recreated dashboard from Friday
* Ran a bunch of tests:
** ycsb workload: increasing levels of concurrency
** The workload appeared largely bottlenecked on one db node, so I went to experiment with a much larger DB and adding splits.
** I ran into a lot of different errors trying to make this work.  I'm not sure what the root cause really was except stuff being really busy?
** The "kv" workload might be easier to run and just as useful a next step.

Around 9am PT, ran:

[source,text]
----
$ cockroach workload run ycsb --concurrency=1 --drop --histograms histograms-ycsbA-c=1-"$(date +%FT%TZ)".out --tolerate-errors --workload A
----

I let this run for an hour.

Around 1pm PT, I ran:

[source,text]
----
$ cockroach workload run ycsb --concurrency=1 --drop --histograms histograms-ycsbA-c=1-"$(date +%FT%TZ)".out --tolerate-errors --workload A --duration 5m
----

Around 1:16PM, I'm running:

[source,text]
----
$ cockroach workload run ycsb --concurrency=2 --drop --histograms histograms-ycsbA-c=2-"$(date +%FT%TZ)".out --tolerate-errors --workload A --duration 5m
----

Around 1:23PM, I'm running:

[source,text]
----
$ cockroach workload run ycsb --concurrency=4 --drop --histograms histograms-ycsbA-c=4-"$(date +%FT%TZ)".out --tolerate-errors --workload A --duration 5m
----

Around 1:34PM, I'm running:

[source,text]
----
$ cockroach workload run ycsb --concurrency=8 --drop --histograms histograms-ycsbA-c=8-"$(date +%FT%TZ)".out --tolerate-errors --workload A --duration 5m
----

Around 1:42PM, I'm running:

[source,text]
----
$ cockroach workload run ycsb --concurrency=16 --drop --histograms histograms-ycsbA-c=16-"$(date +%FT%TZ)".out --tolerate-errors --workload A --duration 5m
----

At this point, db0 CPUs exceeded 90% CPU utilization.  It's busier than all the other nodes, by a lot.  Let's see what happens if we go further.

Around 1:51PM:

[source,text]
----
$ cockroach workload run ycsb --concurrency=32 --drop --histograms histograms-ycsbA-c=32-"$(date +%FT%TZ)".out --tolerate-errors --workload A --duration 5m
----

It's actually degraded okay at this point, by which I mean that throughput did actually increase and p95/p99 didn't get extremely bad.  I note that in the CRDB Admin UI, nearly all of the queries have hit the "n1" store today.  Only during this last workload did we see any queries hit another store, and it was n3.  Maybe CRDB is dynamically splitting by load?

Note that during this workload is where we start seeing replica errors and more "not leaseholder" errors than before.

Digging further into AdminUI, this database is only 128 MiB, with 4 ranges.  It's not shocking that it's not that distributed.

What if we go further?

At 1:58PM PT:

[source,text]
----
$ cockroach workload run ycsb --concurrency=64 --drop --histograms histograms-ycsbA-c=64-"$(date +%FT%TZ)".out --tolerate-errors --workload A --duration 5m
----

The results look similar to the previous one.  Throughput is less consistent, but hasn't gotten below the point where it was previously consistent.  We did seem to see some SQL 'exec_error's, but I don't see them in the client.  This graph in Grafana also doesn't seem totally consistent with the one in CockroachDB's Admin UI.  It's correlated, though.

Why not go further and see how this goes?

At 2:08 PM PT:

[source,text]
----
$ cockroach workload run ycsb --concurrency=128 --drop --histograms histograms-ycsbA-c=128-"$(date +%FT%TZ)".out --tolerate-errors --workload A --duration 5m
----

This one definitely saw spikes in SQL "exec_error", and potentially worse throughput than the previous one.  All db nodes are pretty tapped at this point.

I want to see what happens with this workload if I tune up the split count and total row count, since it seems pretty pokey right now.  I'm going to go back to concurrency 16, which is one step past 8, which was the stablest and most consistent.

[source,text]
----
$ cockroach workload init ycsb --splits 8 --concurrency=16 --drop --insert-count=1000000 --workload A
$ cockroach workload  run ycsb --splits 8 --concurrency=16 --drop --insert-count=1000000 --workload A --histograms histograms-ycsbA-c=16-"$(date +%FT%TZ)".out --tolerate-errors --duration 1h
----

The loading step is taking quite a while.  It's hammering both CPUs on one database node (so, concurrency=1, I guess)?

While this was going on, I was able to:

[source,text]
----
root@192.168.1.118:26257/ycsb> select count(*) from usertable;
  count
----------
  438000
(1 row)

Time: 51.558088946s
----

But when I tried this later, I got a strange error:

[source,text]
----
root@192.168.1.118:26257/ycsb> select count(*) from usertable;
ERROR: driver: bad connection
warning: connection lost!
opening new connection: all session settings will be lost
root@192.168.1.118:26257/ycsb>
----

I'm not sure which host I was connected to.  I checked all three logs but didn't see anything obvious.

The `init` command failed after 20 minutes with:

[source,text]
----
cockroachdb@loadgen0:~$ time cockroach workload init ycsb --splits 8 --concurrency=16 --drop --insert-count=1000000 --workload A
Error: failed insert into usertable: pq: split failed while applying backpressure to [txn: 4705c25f], ConditionalPut [/Table/81/1/"user4211402063788639270"/0,/Min), ConditionalPut [/Table/81/1/"user4211402063788639270"/1/1,/Min), ConditionalPut [/Table/81/1/"user4211402063788639270"/2/1,/Min), ConditionalPut [/Table/81/1/"user4211402063788639270"/3/1,/Min), ConditionalPut [/Table/81/1/"user4211402063788639270"/4/1,/Min), ConditionalPut [/Table/81/1/"user4211402063788639270"/5/1,/Min), ConditionalPut [/Table/81/1/"user4211402063788639270"/6/1,/Min), ConditionalPut [/Table/81/1/"user4211402063788639270"/7/1,/Min), ConditionalPut [/Table/81/1/"user4211402063788639270"/8/1,/Min), ConditionalPut [/Table/81/1/"user4211402063788639270"/9/1,/Min), ConditionalPut [/Table/81/1/"user4211402063788639270"/10/1,/Min), ConditionalPut [/Table/81/1/"user8166519625965030401"/0,/Min), ConditionalPut [/Table/81/1/"user8166519625965030401"/1/1,/Min), ConditionalPut [/Table/81/1/"user8166519625965030401"/2/1,/Min), ConditionalPut [/Table/81/1/"user8166519625965030401"/3/1,/Min), ConditionalPut [/Table/81/1/"user8166519625965030401"/4/1,/Min), ConditionalPut [/Table/81/1/"user8166519625965030401"/5/1,/Min), ConditionalPut [/Table/81/1/"user8166519625965030401"/6/1,/Min), ConditionalPut [/Table/81/1/"user8166519625965030401"/7/1,/Min), ConditionalPut [/Table/81/1/"user8166519625965030401"/8/1,/Min), ... 10976 skipped ..., ConditionalPut [/Table/81/1/"user6890362626482376666"/7/1,/Min), ConditionalPut [/Table/81/1/"user6890362626482376666"/8/1,/Min), ConditionalPut [/Table/81/1/"user6890362626482376666"/9/1,/Min), ConditionalPut [/Table/81/1/"user6890362626482376666"/10/1,/Min), EndTxn(commit:true tsflex:true) [/Table/81/1/"user4211402063788639270"/0]  on range r101:/{Table/81-Max} [(n1,s1):1, (n2,s2):2, (n3,s3):3, next=4, gen=42]: operation "split queue process replica 101" timed out after 1m0s: split at key /Table/81/1/"user1430647350823960411" failed: context deadline exceeded

real    19m28.125s
user    0m15.002s
sys     0m2.580s
----

Amusing sideshow:

[source,text]
----
root@192.168.1.118:26257/ycsb> select count(*) from usertable;
invalid syntax: statement ignored: unexpected error: read tcp 192.168.1.118:54604->192.168.1.118:26257: read: connection reset by peer
warning: error retrieving the transaction status: driver: bad connection
warning: connection lost!
opening new connection: all session settings will be lost
root@192.168.1.118:26257/ycsb ?>
----

But ultimate it had created 714,000 rows:

[source,text]
----
select count(*) from usertable;
  count
----------
  714000
(1 row)

Time: 12.48402931s
----

Details on that https://www.cockroachlabs.com/docs/stable/common-errors.html#context-deadline-exceeded[context deadline exceeded] error.

The database is at least 4 GiB now, although ycsb is only 1.6 GiB (maybe that's logical?)

The database is at least 4 GiB now, although ycsb is only 1.6 GiB (maybe that's logical?).  There's only one range, though.

Resuming with:

[source,text]
----
$ time cockroach workload init ycsb --splits 8 --concurrency=16 --insert-start 714000 --insert-count=1000000 --workload A
----

I realized that isn't right -- the insert count needs to be adjusted.  Tried to get a new count and got:

[source,text]
----
root@192.168.1.118:26257/ycsb> select count(*) from usertable;
ERROR: driver: bad connection
warning: connection lost!
opening new connection: all session settings will be lost
----

This is repeatable.  Is this an haproxy timeout?  I didn't reproduce it (one time) hitting a CRDB node directly.

So now:

[source,text]
----
$ time cockroach workload init ycsb --splits 8 --concurrency=16 --insert-start 714000 --insert-count=286000 --workload A
Error: failed insert into usertable: pq: duplicate key value (ycsb_key)=('user10357802244052365217') violates unique constraint "primary"

real    1m7.990s
user    0m0.787s
sys     0m0.831s
----

Yeesh.

I'm seeing this repeatedly now, even when I bump the count up.  When I bumped it way up:

[source,text]
----
cockroachdb@loadgen0:~$ time cockroach workload init ycsb --splits 8 --concurrency=16 --insert-start 800000 --insert-count=200000 --workload A
Error: failed insert into usertable: driver: bad connection

real    3m1.377s
user    0m1.018s
sys     0m0.641s
----

I'm going to try without going through haproxy.

[source,text]
----
$ time cockroach workload init ycsb --splits 8 --concurrency=16 --insert-start 900000 --insert-count=100000 --workload A postgresql://root@192.168.1.104:26257/ycsb?sslmode=disable
----

This ultimately failed with another constraint violation error.  There are now 721,000 rows in `usertable`.

For kicks, I'm going to start the above workload anyway to see how it goes.  Tomorrow, I'll probably reset and do the "kv" workload.  This should have a few advantages because it doesn't do so much work during the "init" phase.  That's good because this phase is harder to observe and not parallelized, as far as I can tell.

It may still be worth digging into the ycsb issues to better understand how things fail when they go wrong.  It would be good to better understand what SQL it's running (how many rows is it trying to insert at once?), with what concurrency, how long those INSERTs are taking, etc.

I realized as I started this that I wasn't sure the splits had been applied.  So I'll run this:

[source,text]
----
$ time cockroach workload init ycsb --splits 8 --concurrency=16 --insert-count=0 --workload A
I200908 22:38:20.458190 1 workload/workloadsql/workloadsql.go:113  starting 8 splits

real    0m1.885s
user    0m0.106s
sys     0m0.058s
$ cockroach workload  run ycsb --splits 8 --concurrency=16 --workload A --histograms histograms-ycsbA-c=16-"$(date +%FT%TZ)".out --tolerate-errors --duration 1h
----

Incidentally, this command's documentation is rather confused.  Some of these (like `--splits`) apply at init time, but that's not clear.  Other things are just documented wrong (`--insert-start` vs. `--initial-count`).

A few minutes into this workload (around 3:47pm PT), the Grafana metrics tanked.  Activity went to zero, CPU utilization is no longer reported.  All services in all VMs appear to be running as normal.  The workload is reporting a bunch of successful operations per second!

It looks like the "mon" zone ran out of disk space.  It's still got a 2 GiB disk for some reason, even though the disk is 10 GiB.  The other nodes had this problem earlier, and rebooting fixed it because Joshua's image automatically expands the pool to match the physical size.  Maybe I forgot to reboot this one?  Anyway, I made the mistake of trying to fix this by rebooting it.  I doubt this will work because it probably won't be able to come up with 0 bytes available.  I may have to redeploy this VM, in which case I'll have lost today's testing data.  I do have screenshots and the client-side data, if it's really important.  It's also presumably reproducible.

I redeployed this zone (having saved the dashboard JSON!).  As the workload is running now (see above): CPU utilization is high for all CPUs on all db nodes (77%-90%).  db1 is a little lower -- closer to the 77% level.  Queries aren't perfectly distributed across the nodes, but it's not bad.  Average throughput is about 1K selects + 1K updates per second, which is a little less than c=16 earlier today, but the database is much bigger now.

Throughput dropped to zero for a while and spat this out:

[source,text]
----
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
 2101.0s        0         1157.6         1059.4      3.3      7.3     13.1     21.0 read
 2101.0s        0         1139.6         1058.5     10.0     17.8     28.3     46.1 update
 2102.0s        0         1123.4         1059.4      3.4      7.9     18.9     28.3 read
 2102.0s        0         1092.3         1058.6     10.5     18.9     33.6     52.4 update
 2103.0s        0         1117.0         1059.5      3.3      6.8     15.7     37.7 read
 2103.0s        0         1117.0         1058.6     10.0     19.9     28.3     35.7 update
 2104.0s        0         1229.1         1059.5      3.5      6.8     11.0     23.1 read
 2104.0s        0         1145.1         1058.6     10.0     16.3     24.1     30.4 update
 2105.0s        0         1192.1         1059.6      3.4      8.4     14.2     26.2 read
 2105.0s        0         1083.1         1058.6     10.0     19.9     29.4     39.8 update
 2106.0s        0         1160.1         1059.6      3.4      7.1     14.2     26.2 read
 2106.0s        0         1146.1         1058.7     10.0     17.8     26.2     35.7 update
 2107.0s        0         1131.9         1059.7      3.4      7.6     13.6     27.3 read
 2107.0s        0         1129.9         1058.7     10.0     18.9     28.3     35.7 update
 2108.0s        0         1142.0         1059.7      3.4      8.1     14.2     35.7 read
 2108.0s        0         1120.0         1058.7     10.0     18.9     26.2     39.8 update
 2109.0s        0         1155.2         1059.8      3.4      7.1     10.5     21.0 read
 2109.0s        0         1207.2         1058.8     10.0     16.3     21.0     31.5 update
 2110.0s        0         1154.4         1059.8      3.4      8.4     16.3     23.1 read
 2110.0s        0         1056.4         1058.8     10.0     21.0     32.5     48.2 update
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
 2111.0s        0         1134.6         1059.8      3.3      6.8     17.8     35.7 read
 2111.0s        0         1124.6         1058.8     10.0     17.8     28.3     60.8 update
 2112.0s        0            0.0         1059.3      0.0      0.0      0.0      0.0 read
 2112.0s        0            0.0         1058.3      0.0      0.0      0.0      0.0 update
 2113.0s        0            0.0         1058.8      0.0      0.0      0.0      0.0 read
 2113.0s        0            0.0         1057.8      0.0      0.0      0.0      0.0 update
 2114.0s        0            0.0         1058.3      0.0      0.0      0.0      0.0 read
 2114.0s        0            0.0         1057.3      0.0      0.0      0.0      0.0 update
 2115.0s        0            0.0         1057.8      0.0      0.0      0.0      0.0 read
 2115.0s        0            0.0         1056.8      0.0      0.0      0.0      0.0 update
 2116.0s        0            0.0         1057.3      0.0      0.0      0.0      0.0 read
 2116.0s        0            0.0         1056.3      0.0      0.0      0.0      0.0 update
 2117.0s        0            0.0         1056.8      0.0      0.0      0.0      0.0 read
 2117.0s        0            0.0         1055.8      0.0      0.0      0.0      0.0 update
 2118.0s        0            0.0         1056.3      0.0      0.0      0.0      0.0 read
 2118.0s        0            0.0         1055.3      0.0      0.0      0.0      0.0 update
 2119.0s        0            0.0         1055.8      0.0      0.0      0.0      0.0 read
 2119.0s        0            0.0         1054.8      0.0      0.0      0.0      0.0 update
 2120.0s        0            0.0         1055.3      0.0      0.0      0.0      0.0 read
 2120.0s        0            0.0         1054.3      0.0      0.0      0.0      0.0 update
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
 2121.0s        0            0.0         1054.8      0.0      0.0      0.0      0.0 read
 2121.0s        0            0.0         1053.8      0.0      0.0      0.0      0.0 update
 2122.0s        0            0.0         1054.4      0.0      0.0      0.0      0.0 read
 2122.0s        0            0.0         1053.3      0.0      0.0      0.0      0.0 update
 2123.0s        0            0.0         1053.9      0.0      0.0      0.0      0.0 read
 2123.0s        0            0.0         1052.9      0.0      0.0      0.0      0.0 update
 2124.0s        0            0.0         1053.4      0.0      0.0      0.0      0.0 read
 2124.0s        0            0.0         1052.4      0.0      0.0      0.0      0.0 update
 2125.0s        0            0.0         1052.9      0.0      0.0      0.0      0.0 read
 2125.0s        0            0.0         1051.9      0.0      0.0      0.0      0.0 update
 2126.0s        0            0.0         1052.4      0.0      0.0      0.0      0.0 read
 2126.0s        0            0.0         1051.4      0.0      0.0      0.0      0.0 update
 2127.0s        0            0.0         1051.9      0.0      0.0      0.0      0.0 read
 2127.0s        0            0.0         1050.9      0.0      0.0      0.0      0.0 update
 2128.0s        0            0.0         1051.4      0.0      0.0      0.0      0.0 read
 2128.0s        0            0.0         1050.4      0.0      0.0      0.0      0.0 update
 2129.0s        0            0.0         1050.9      0.0      0.0      0.0      0.0 read
 2129.0s        0            0.0         1049.9      0.0      0.0      0.0      0.0 update
 2130.0s        0            0.0         1050.4      0.0      0.0      0.0      0.0 read
 2130.0s        0            0.0         1049.4      0.0      0.0      0.0      0.0 update
E200908 23:14:11.770407 1 workload/cli/run.go:445  pq: result is ambiguous (error=rpc error: code = Unavailable desc = transport is closing [propagate])
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
 2131.0s        1            0.0         1049.9      0.0      0.0      0.0      0.0 read
 2131.0s        1            0.0         1048.9      0.0      0.0      0.0      0.0 update
 2132.0s        3          745.8         1049.8      3.3      7.1     12.6  20401.1 read
 2132.0s        3          772.8         1048.8     10.0     18.9     27.3  20401.1 update
 2133.0s        3          836.1         1049.7      3.0      6.3      8.9     12.1 read
 2133.0s        3          864.1         1048.7      9.4     16.3     28.3     35.7 update
 2134.0s        3          815.0         1049.5      3.1      6.6     11.5     26.2 read
 2134.0s        3          808.0         1048.6      9.4     18.9     33.6  22548.6 update
 2135.0s        3          879.2         1049.5      3.0      6.6      8.9     13.1 read
 2135.0s        3          854.1         1048.5      9.4     14.7     22.0     37.7 update
 2136.0s        3          856.1         1049.4      3.1      6.3      7.6     12.1 read
 2136.0s        3          849.1         1048.4      9.4     16.3     23.1     27.3 update
 2137.0s        3          834.9         1049.3      3.0      6.6     11.0     13.6 read
 2137.0s        3          805.9         1048.3     10.0     17.8     24.1     30.4 update
 2138.0s        3          930.8         1049.2      3.1      6.3     10.5     18.9 read
 2138.0s        3          864.8         1048.2      8.9     14.7     21.0     29.4 update
 2139.0s        3          829.2         1049.1      2.9      6.0     10.0     62.9 read
 2139.0s        3          875.2         1048.1      9.4     16.3     21.0     32.5 update
 2140.0s        3          833.7         1049.0      3.0      6.8      9.4     21.0 read
 2140.0s        3          840.7         1048.0      9.4     17.8     23.1     31.5 update
----

Another one I saw was:

[source,text]
----
E200908 23:15:23.487617 1 workload/cli/run.go:445  pq: result is ambiguous (error=unable to dial n1: breaker open [exhausted])
----

Maybe I'm running too close to saturation?  Until this point, p95 latency was very steady around 18ms across all three nodes.  p99 was very steady at around 30ms across all three nodes.  Now the thing is falling apart.  I wonder if this would work better with three different load generator instances (processes, not VMs) instead of haproxy?  But these look like internal errors.


A few minutes later, the workload has recovered to where it was before.  It seems like we triggered a crash?  But the uptime on all of them shows 6 hours.  That said, there was a loss of connections to .236 and a bunch of ranges reported being uner-replicated for a minute.  CockroachDB did not actually restart on that node.  I do see some errors in the logs:

[source,text]
----
W200908 23:15:19.027322 198 kv/kvserver/node_liveness.go:592  [n3,liveness-hb] failed node liveness heartbeat: oper
ation "node liveness heartbeat" timed out after 4.5s
(1) operation "node liveness heartbeat" timed out after 4.5s
Wraps: (2) context deadline exceeded
Error types: (1) *contextutil.TimeoutError (2) context.deadlineExceededError

An inability to maintain liveness will prevent a node from participating in a
cluster. If this problem persists, it may be a sign of resource starvation or
of network connectivity problems. For help troubleshooting, visit:

    https://www.cockroachlabs.com/docs/stable/cluster-setup-troubleshooting.html#node-liveness-issues

...

I200908 23:15:19.062947 196 server/status/runtime.go:504  [n3] runtime stats: 0 B RSS, 242 goroutines, 108 MiB/1004
 MiB/269 MiB GO alloc/idle/total, 174 MiB/221 MiB CGO alloc/total, 187.1 CGO/sec, 0.0/0.0 %(u/s)time, 0.0 %gc (1x),
 0 B/0 B (r/w)net
W200908 23:15:19.482447 98 kv/kvserver/closedts/provider/provider.go:155  [ct-closer] unable to move closed timesta
mp forward: not live
(1) attached stack trace
  | github.com/cockroachdb/cockroach/pkg/kv/kvserver.init
  |     /ws/cockroach/gopath/src/github.com/cockroachdb/cockroach/pkg/kv/kvserver/node_liveness.go:60
  | runtime.doInit
  |     /opt/go/1.14.4/src/runtime/proc.go:5420
  | runtime.doInit
  |     /opt/go/1.14.4/src/runtime/proc.go:5415
  | runtime.doInit
  |     /opt/go/1.14.4/src/runtime/proc.go:5415
  | runtime.doInit
  |     /opt/go/1.14.4/src/runtime/proc.go:5415
  | runtime.doInit
  |     /opt/go/1.14.4/src/runtime/proc.go:5415
  | runtime.main
  |     /opt/go/1.14.4/src/runtime/proc.go:190
  | runtime.goexit
  |     /opt/go/1.14.4/src/runtime/asm_amd64.s:1373
Wraps: (2) not live
Error types: (1) *withstack.withStack (2) *errors.errorString
----

== 2020-09-09

Switching to "kv" workload (see yesterday's notes).

cockroach workload init kv --concurrency 4 --max-block-bytes=4096 --min-block-bytes=3072
for c in 4 8 16 32 64 128; do
    cockroach workload run kv --concurrency $c --duration 10m --histograms histograms-kv-c=$c-$(date +%FT%TZ)Z.out  --max-block-bytes=4096 --min-block-bytes=3072 --read-percent=50 --tolerate-errors
done

Notes:

* This is a considerably larger record size than I had been testing previously.
* The database is getting bigger each time this way so it's not a totally fair test among different levels of concurrency.

Results:

* The total number of SQL connections and active queries scales up with the concurrency as we'd expect.
* Starting with c=8:
** the CPU utilization graphs look about the same for all runs.
** the distribution of SQL queries to each node looks about the same.
* The SQL query throughput looks about the same among all these runs.
* The SQL query p95 latency increases with each run.
* According to `iostat`, the disk is quite busy much of the time (essentially 100% at c=64).  Occasionally, the wait time at the zpool level is upwards of 100 (ms?), but it never gets nearly that high on the actual disk.
* There was one spike in p99 SQL latency of 9s on one node (192.168.1.236 @ 15:48:30Z).

Conclusions:

* The system is basically saturated at c=4.
* Extreme outliers start around c=32.  Things really start getting nonlinear around c=128.

Possible way to go next: stick with c=16 and expand the cluster while all this is going on.  From initial 3 nodes -> 6 nodes -> 9 nodes -> 12 nodes.

17:10Z: deployed node#4.
17:14Z: the new node is definitely in service.  CPU utilization of other nodes has gone down a bit, as has query throughput.  p95/p99 latency spiked a lot.  Heartbeat latency spiked to over 5s.  Big spike in exec errors over 4Kps.
17:17Z: another spike in p95/p99 to 10s.  I don't know why this is happening -- the client isn't even updated to establish new connections so it shouldn't be using the new node.

[source,text]
----
  760.0s        0            0.0          366.8      0.0      0.0      0.0      0.0 write
E200909 17:13:18.084065 1 workload/cli/run.go:445  ERROR: result is ambiguous (error=unable to dial n2: breaker open [exhausted]) (SQLSTATE 40003)
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
  761.0s        1           21.0          364.9    130.0  13421.8  13421.8  13421.8 read
...
  969.0s        4            0.0          304.6      0.0      0.0      0.0      0.0 write
E200909 17:16:47.529391 1 workload/cli/run.go:445  EOF
  970.0s        5            6.0          303.1     13.1     35.7     35.7     35.7 read
  970.0s        5            5.0          304.3     18.9  60129.5  60129.5  60129.5 write
E200909 17:16:48.580680 1 workload/cli/run.go:445  EOF
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
  971.0s       15          115.0          302.9     17.8  60129.5  60129.5  60129.5 read
  971.0s       15          111.0          304.1     15.2    113.2  60129.5  60129.5 write
----

As of 17:19Z: consistently seeing 20 errors per second with 100-200 ops per second.

Note: Prometheus didn't pick up the new node right away.  Maybe a better methodology is to preprovision everything, then shut down a bunch?

17:22:45: restarted Prometheus

Note: replication started around 17:12:30 and finished around 17:25:Z.

At 17:30Z, I'm going to restart the load generator to force it to pick up all four nodes.

Perhaps not surprisingly?  This only made some performance worse because some ranges moved to a node that's not handling any requests.

17:28:50Z: restarted client workloads  However, despite having sent SIGHUP to haproxy, it doesn't seem to have picked up the fourth server.
17:30:40Z: restart both haproxy and load generator.
Now we do see 4 active servers in haproxy and all four CRDB nodes have connections.

Note: I just checked the CRDB Admin UI to look at "queries per store", which has previously been a proxy for whether queries were being served equally by the different nodes, and it does look well distributed both before and after the new node was added.

The performance now is decidedly worse than before I added the new node.  p95/p99 latency is definitely higher for two nodes, and about the same for the other two (even the one which wasn't up, which is weird).  Right now, the disks on db0 are 100% pegged all the time.  Disks on the other two original nodes have a fair bit of headroom.  Why is that one so busy?  Is this because the kv workload is not random?

In the Admin UI, I see that the UPSERT statements have seen 10 retries, but that's cumulative -- doesn't seem important.

I can see that all four nodes are serving a comparable number of queries, and their CPU utilization isn't to far off (and all under 30% per CPU).  But db0 is still being hammered on I/O.  db1 is going through periods where it's busy too, but not nearly as much.  Note that average service time on this disk is worse than disk on db1, even when db1's is busy too, by a factor of 3 or so (6ms for db1, 20ms for db0).  A few minutes later: the 100% busy seems to have moved to db2 and db0 was idle for a little while.  A few seconds later we're back to db0.

Maybe it would be more interesting to do just 20% writes / 80% reads.

CRDB does recommend:

> Disks must be able to achieve 500 IOPS and 30 MB/s per vCPU....Monitor IOPS for higher service times. If they exceed 1-5 ms, you will need to add more devices or expand the cluster to reduce the disk latency.

Anyway, the preformance has been the same for a while.  I'm going to shut this down and drop both databases for now in prep for future runs.  I'm also going to decommission that fourth node.

This failed:

[source,text]
----
# cockroach node decommission  4

  id | is_live | replicas | is_decommissioning | membership | is_draining
-----+---------+----------+--------------------+------------+--------------
   4 |  true   |       53 |       false        |   active   |    false
(1 row)

  id | is_live | replicas | is_decommissioning |   membership    | is_draining
-----+---------+----------+--------------------+-----------------+--------------
   4 |  true   |       53 |        true        | decommissioning |    false
(1 row)
..........
  id | is_live | replicas | is_decommissioning |   membership    | is_draining
-----+---------+----------+--------------------+-----------------+--------------
   4 |  true   |       52 |        true        | decommissioning |    false
(1 row)
...........
ERROR: connection lost.

while trying to mark as decommissioning: rpc error: code = Unavailable desc = transport is closing
Failed running "node decommission"
----

More haproxy woes?

== 2020-09-10

Not much testing today, but I'm reflecting on the issues I've hit so far.  I've hit a bunch of different client issues that seem potentially related to overload, and seen symptoms of overloaded servers (e.g., missed heartbeats).  I've potentially been pushing the system beyond its intended capacity, particularly in terms of I/O.  It _should_ handle that okay, but maybe isn't a great first test.

Maybe try a few simplifying changes:

* Provision 6 database nodes up front, but don't start CockroachDB on three of them.  (Or, disable cockroachdb on the last three before running `cockroach init`.)  This way I eliminate any disruptive change to the initially-running three (like restarting them, which my Terraform config normally does, although I had commented that out yesterday).  And I know exactly when each one starts.
* Factor out haproxy: instead of one load generator process using haproxy to talk to CRDB nodes, maybe use separate client processes pointed at specific CRDB nodes.  They won't automatically start using new nodes this way so I will have to start more up again.
* Let's not start by pushing the cluster to its limit.  Instead, let's separate out a few different questions:
** try to replicate something close to the https://www.cockroachlabs.com/docs/stable/performance.html#throughput[basic sysbench numbers that they got on AWS]?  These are much bigger machines, but even if we can just achieve that latency at a lower level of concurrency, that'd be useful.
** demonstrate horizontal scalability (_not_ necessarily online): maybe the way to think about this is: ramp up load generators until p95 latency reaches some target.  See how that point differs at different cluster sizes.
** demonstrate expanding the cluster under modest load (largely ignoring performance -- it would be enough that it doesn't get worse or experience errors)
** demonstrate shrinking the cluster under modest load (similar to expansion)
** demonstrate the impact of failures on a modest load (again, largely ignoring performance)

Last item of the day: trying to get %busy and average I/O time metrics in Grafana.  This is a little tricky from the kstats.

== 2020-09-14

Conclusions from today:

* Built sysbench and started using its oltp_insert workload for testing.
* Ran into major interference from AWS "gp2" (storage volume) performance, which falls off a cliff potentially hours after starting a workload.  Confirmed this with CloudWatch "burst" metric.  Will work around this with "io1" volumes instead of "gp2".
* Aside from that, performance was reasonably stable.  I successfully expanded the cluster a few times.  Performance got better, but not linearly so, and load was not perfectly distributed with n=4 or n=5.  (Did not get to n=6 because of the I/O problem.)
* Ran into minor issue with the image I'm using: can't install packages with `pkg`, apparently due to missing SSL certs.

Details:

* I've brought up a cluster with 6 database nodes, but only three had cockroachdb running when I initialized the cluster.
* Made a build of sysbench:
** in my build machine, had to install postgresql-12, autotools, libtool
** note: could not do this in Joshua's image because pkg tools can't do anything because they're looking for /etc/openssl/certs.  In my build zone, that appears to be /etc/{ssl,crypto}/certs.
** set --prefix=/opt/sysbench, tarred up directory, and copied to "loadgen0".  Also needed to add libpq.so, which I did by hand afterwards.
** `LDFLAGS='-R /opt/sysbench/lib'  ./configure --without-mysql --with-pgsql --prefix=/opt/sysbench`

Around 9:23AM PT:

[source,text]
----
# sysbench --threads=1 --time=0 --pgsql-host=192.168.1.227 --pgsql-port=26257 --pgsql-user=root --pgsql-db=sbtest oltp_insert run
----

from loadgen0.  That settled around 200 inserts per second (all handled by .227, which is db0), p95 latency=6ms, p99 latency=13ms, about 50% CPU utilization in all four CPUs of db0 and db1, just over 40% disk busy time in all three db nodes.  That's all writes, about 4 MBps, with spikes up to almost 8.

By 9:37, this has been quite stable.  Let's start another load generator aimed at db1:

[source,text]
----
# sysbench --threads=1 --time=0 --pgsql-host=192.168.1.66 --pgsql-port=26257 --pgsql-user=root --pgsql-db=sbtest oltp_insert run
----

By 9:47, this has been quite stable in throughput, with some variation in latency.  We're at 300 inserts / second, evenly split between db0 and db1.  CPU utilization for those nodes is 50%-60% (per CPU).  p95 latency (both nodes) is around 8ms, p99 latency ranging from 16-17.5ms.  Disks almost 50% busy on all nodes.  Average disk I/O time is unchanged, largely maxing around 800us -- pretty good!  CockroachDB heartbeat p99 latency is pretty consistently under 10ms.

At 9:50AM, I started a third load generator (same loadgen VM):

[source,text]
----
# sysbench --threads=1 --time=0 --pgsql-host=192.168.1.214 --pgsql-port=26257 --pgsql-user=root --pgsql-db=sbtest oltp_insert run
----

This has settled at a throughput of about 400 inserts per second, evenly split among all three db hosts.  CPU utilizationr anges from 36-60% (per CPU) on the db nodes, with db2 closer to 40% (lower than the other two).  p95 ranges from 8-10ms, p99 15-22ms.  Disks are about 50% busy.  The disk write IOPS and busy time haven't changed a lot with the last addition.  I'm seeing CockroachDB p99 heartbeat outliers up to 110ms, but that's still quite quick.

I'm going to let this run a little while longer to see what happens.

At 10:47AM: this has been fairly stable at the above numbers.  There are a couple of ways to go from here:

* could add more write load (an additional thread for each load generator)
* could add a read workload
* could try turning off one node, but we're not going to be able to take writes that way anyway since the replication factor is 3
* try expanding the cluster without changing the load
** more conservative: do it without the workload running and see if it affects anything
** more aggressive: do this with the workload running

I'm leaning towards online expansion of the cluster.  At 10:51am: enabled cockroachdb on db3.

10:57am: there was a burst of errors (peaking at 1 error per second) when I enabled CRDB, though the load generators didn't complain at all.  Cockroach heartbeat p99 latency peaked at 40ms.  I can see the new node took on some of the write workload, particularly from db1 (looking at a shift in the write IOPS graph and percent-busy graph).  Average I/O time is still well under 1ms, and disks remain about 50% busy on the busy nodes (less so on db1 and the new db3).  Overall average throughput is slightly increased (just under 400 inserts per second to about 420).  p95 and p99 decreased a few ms each but are largely the same.  Next I will add the next database node.

11:01AM PT: added next node.

11:13AM PT: average throughput essentially unchanged, though there was a momentary crash at 18:01:30Z to just under 300 inserts per second.  That correlated with a spike in p95 on all nodes to about 15-19ms and p99 to about 35-41ms.  This seems to have shifted load from db2 (in terms of CPU utilization and disk usage and write IOPS).  Heartbeat latency p99 peaked at 204ms on the newly-added node.  Again, we had a small spike in error rate.  It's not clear if the client saw theses.

11:17AM PT: I'm going to turn off the load generators and turn them on again in order to get summary reports from them and to make sure the load isn't somehow state-dependent.  Before I do that, I see why had another small crash in throughput at 18:18Z.  This affected all nodes, like the previous one.  Note that there's plenty of CPU headroom on each CPU (though utilization spiked to 80% on a few CPUs at the time of the crash).  Similarly, disk %busy never exceeded 60% on the peak node, and it actually dipped at this time.  Average disk I/O latency was unchanged around this time.  Network throughput had a small dip.  There's no spike in heartbeat latency.  The only thing I see is a small spike in CRDB "exec_error", but it's less even than when adding new nodes before.  Well, as I said, I'm going to restart the load generators to get their numbers.

11:22AM PT: I killed the load generators, but they did not report any numbers, unfortunately.  I restarted these all around 11:23:31 PT.

11:48AM PT: noticed a major reduction in throughput that started around 11:41.  This seems to have been caused by a very sudden spike in average disk I/O latency on db0, from about about 300us to about 3ms.  Other VMs were not affected by this, but naturally the %busy on db0 shot up, to about 88%.  Write IOPS went down everywhere by a factor of ~4-5.  Net throughput dropped significantly too.  No spike in CRDB heartbeat latency nor errors.  CPU utilization down across the board.  p95 CRDB latency shot from about 10ms to about 40ms across the board, and p99 from about 20ms to about 50ms.  This is all consistent with a sudden, terrible degradation in performance from EBS, and I can't think of an obvious cause in the application.

Digging into this, there's some https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html[documentation on this].

> Each volume receives an initial I/O credit balance of 5.4 million I/O credits, which is enough to sustain the maximum burst performance of 3,000 IOPS for 30 minutes. This initial credit balance is designed to provide a fast initial boot cycle for boot volumes and to provide a good bootstrapping experience for other applications. Volumes earn I/O credits at the baseline performance rate of 3 IOPS per GiB of volume size. For example, a 100 GiB gp2 volume has a baseline performance of 300 IOPS. 
> The maximum I/O credit balance for a volume is equal to the initial credit balance (5.4 million I/O credits). 

Baseline performance for my 60 GiB volume would be 180 IOPS.

Based on their equation:

[source,text]
----
burst duration = (credit balance) / (burst IOPS - 3 * volsize)
    = 5.4M / (1600 IOPS - 3 * 60GiB)
    = 5.4M / (1600 - 180)
    = 5.4M / 1420
    = 3800
----

Presumably that's 3800 seconds, or 63 minutes.  I confirmed with CloudWatch that this instance ran out of its credit around 18:40Z.

How to work around this?  It seems very hard to manage this in a benchmarking environment.  Even if I could spend all the credits up front, it'd be hard to make sure it was constantly zero -- and across all six database hosts.

Here are a few different pricing options:

* https://calculator.aws/#/estimate?id=16e6ed9a0102c9e24880a0175edaa9eef88ac8c9[Original estimate] (6 c4.large instances with 60 GiB gp2 volumes (180 IOPS)): $474 / month
* https://calculator.aws/#/estimate?id=184d382407f5e4a64b296ec69c374f3155419801[Estimate using 167GiB gp2 volumes] to get 500 IOPS: $538.20 / month
* https://calculator.aws/#/estimate?id=efaf0e10e9cf496d4dfcc95a26dbbf4cecef56b7[Estimate using 60 GiB io1 volumes] with 500 IOPS: $678 / month

It's cheaper to just get bigger "gp2" volumes than to buy provisioned IOPS.  The problem is that I actually kind of want the consistency: I don't want the performance to plummet like it did today, even if the low value is actually fine.  It sure sucks to pay more for the same IOPS and less storage, though.

Note that the load generator does not need this.  It can stay with "gp2".

At 3:10PM PT, I'm re-evaluating performance from the last three hours:

* p95 has been pretty consistently around 42ms
* p99 has been 50-100ms with lots of spikes (peak spike was 125ms)
* CPU utilization has been consistently low
* Query throughput has consistently averaged about 125 inserts per second, though it's been rocky ranging from 100-125 qps.
* Average I/O time for all disks has been under 1ms except for db0, which has averaged 3.4ms (see above).  All disks except that one have been under 20% busy, while that one averaged 90%.
* Write IOPS differs across hosts and ranges from about 200 to 550.  Read IOPS are negligible.
* Network throughput is negligible -- averaged under 400 KBps for either inbound or outbound for all hosts.
* NTP: according to Cockroach, the mean RPC clock offset has maxed at around 200us.
* Average p99 CockroachDB heartbeat latency for the worst node is 44ms.  Peak was 354ms.
* Peak error rate was 0.35 errors per second at one point.

Overall I'd say it's been fairly stable, for a system that's maxxed out at I/O capacity.

I used Terraform to update the storage class from "gp2" to "io1" (that can be done online) and redeployed the load generator completely to get the sysbench binaries.

Plan for tomorrow:

* Run a similar sequence of steps now that I'm on "io1" storage.

== 2020-09-16

Summary of the day:

* Restarted testing on sysbench database, now that I'm using provisioned IOPS.
* Forgot to scale-down the cluster to 3 nodes, so I decided to try this dynamically.  Found some surprising replication behavior and posted to the forum about this.
* Also found that the workload got very suddenly faster (2x) after an hour or so.  Resource utilization went down, but I/O latency didn't get any faster.  It's as though it just got twice as efficient.  Relatedly, there was a range merge when this happened, so maybe some writes avoided some round-trips after that?  This is great (load-based range merges), but also makes testing performance harder.

Plan for today:

* drop previous sysbench database
* disable db3, db4 to get back to a 3-node cluster
* ramp up workload:
** start one load generator for each db node, about 10 minutes apart
* expand cluster, one node at a time, about 10 minutes apart

17:12Z: dropped database.  This was fairly cheap and didn't seem to do much work (I/O or otherwise).  Somewhat surprisingly, this didn't affect percentage of capacity used or the total number of ranges.  This https://forum.cockroachlabs.com/t/reclaiming-storage-capacity/1024[appears to be a result of the delayed GC process].  This might be an opportunity to reconfigure the TTL period as a test.

At around 17:19:30Z:

[source,text]
----
root@192.168.1.227:26257/defaultdb> SHOW ZONE CONFIGURATION FOR RANGE default;
     target     |              raw_config_sql
----------------+-------------------------------------------
  RANGE default | ALTER RANGE default CONFIGURE ZONE USING
                |     range_min_bytes = 134217728,
                |     range_max_bytes = 536870912,
                |     gc.ttlseconds = 90000,
                |     num_replicas = 3,
                |     constraints = '[]',
                |     lease_preferences = '[]'
(1 row)

Time: 1.926221ms

root@192.168.1.227:26257/defaultdb> ALTER RANGE default CONFIGURE ZONE USING gc.ttlseconds=300 ;
CONFIGURE ZONE 1

Time: 27.301437ms

root@192.168.1.227:26257/defaultdb>
----

After this, I see:

- small spikes in CPU utilization (way more than before, but peaking at about 18% per CPU)
- some disk I/O and disk writes.
- a reduction in ranges per node from 38 to 34
- a reduction in capacity used per node from about 2.4% back to about 0.6%

That seems to have worked as expected.  I'm going to configure it back to the default:


[source,text]
----
root@192.168.1.227:26257/defaultdb> ALTER RANGE default CONFIGURE ZONE USING gc.ttlseconds=90000;
CONFIGURE ZONE 1

Time: 26.959762ms

root@192.168.1.227:26257/defaultdb> SHOW ZONE CONFIGURATION FOR RANGE default;
     target     |              raw_config_sql
----------------+-------------------------------------------
  RANGE default | ALTER RANGE default CONFIGURE ZONE USING
                |     range_min_bytes = 134217728,
                |     range_max_bytes = 536870912,
                |     gc.ttlseconds = 90000,
                |     num_replicas = 3,
                |     constraints = '[]',
                |     lease_preferences = '[]'
(1 row)

Time: 1.984135ms
----

So that's good.  On to the workloads.

I forgot (and hadn't previously noted) that I needed to run:

[source,text]
----
root@192.168.1.227:26257/defaultdb> CREATE DATABASE sbtest;
CREATE DATABASE

Time: 28.410777ms

root@192.168.1.227:26257/defaultdb> ^D
root@loadgen0:~# sysbench --threads=1 --time=0 --pgsql-host=192.168.1.227 --pgsql-port=26257 --pgsql-user=root --pgsql-db=sbtest oltp_insert prepare
WARNING: Both event and time limits are disabled, running an endless test
sysbench 1.1.0-bbee5d5 (using bundled LuaJIT 2.1.0-beta3)

Creating table 'sbtest1'...
Inserting 10000 records into 'sbtest1'
Creating a secondary index on 'sbtest1'...
----

Now the work:

[source,text]
----
sysbench --threads=1 --time=0 --pgsql-host=192.168.1.227 --pgsql-port=26257 --pgsql-user=root --pgsql-db=sbtest oltp_insert run
----

That started at 17:27:15.

17:34Z.  Oops.  I forgot to shut down the two database nodes.  I will try that now, while the workload is running.  Did that around 17:35:15.  Both were graceful shutdowns (well, `svcadm disable`).  As expected, we see a bunch of under-replicated ranges.  In about 5 minutes we should see that number go back to zero.  Note that after doing this:

* CPU utilization changed as expected: the two nodes I shut down went very low.  One of the remaining three nodes went up.  For whatever reason, db1 remains pretty idle.
* Transaction throughput is holding steady around 125 inserts / second.  It's unchanged after this change.
* p95 is about 11-12ms.  p99 is around 18ms.  These are unchanged after this change.
* I'm not sure why, but disk utilization on db0 went down, and db2 went up.
* db0 and db2 are doing almost exactly 1000 write IOPS, unchanged by the change.  db4 had been doing a lot, but that crashed (as expected) when I brought it offline).

Around 17:41Z, we see the cluster rebalance itself:

* the count of under-replicated ranges starts falling
* db1 quickly starts hitting 1000 write IOPS and its percent busy increases
* db0's disk utilization (%busy) goes down a bit from about 50% to about 40% (eyeballing it)
* db2's disk utilization (%busy) goes up by a comparable amount.
* CPU utilization increases on db1 from about idle to about 40% -- less than the other two, but doing a bunch of work now.
* In terms of impact: throughput was a little less consistent, but not much, and holding steady around 125 inserts / second.
* p95 and p99 are not visibly different.
* p99 heartbeat latency has peaked at about 95ms through this whole activity.

For some reason I don't understand, the system seems to have settled at 33 under-replicated ranges.  It's not clear why this would be.  I dug into the admin UI a bit and found that the `sbtest` database has one table, `sbtest`, which uses 95 MiB and 2 total ranges.  That may explain why only two nodes were busy earlier.  There are 22 ranges in the "system" database.  I'm not sure where the other 12 ranges come from, sicne the system reports 36 total ranges.  And I'm not sure why 33 are under-replicated.

In the "Advanced Debug" "Problem Ranges Report", I can see that the under-replicated ranges are 1-34, except for range 4.  I picked range r3 arbitrarily.  The leader is n3, which is also the leaseholder.  That node is still online.  The replicas appear to be on n1 and n2, also both up.  There's a neat log for the range.  Key events:

15:43 (long before I started): looks like the range is on n1, n2, and n3.  It already says "reason: range under-replicated".
17:51:26Z: begin adding n4 because of rebalance
17:51:26Z: begin removing n3 because rebalance (that seems weird)
17:51:26Z: seem to be related to adding n4 again (VOTER_INCOMING vs. LEARNER)
17:51:26Z: removed n3 ("abandoned learner replica")
18:01:24Z: begin adding n5 because range under-replicated
18:01:24Z: finish adding n5? (how is this possible?)
18:01:24Z: begin adding n3 as a replica because range under-replicated
18:01:24Z: finish adding replica n3

Things I don't understand about this:

* n4 and n5 should be suspect from 17:36 to 17:41 and dead after that.  How did we pick them as new replicas at 17:51?
* why did we abandon n3 at 17:51Z?
* how did we finish adding n5 as a replica at 18:01 if it's offline?
* why aren't we trying to fix the fact that it's under-replicated?
* (what are all the different states for replicas?)

Of note, according to https://www.cockroachlabs.com/docs/v20.1/cluster-setup-troubleshooting#admin-ui-shows-under-replicated-unavailable-ranges[this section in the docs]:

>  The number of failures that can be tolerated is equal to (Replication factor - 1)/2. Thus CockroachDB requires (n-1)/2 nodes to achieve quorum. For example, with 3x replication, one failure can be tolerated; with 5x replication, two failures, and so on.

In this case, we had five nodes, but the replication factor was only 3, which means we can only tolerate one failure.  Given that, I suppose it wasn't reasonable to expect that we could retain cluster liveness after this operation; however, it's a little surprising the data remains under-replicated given that at least one copy is available, and this range was never leased to a node that was down.

The link above has some useful debugging instructions, but they require you to look at the "Simulated Allocator Output".  I get an error accessing that:

> An error was encountered while loading this data: This information is not available due to the current value of the 'server.remote_debugging.mode' setting. Insufficient privileges to view this resource.

There's a "Learn more" link, but the content there implies that on an insecure cluster, there should be no privilege issue.  That setting is documented thus:

> set to enable remote debugging, localhost-only or disable (any, local, off)

with a default of "local".  I guess the problem here is that I'm not coming in over localhost.

Here, I updated it:

[source,text]
----
root@loadgen0:~# cockroach sql --host 192.168.1.227
#
# Welcome to the CockroachDB SQL shell.
# All statements must be terminated by a semicolon.
# To exit, type: \q.
#
# Server version: CockroachDB CCL v20.2.0-alpha.1-1729-ge9c7cc561c-dirty (x86_64-sun-solaris2.11, built 2020/08/04 04:08:24, go1.14.4) (same version as client)
# Cluster ID: cfb6ffc3-3553-4629-a174-beb9328b4f57
#
# Enter \? for a brief introduction.
#
root@192.168.1.227:26257/defaultdb> SHOW CLUSTER SETTING server.remote_debugging.mode;
  server.remote_debugging.mode
--------------------------------
  local
(1 row)

Time: 905.67µs

root@192.168.1.227:26257/defaultdb> SET CLUSTER SETTING server.remote_debugging.mode = "any";
SET CLUSTER SETTING

Time: 37.307198ms

root@192.168.1.227:26257/defaultdb> SHOW CLUSTER SETTING server.remote_debugging.mode;
  server.remote_debugging.mode
--------------------------------
  any
(1 row)

Time: 804.065µs

root@192.168.1.227:26257/defaultdb>
----

Now that web page works.  The messages are:

> kv/kvserver/allocator.go:402 [n3,status] replace dead - replacement for 2 dead replicas priority=12000.00
> kv/kvserver/replicate_queue.go:343 [n3,status] next replica action: replace dead
> kv/kvserver/allocator.go:508 [n3,status] allocate candidates: []
> kv/kvserver/store.go:2630 [n3,status] error simulating allocator on replica [n3,s3,r3/6:/System/{NodeLive…-tsd}]: 0 of 3 live stores are able to take a new replica for the range (3 already have a replica); likely not enough nodes in cluster

This is consistent with the documentation, but a little surprising: if the live nodes have replicas (which they do), why is it under-replicated?  If they don't, why can't we create a replica there?

Stopped to debug this a bit further.
Found https://godoc.org/github.com/cockroachdb/cockroach/pkg/roachpb#ReplicaType[documentation about the replica states].

Note also that performance skyrocketed at 18:27Z, from 125 inserts per second to just over 300.  They're still all being served from the same node.  CPU utilization and disk busy time went down.  Naturally, p95 and p99 went down.  However, average I/O time didn't go down, and bytes written per second _did_ -- it's as though the thing got suddenly more efficient.  Did we have a split?  Is that why it got better?  If so, how?

I did see that in Admin UI, under "KV Transactions", we had been doing 100% "committed" transactions and 0 "fast-path committed".  At about this time, we went to 330 of both "committed" and "fast-path committed".  At the same time, we went from a fair number of "partial batches" to 0.

One big change is that from about 17:30 to 18:30, queries were split evenly between stores n1 and n2.  At 18:30, they were all on n2.  This was about the time the cluster dropped one range (from 36 to 35).  There was a merge around this time, and "sbtest1" is now in one range.  This kind of makes sense -- no more round-trip latency?  But it's going to make testing hard again.

Back to the replication question, I'm looking through forum posts:

- Here's one about https://forum.cockroachlabs.com/t/resurrect-broken-cluster/3477/2[truly bad DR].
- Here's an https://forum.cockroachlabs.com/t/under-replicated-range-in-the-cluster/3558[interesting tool: manually re-replicate a range].

I submitted a post to the forum about this that's currently awaiting moderation.  My notes and screenshots are in https://gist.github.com/davepacheco/5f6dcf64104bfdf49802504c2f30feb1#file-notes-md[this gist].

Out of convenience, I let this workload run for several more hours.  As of 23:40Z (5 hours since the jump in performance at 18:30Z), over the last five hours:

* Throughput has been stable at just over 300 inserts / second.
* p95 has been stable at just over 3ms.
* p99 has been stable at 4-7ms.
* Disk %busy has been stable at 15-20% (max).
* With one exceptional spike to almost 6ms on db5 (which is down!), average I/O latency has been stable under 1ms (mostly 300-400us on the active db nodes)
* Disk write IOPS hover around 700 on the active nodes.  Less than 1 read IOPS.
* There was only one p99 heartbeat latency above about 200ms, and that was about 1.7ms on one node around 18:50.  Average p99 is under 20ms.

This is not an impressive load, but that seems pretty good behavior.

== 2020-09-17

Summary for today:

* brought the down nodes back up and answered a bunch of my questions from yesterday.

Details:

* dropped `sbtest` database and recreated it
* temporarily set gc.ttlseconds=30 to purge that data
* at this point, the problematic ranges were not affected (still 33 under-replicated ranges)
* enabled cockroachdb on db3, db4 at 21:17Z.  Under-replicated ranges quickly dropped to zero.

Out of curiosity, what happened on our range 3 that we inspected yesterday?

* The extra two columns show up in the top table for the replicas on n1 - n5.
* There's nothing new in the range log since 9/14.  Wait, that seems like the wrong date!

Was everything I was looking at yesterday garbage?  It seems like it was.  This answers most of our questions from yesterday, which were (taken from my post):

> Why is this range considered “under-replicated” at all? As far as I can tell from the report, it has three replicas, one on each of the remaining available nodes. Relatedly, it seems contradictory that there could be no “live stores able to take a new replica” because all of them already have a replica (and given that there are as many live stores as the replication factor).

This range has five replicas to begin with.  I checked the zone configuration, and the system ranges are all configured for 5 replicas:

```
root@192.168.1.227:26257/defaultdb> SHOW ZONE CONFIGURATIONS;
                       target                      |                               raw_config_sql
---------------------------------------------------+------------------------------------------------------------------------------
  RANGE default                                    | ALTER RANGE default CONFIGURE ZONE USING
                                                   |     range_min_bytes = 134217728,
                                                   |     range_max_bytes = 536870912,
                                                   |     gc.ttlseconds = 90000,
                                                   |     num_replicas = 3,
                                                   |     constraints = '[]',
                                                   |     lease_preferences = '[]'
  DATABASE system                                  | ALTER DATABASE system CONFIGURE ZONE USING
                                                   |     range_min_bytes = 134217728,
                                                   |     range_max_bytes = 536870912,
                                                   |     gc.ttlseconds = 90000,
                                                   |     num_replicas = 5,
                                                   |     constraints = '[]',
                                                   |     lease_preferences = '[]'
  RANGE meta                                       | ALTER RANGE meta CONFIGURE ZONE USING
                                                   |     range_min_bytes = 134217728,
                                                   |     range_max_bytes = 536870912,
                                                   |     gc.ttlseconds = 3600,
                                                   |     num_replicas = 5,
                                                   |     constraints = '[]',
                                                   |     lease_preferences = '[]'
  RANGE system                                     | ALTER RANGE system CONFIGURE ZONE USING
                                                   |     range_min_bytes = 134217728,
                                                   |     range_max_bytes = 536870912,
                                                   |     gc.ttlseconds = 90000,
                                                   |     num_replicas = 5,
                                                   |     constraints = '[]',
                                                   |     lease_preferences = '[]'
  RANGE liveness                                   | ALTER RANGE liveness CONFIGURE ZONE USING
                                                   |     range_min_bytes = 134217728,
                                                   |     range_max_bytes = 536870912,
                                                   |     gc.ttlseconds = 600,
                                                   |     num_replicas = 5,
                                                   |     constraints = '[]',
                                                   |     lease_preferences = '[]'
  TABLE system.public.replication_constraint_stats | ALTER TABLE system.public.replication_constraint_stats CONFIGURE ZONE USING
                                                   |     gc.ttlseconds = 600,
                                                   |     constraints = '[]',
                                                   |     lease_preferences = '[]'
  TABLE system.public.replication_stats            | ALTER TABLE system.public.replication_stats CONFIGURE ZONE USING
                                                   |     gc.ttlseconds = 600,
                                                   |     constraints = '[]',
                                                   |     lease_preferences = '[]'
(7 rows)

Time: 16.715016ms

```

Given that, it's expected that there would be five replicas, so it makes sense that with two nodes down, these ranges are under-replicated.

> n4 and n5 were “suspect” by 17:36Z and “dead” by 17:41Z. Why did CockroachDB decide at 17:51Z to rebalance ranges from n3 onto these dead nodes? Does it not take into account that a node is dead before rebalancing?

These timestamps were from the day before, when the nodes were probably up.

> How is it possible that the replication apparently succeeded for n5 when that node was offline?

Again, these timestamps were from the day before, when the nodes were probably up.

> Why is that that the latest range descriptor in the log has all five nodes in it, but we only see three columns in the range report? Are there really five replicas and we don’t see those columns because the other two nodes are down?

I don't have more information about this, but I suspect my guess is right there, that it just doesn't show columns from nodes that are down.  When I brought the nodes back up, the columns showed up.  When I temporarily bring down n4 again, the column disappears again.  When I bring it back, the column comes back.

> In other words, maybe this is under-replicated not because there aren’t 3 (the replication factor), but because there are five, but two of them are on dead nodes? If that’s true, is there operationally a way to distinguish between replicas that are under-replicated because they’re under the replication factor vs. under-replicated because there are some dead replicas?

The premise for this question is no longer valid -- the replication factor _is_ 5.

> Relatedly, is there a way to know operationally how many under-replicated ranges are not making forward progress (e.g., because they require another node to be up)?

I think this is a valid question.

New questions:

* Is there a way to determine what zone a range is part of?  The best way I know now is to figure out what database/table it's part of (for which I think there may be a reverse index, in the Admin UI, if you browse the databases/tables?), and then use `show zone configurations`.  You can also do the first part with `SHOW RANGES FROM ...`

I wrote a new post on the CockroachDB forum.

I ran out of time today -- got distracted with other things.

== 2020-09-18

Plan: basically same as 9/16, but try to do it right this time.

* check on the https://forum.cockroachlabs.com/t/understanding-under-replicated-ranges/3982[CockroachDB forum thread I created].
* drop and recreate previous sysbench database
* disable db3, db4 to get back to a 3-node cluster
* ramp up workload:
** start one load generator for each db node, about 10 minutes apart
* expand cluster, one node at a time, about 10 minutes apart

Details:

15:26Z: cluster started
15:34Z: shut down nodes db3 and db4.  Both timed out and where forcibly killed by SMF.  As we'd expect, we have 33 under-replicated ranges -- presumably all system ranges.
15:45Z: started sysbench workload from one client:

[source,text]
----
sysbench --threads=1 --time=0 --pgsql-host=192.168.1.227 --pgsql-port=26257 --pgsql-user=root --pgsql-db=sbtest oltp_insert prepare
...
sysbench --threads=1 --time=0 --pgsql-host=192.168.1.227 --pgsql-port=26257 --pgsql-user=root --pgsql-db=sbtest oltp_insert run
----

15:55Z: started sysbench workload from second client (now two workloads running):

[source,text]
----
sysbench --threads=1 --time=0 --pgsql-host=192.168.1.66 --pgsql-port=26257 --pgsql-user=root --pgsql-db=sbtest oltp_insert run
----

16:05Z: started sysbench workload from third client (now three workloads running):

[source,text]
----
sysbench --threads=1 --time=0 --pgsql-host=192.168.1.214 --pgsql-port=26257 --pgsql-user=root --pgsql-db=sbtest oltp_insert run
----

I let this run for a few hours and ran into a few issues.  In the second and third load generators, sysbench reported:

[source,text]
----
client_loop: send disconnect: Broken pipe
----

This appeared to happen at 18:05Z and 18:15Z, based on the throughput graphs.

From about 16:10 to about 18:05Z, throughput was well-distributed across the nodes at about 280 total inserts per second.  There was plenty of CPU headroom and disks topped out around 75% busy.  p95 was steady around 16ms, p99 ranged from 18-40ms.  It's not at all clear what caused the client issues.  There was a tiny blip in "exec_error" around 18:05, but none around 18:15.  p99 heatbeat latency did spike a few times to almost 4s, including two spikes to 2.7s around 18:12:30 and 18:15.  Both of those were on .214, which is db2 (n3).  That's what the third load generator was talking to.

Checked that CockroachDB has not restarted on either of those nodes.

[source,text]
----
192.168.1.66  db1 == n3 workload failed at 18:05
192.168.1.214 db2 == n2 workload failed at 18:15
----

Interesting that they started 10 minutes apart and failed 10 minutes apart,
just about 2h10m after starting.


Next step: check logs.  On db1, this would be around 18:05.  This looks surprising from the log:

[source,text]
----
W200918 18:04:53.861971 1762946 vendor/google.golang.org/grpc/internal/channelz/logging.go:73  grpc: addrConn.createTransport failed to connect to {192.168.1.103:26257  <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 192.168.1.103:26257: connect: connection refused". Reconnecting...
W200918 18:04:53.862255 1762949 vendor/google.golang.org/grpc/internal/channelz/logging.go:73  grpc: addrConn.createTransport failed to connect to {192.168.1.152:26257  <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 192.168.1.152:26257: connect: connection refused". Reconnecting...
W200918 18:04:54.862449 1762949 vendor/google.golang.org/grpc/internal/channelz/logging.go:73  grpc: addrConn.createTransport failed to connect to {192.168.1.152:26257  <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing cannot reuse client connection". Reconnecting...
W200918 18:04:54.862545 1762946 vendor/google.golang.org/grpc/internal/channelz/logging.go:73  grpc: addrConn.createTransport failed to connect to {192.168.1.103:26257  <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing cannot reuse client connection". Reconnecting...
----

Note that .103 and .152 are other cockroachdb server nodes: n5 and n4, respectively.  Those should both be offline -- and they are, and have been since 15:35Z.  Why those messages then?  Actually, these seem to be spat out every 30 seconds ro so.  So this is probably a red herring.

I just realized that the `client_loop` error may have come from `ssh` -- around 11:05am PT and 11:15am PT.  I'm not sure why it would have affected those two and not the other load generator, and at different times, too.  I don't see anything too correlated in the system log on the load generator:

[source,text]
----
Sep 18 18:08:16 loadgen0 sshd[2335]: [ID 800047 auth.error] error: kex_exchange_identification: Connection closed by remote host
Sep 18 18:24:10 loadgen0 sshd[2610]: [ID 800047 auth.error] error: kex_exchange_identification: Connection closed by remote host
----

Obvious sources would be:

- NAT state drop in my home router.  (No log message in the remote side?)
- NAT state drop inside AWS?

but why not the first ssh session?  Maybe I should use ServerAliveInterval to keep these open.

Anyway, there's no indication of a CockroachDB problem here.  I'm going to restart the two workloads.

20:44Z: restarted those two workloads
20:59:24: brought up cockroachdb on db3.
22:15Z: brought up cockroachdb on db4
23:12Z: brought up cockroachdb on db5
23:35Z: start load generators pointed at db3, db4, db5

Summary of results:

* When I brought up db3 at 20:59: there was a spike in p95 latency to over 40ms (from about 18ms) and a brief dip in throughput from about 280 qps to about 70 (but the period was so short that that time average is probably not meaningful).  After that, p95 latency was slightly lower than before, and throughput was slightly better (about 320qps).
* When I brought up db4 at 22:15Z: throughput shot up to about 460qps.  p95 latency improved accordingly.
* When I brought up db5 at 23:12Z, there was another spike in latency and brief dip in throughput, after which they were both about the same as before.  However, three nodes were still processing 0 requests.
* When I brought up the extra three load generators at 23:35Z, request throughput evened out across all six nodes.  Overall throughput increased a fair bit, but it's ranged from 620-750qps -- not super consistent yet (as of 23:53Z).

Through all of this:

* CPU utilization on all CPUs has been below 80%.
* There have been two disk %busy spikes over 75%, both very brief -- mostly they've been below 65%.
* Average disk I/O time has largely been under 1ms, with a few spikes as high as 15ms or so.  (Interestingly, that 15ms outlier correlates with 5ms spent queued in the device driver, which is very rare -- that generally doesn't exceed a few tens of microseconds.)
* p99 CockroachDB heartbeat latency has peaked at around 850ms, correlated with an 800ms spike in p99 round-trip latency around that time.
* The error rate has peaked at about 3 per second.  It's largely been zero, with spikes around 2100Z, 22:05Z, 22:15Z, 23:15Z.  The biggest spikes have been around when we bring up new nodes.  I'm not sure I'm looking at SQL errors though -- these might all be internal errors.

This has been good operational experience playing around with these operations, but feels a bit unfocused.

---

Stepping back, here are a few things I would like to better understand:

* What's the performance impact of making write requests to the leaseholder of a range vs. one of the other nodes with a replica vs. one of the other nodes in the cluster?
* How does that inform the best way to do basic load testing?
* When is splitting better for load?  When is merging better?  How can you tell from the metrics?

Is it worth trying to demonstrate horizontal scalability by starting up, say, 6 load generators pointing at a 3-node cluster, then expanding it to 6 nodes?  (Or, equivalently? starting 6 load generators, each pointing to a node in the 6-node cluster I already have, and then gradually repointing a load generator at one of the first three as I remove the last three nodes?)

It might be worth stepping back to better understand what we'd really like to know, which is probably something like:

* how is a moderately heavy read-write workload affected when one node disappears
** for just a minute?
** for a while (at least long enough for the cluster to rebalance)?
** immediately after that node comes back?
** for an extended period after that node comes back (long enough for the cluster to rebalance)
 In particular, I think we want to know the change in throughput, tail latency, and errors.
* how does a moderately heavy read-write workload run for an extended period (at least 24h, maybe a week)
* similar questions for a few different actions:
** kill -9 a node
** OS panic a node
** hardware reset a node
** decommission a node
** drain a node
** introduce a partition around a node
*** for just a little while
*** for an extended period

Next steps:

* Make sure I know how to monitor SQL errors.
* Figure out tests to answer my questions above.
