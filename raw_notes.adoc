// Include a Table of Contents on the left hand side.
:toc: left
// ":icons: font" is needed for adminition and callout icons.
:icons: font

= Raw notes

== Up to 2020-08-25

* basic research: AWS, Terraform, CockroachDB deployment, OmniOS
** find / build a simple load generator. (will use "cockroach workload" for now)
** calculate how much it will cost to run a small cluster on AWS for a while.
*** three instances
*** Prometheus
*** Grafana
*** 1-3 load generators
* Successfully used Terraform to provision a 3-node CockroachDB cluster on AWS, using OmniOS and https://sysmgr.org/~jclulow/tmp/cockroach.tar.gz[Joshua's CRDB binaries].  You need to manually run `cockroach init --insecure --host ...` on one of them to get the cluster to come up.

== 2020-08-26

* fixed bugs in Terraform config
** cockroachdb SMF service was disabled on reboot (was using `svcadm enable -t`)
** `terraform apply` could fail if the VPC subnet wound up in us-west-2d because our instance types aren't supported there
** it would be convenient if the instance names didn't have spaces
** it would be convenient if there were a single tag for all of our instances so
we could select them without relying on my specific key
* successful cold start
* lots of NTP issues: see GitHub issue #1.  These appear to be mitigated.

== 2020-08-27

Summary of the day:

* Ran into a lot of issues with NTP.  Installed Chrony.  The issues appear
  resolved.
* Got workloads running.  Exercised a bunch of the options for duration, ramp-up time, percent reads, etc.

Details follow.

* Three databases, 1 load generator.  Each load generator can only be pointed at one database, so this shouldn't be too heavy for the whole cluster, but let's see what happens.
* I'm going to start with the "kv" worklaod.

 /cockroachdb/bin/cockroach workload init kv postgres://root@192.168.1.152:26257?sslmode=disable
/cockroachdb/bin/cockroach workload run kv --histograms kv-histograms-$(date +%FT%TZ).out postgres://root@192.168.1.152:26257?sslmode=disable

Things to play with:

--ramp
--max-rate
--max-ops
--read-percent
--tolerate-errors

I let that run for about 25-30 minutes.  End of the run:

[source,text]
----
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
 1661.0s        0          830.6          802.5      4.7      6.3      9.4     24.1 write
 1662.0s        0          827.3          802.5      4.7      6.3      9.4     16.8 write
 1663.0s        0          820.8          802.5      4.7      6.6     12.1     17.8 write
 1664.0s        0          808.1          802.5      4.7      6.6     11.5     16.8 write
 1665.0s        0          789.3          802.5      5.0      7.1      9.4     16.3 write
 1666.0s        0          764.4          802.5      5.0      7.3     11.0     16.8 write
 1667.0s        0          806.0          802.5      5.0      6.8      8.9     15.7 write
 1668.0s        0          803.0          802.5      4.7      6.6     11.0     23.1 write
 1669.0s        0          787.9          802.5      5.0      6.8      8.4     18.9 write
 1670.0s        0          809.2          802.5      5.0      6.8      9.4     12.1 write
 1671.0s        0          799.8          802.5      5.0      7.1      9.4     15.7 write
 1672.0s        0          838.8          802.5      4.7      6.3     11.0     19.9 write
 1673.0s        0          840.4          802.5      4.5      6.3     11.0     16.3 write
 1674.0s        0          806.9          802.5      4.7      7.3      9.4     14.7 write
^CHighest sequence written: 1343922. Can be passed as --write-seq=R1343922 to the next run.

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
 1674.6s        0        1343922          802.5      5.0      4.7      6.8     11.5     65.0  write

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__result
 1674.6s        0        1343922          802.5      5.0      4.7      6.8     11.5     65.0
----

This created kv-histograms-2020-08-27T17:29:16Z.out.

I'm going to try it again for a few minutes to see if the initial spike in latency is one-time or not.

[source,text]
----
$ /cockroachdb/bin/cockroach workload run kv --histograms kv-histograms-$(date +%FT%TZ).out --duration 5m postgres://root@192.168.1.152:26257?sslmode=disable 

...

Highest sequence written: 239288. Can be passed as --write-seq=R239288 to the next run.

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
  300.0s        0         239284          797.6      5.0      4.7      6.8     12.6    125.8  write

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__result
  300.0s        0         239284          797.6      5.0      4.7      6.8     12.6    125.8
----

This created kv-histograms-2020-08-27T17:59:04Z.out.

The latency spike up front happened again.

Let's try out the --max-rate option to place a cap at 500 operations.  (I accidentally used --max-ops first, which exited quickly!)

cockroachdb@ip-192-168-1-192:~$ /cockroachdb/bin/cockroach workload run kv --histograms kv-histograms-$(date +%FT%TZ).out --max-rate 500 postgres://root@192.168.1.152:26257?sslmode=disable 

That seemed to work reasonably well.  There are a ton of metrics in the Admin UI dashboard!

[source,text]
----
_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
  362.6s        0         178192          491.4      4.0      3.7      5.8     12.1     88.1  write

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__result
  362.6s        0         178192          491.4      4.0      3.7      5.8     12.1     88.1  
----

This created kv-histograms-2020-08-27T18:08:37Z.out.

Let's try `--ramp`.  I used 30s first, but that's too fast to really see the effect.  I'm going to try this again with 5m.

[source,text]
----
cockroachdb@ip-192-168-1-192:~$ /cockroachdb/bin/cockroach workload run kv --histograms kv-histograms-$(date +%FT%TZ).out --ramp=5m --max-rate 700 postgres://root@192.168.1.152:26257?sslmode=disable 
...
^CHighest sequence written: 588373. Can be passed as --write-seq=R588373 to the next run.

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
  655.4s        0         435950          665.2      4.7      4.5      6.6     11.5     56.6  write

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__result
  655.4s        0         435950          665.2      4.7      4.5      6.6     11.5     56.6
----

This created kv-histograms-2020-08-27T18:18:53Z.out.  That seemed to do what I expected -- ramped up over several minutes and capped around 700.

The histogram file looks to be per-second histograms.

I want to throw some reads into the mix, but one of the nodes has become "suspect" because its clock is too far off.  I'm starting to get:

[source,text]
----
W200827 18:41:29.280504 1064 kv/kvserver/replica_range_lease.go:555  [n2,s2,r10/
3:/Table/1{4-5}] can't determine lease status of (n2,s2):3 due to node liveness
error: node not in the liveness table
(1) attached stack trace
  | github.com/cockroachdb/cockroach/pkg/kv/kvserver.init
  |     /ws/cockroach/gopath/src/github.com/cockroachdb/cockroach/pkg/kv/kvserve
r/node_liveness.go:44
  | runtime.doInit
  |     /opt/go/1.14.4/src/runtime/proc.go:5420
  | runtime.doInit
  |     /opt/go/1.14.4/src/runtime/proc.go:5415
  | runtime.doInit
  |     /opt/go/1.14.4/src/runtime/proc.go:5415
  | runtime.doInit
  |     /opt/go/1.14.4/src/runtime/proc.go:5415
  | runtime.doInit
  |     /opt/go/1.14.4/src/runtime/proc.go:5415
  | runtime.main
  |     /opt/go/1.14.4/src/runtime/proc.go:190
  | runtime.goexit
  |     /opt/go/1.14.4/src/runtime/asm_amd64.s:1373
----

Two of them have gone into maintenance now.

Several hours later: I've built and deployed chrony to these boxes to see if
this goes better.  Let's go ahead and run that mixed workload I wanted to do
next.

[source,text]
----
$ /cockroachdb/bin/cockroach workload run kv --histograms kv-histograms-$(date +%FT%TZ).out --read-percent=30 --ramp=5m postgres://root@192.168.1.152:26257?sslmode=disable 
...
^CNumber of reads that didn't return any results: 2.
Highest sequence written: 2550079. Can be passed as --write-seq=R2550079 to the next run.

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
 3465.5s        0        1028361          296.7      2.0      1.9      3.0      5.0     67.1  read

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
 3465.5s        0        2395944          691.4      4.9      4.7      6.8     11.0    201.3  write

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__result
 3465.5s        0        3424305          988.1      4.0      4.5      6.6     10.0    201.3
----

I let this run for about an hour.  This created kv-histograms-2020-08-27T22:54:19Z.out.  Note that this file has two lines per second -- one for reads and ones for writes.

The clocks are consistently within 1ms of each other now (!).  This workload is running well.

At about 2020-08-27T23:16Z, I activated statement diagnostics for the UPSERT that this thing is running to see what it does.  This produced a bundle that was 23 bytes (0 bytes downloaded, for some reason).  This looks like this bug fixed in https://www.cockroachlabs.com/docs/releases/v20.2.0-alpha.3.html[v20.2.0-alpha.3]:

> Fixed a bug causing the raw trace file collected inside a statement diagnostics bundle to be sometimes empty when the cluster setting sql.trace.txn.enable_threshold was in use. #50914

although in our case `sql.trace.txn.enable_threshold` is 0 (disabled).  Maybe not the same issue.

== 2020-08-31

Went through:

* https://www.cockroachlabs.com/docs/v20.1/learn-cockroachdb-sql.html[Learn CockroachDB SQL] (this was just basic SQL)
** https://www.cockroachlabs.com/docs/v20.1/developer-guide-overview.html[Developer Guide]
** Skipped exercises under https://www.cockroachlabs.com/docs/v20.1/deploy-a-test-cluster.html[Test deployment] -- these were too basic or exercised K8s behavior.
** Skimmed the https://www.cockroachlabs.com/docs/v20.1/performance.html[Performance Guide]
** https://www.cockroachlabs.com/docs/v20.1/monitoring-and-alerting.html[Prometheus stuff]
** Skimmed https://www.cockroachlabs.com/docs/v20.1/configure-replication-zones.html[Replication Zones]
** https://www.cockroachlabs.com/docs/v20.1/manage-long-running-queries.html[Long-running queries]
** Read through https://www.cockroachlabs.com/docs/v20.1/remove-nodes.html[Decommision nodes]
** Read through https://www.cockroachlabs.com/docs/v20.1/disaster-recovery.html[disaster recovery]
** Skimmed through https://www.cockroachlabs.com/docs/v20.1/troubleshooting-overview.html[Troubleshooting section]

Exercised replication + rebalancing tutorial:

* Started with a cluster with 65 ranges: internal data + some poking around with the "movr" dataset.
* That's 65 ranges with replication factor 3 divided across 3 nodes = 65 replicas per node (confirmed).
* Started a fourth node: expect ~48 replicas per node (65 ranges times replication factor 3 divided by 4 nodes)
* Final state: between 46 - 50 replicas per node.  Stopped slightly before I expected, but well within reasonable.

Now I want to decommission that fourth node.

```
/cockroachdb/bin/cockroach node decommission 4 --insecure --host 192.168.1.46
...
  id | is_live | replicas | is_decommissioning |   membership   | is_draining
-----+---------+----------+--------------------+----------------+--------------
   4 |  true   |        0 |        true        | decommissioned |    false
(1 row)

No more data reported on target nodes. Please verify cluster health before removing the nodes.
```

For good measure, I drained it before disabling it:

```
root@ip-192-168-1-46:~# /cockroachdb/bin/cockroach node drain --insecure --host 192.168.1.46
node is draining... remaining: 1
node is draining... remaining: 0 (complete)
ok
root@ip-192-168-1-46:~# svcadm disable -s cockroachdb
root@ip-192-168-1-46:~#
```

Then I removed it with Terraform.  (Fortunately, just decrementing the count of db nodes caused Terraform to want to destroy this one and not some other one.)

After a few minutes, the UI reports the node as decommissioned.

---

I'm now switching over to fleshing out more of the deployment: Prometheus + Grafana for better situational awareness, plus haproxy so I can do more interesting load testing like shutting off individual nodes.

---

Prometheus:
* building from scratch for illumos
** need: golang, nodejs, yarn
*** added OmniOSce "extra" publisher
*** installed golang 1.14 (plus add path)
*** installed nodejs 12
*** used `npm install -g yarn` (plus add path)
*** needed to install gnu-tar and put that onto PATH before tar
*** needed to set TMPDIR=/var/tmp because /tmp isn't big enough.
*** needed to build `promu` first because the build doesn't have a binary for that but doesn't handle that case.  See https://elatov.github.io/2020/04/monitoring-other-targets-with-prometheus/#compiling-node_exporter-on-omnios[here].  Worked around as described there, by pulling `promu` source.
*** also needed to apply patch below to client_unix.go.

[source,text]
----
diff --git a/vendor/github.com/docker/docker/client/client_unix.go b/vendor/github.com/docker/docker/client/client_unix.go
index 178ff6740..69fb1b48f 100644
--- a/vendor/github.com/docker/docker/client/client_unix.go
+++ b/vendor/github.com/docker/docker/client/client_unix.go
@@ -1,4 +1,4 @@
-// +build linux freebsd openbsd netbsd darwin dragonfly
+// +build linux freebsd openbsd netbsd darwin dragonfly illumos

 package client // import "github.com/docker/docker/client"
----

Grafana: huge pain, but ultimately:
* need at least 8G of memory (!)
* install yarn, node, go, etc.
* git clone
* git checkout # tag you want
* `rm -rf packages/grafana-e2e`
* `yarn install --pure-lockfile` or whatever
* `yarn start` or whatever (might be able to use `go run build.go build-frontend` instead)
* `go run build.go build`
* `go run build.go pkg-archive`
* (appeared to be missing `make build` (for `make build-js`) there?)

== 2020-09-01

* Set up elastic IP for my dev zone.  This looks like about $44/year if my instance were off the whole year, which seems reasonable.
* Next:
** manual deployment of Prometheus in "mon" VM
** manual deployment of Grafana in "mon" VM
** automatically distribute "db" cluster config to all VMs so that we can easily source this in cockroach tools and Prometheus service discovery
** automate deployment of Prometheus + Grafana
** set up haproxy in load generator VM(s)
** set up other Prometheus exporters (blackbox_exporter, node_exporter)

Setting up "mon" VM:

* use user called "mon" for Prometheus and Grafana
* /export/home/mon/{bin,etc,grafana,var/prometheus/data}

So it will look like:

[source,text]
----
/export/home/mon/bin/prometheus
/export/home/mon/etc/prometheus.yml
/export/home/mon/var/prometheus/data/...
/export/home/mon/grafana/
----

prometheus to be invoked as: prometheus --storage.tsdb.path=... --config.file=... &
restart, stop with :kill?  (that doesn't make sense)
refresh: kill -HUP?

NOTE: cockroachdb on one node went into maintenance on boot again because of clock issues.  This time, chrony had definitely finished starting before cockroachdb went into maintenance.  Is this going to be a serious problem?

I did eventually get Prometheus set up pulling from CockroachDB.

I tried running Grafana, but found that my build was busted in a way that only fails when you go to configure a data source in the web UI.

Finally got that fixed.

TODO:
* Some kind of alias for my ssh tunnels:
`ssh -L9090:192.168.1.4:9090 -L3000:192.168.1.4:3000 -L80:192.168.1.220:8080 root@52.10.93.230`
* Package up the "mon" VM and redeploy it with Terraform
** use EC2 service discovery?
* Grafana tweaks:
** config file, generated data should live outside of the unpacked tarball so
   it's easier to upgrade
** include dashboards?  but these might need to be tweaked
** how do I see client queries failing due to unavailability of the cluster?
* https://github.com/prometheus/blackbox_exporter
* https://github.com/prometheus/node_exporter
