// Include a Table of Contents on the left hand side.
:toc: left
// ":icons: font" is needed for adminition and callout icons.
:icons: font

= Control plane database testing

See https://53.rfd.oxide.computer/[RFD 53 ("Control plane data storage requirements")] for background.

== Current status

Again, see RFD 53 for context.

The code in this repo uses Terraform to provision a multi-node CockroachDB cluster on AWS, using OmniOS and https://sysmgr.org/~jclulow/tmp/cockroach.tar.gz[Joshua's CRDB binaries].  This covers:

* terraform configuration to deploy the whole thing
* working illumos builds (and configuration and SMF manifests) for:
** cockroachdb
** Prometheus
** Grafana
** sysbench
** chrony
** Prometheus's node_exporter
* monitoring:
** generic VM metrics: boot time, CPU utilization, etc.
** illumos-specific metrics: I/O latency and utilization, network throughput
** CockroachDB metrics
** Grafana and Prometheus metrics
** many of these use AWS-based service discovery

We've gained a bunch of basic operational experience about CockroachDB:

* general: starting/stopping nodes, pulling metrics, understanding the key ones
* running workloads with both `cockroach workload` and `sysbench`
* observing behavior using the metrics available
* expanding the cluster
* decommissioning nodes
* directing load to individual nodes
* ranges: replicas, splits, merges, rebalancing, reading through range history, basic metrics, availability, using commands to observe them (`SHOW RANGES ...`)

We've learned a bit about it too:

* Very broadly, CockroachDB has worked well.
** To my knowledge, it hasn't crashed except because of NTP sync.  Since deploying chrony, that's only ever happened on boot, and not most of the time.  (That problem has not been root-caused.)  (That said, it _is_ very sensitive to NTP sync and we'll need to operationalize this well.)
** Generally, it's worked as advertised: we've brought up new nodes, decomissioned nodes, etc., and it's mostly worked fine.
* Performance can vary somewhat significantly (10% to 100%) when requests are issued directly to the leaseholder of a range vs. issued to some other node.  Further, CockroachDB dynamically adds new ranges, splits existing ranges, merges existing ranges, and rebalances ranges for both storage and load.  These two behaviors are both reasonable, but they complicate the testing process significantly if you're looking at absolute performance numbers because considerable variation is possible even when you haven't made any changes to the workload (because of these background changes).
* We ran into issues early on using haproxy as a client-side load balancer.  These aren't necessarily bugs nor deal-breakers, but we switched to directing each load generator at a specific cluster instance to better control and understand what was going on.
* Not related to CockroachDB but AWS: "gp2" storage volumes have significantly better performance for the first 5.4M IOPS after boot, which can easily be several hours.  After that, their rate is very inconsistent unless you're consistently saturating the max rate for your volume size.  These are unsuitable for our testing.
* We've verified sysbench numbers reasonably close to what CockroachDB reports.
* We've done some basic testing of online expansion and contraction of a cluster.  It works okay, but we found pretty long periods (20 minutes) of pretty massive latency degradation (~3-5 minutes where latency shoots to 10 or more seconds followed by 15-20 minutes of ~2-3x normal latency).
* It's never required manual intervention to restore service when enough nodes were available.


Tests done (mentioned above):

* Online expansion: verify that when clients are pointing at specific nodes, if we expand the cluster, there's no significant increase in latency or error rate, and that overall throughput of the cluster does not go down significantly.  (Based on experience so far, there may be an initial brief spike in latency (and dip in throughput) and overall throughput may go down a bit because some ranges are now remote that were previously local to some nodes.)
* Online contraction: verify that when clients are pointing at specific nodes, if we stop the load generator(s) for one node and decommission the node, then here's no significant increase in latency or error rate, and that overall throughput of the cluster does not go down significantly more than was caused by just shutting down the load generator.
* Fault tests: `kill -9`, `uadmin 2 1` (OS reboot), OS panic, transient partition, longer partition

== Next steps

There are several tests we'd still like to run.  These are largely the same questions we started with, but we understand enough now to formulate them into something testable:

* Horizontal scalability (_not_ online):
** Start by ramping up client load until we reach something reasonably heavy and evenly distributed across nodes.
** Increase the size of the cluster and add a new load generator for the new node.  We should see comparable latency, and overall throughput should increase nearly linearly.
** Continue until we reach 2x or 3x the initial cluster size.
** Critically: we're separating the testing of online expansion vs. performance after expansion because as we've learned, it's hard to control enough variables (particularly the client workloads) to combine these tests.
* Sustained heavy workload with a database larger than DRAM.  Take the 1x from above and run it for an extended period: at least 24 hours, and maybe a week.
* Absolute performance numbers (ballpark).  This is hard, given all the changes the cluster makes, and also given that we're not using top-notch hardware.  But we probably only need order-of-magnitude understanding here.  Maybe see whether latency and throughput stabilize towards the end of the sustained heavy workload test, and if so, use those numbers.
* General fault testing: `kill -9`, OS panic, hardware reset, network partition.  These may induce some latency bubbles or errors for a brief time, but the system should recover on its own.  Ideally, the bubbles and errors will be limited only to in-flight requests.
* Special fault testing: see 9/18 notes: transient and extended outages, lots of data written during an extended outage or partition.  What about recovering from zfs snapshot?
* Look back at the YCSB issues from 9/8.  We have more metrics now (I/O average latency, %busy, CPU utilization, etc.)  We're also using provisioned IOPS now.
* Look back at the KV workload issues from 9/9?  I do understand some of the internals a bit better now.
* From 9/23: better understand compactions and flushes
* From 9/23: big spikes in latency associated with spike in reads

For all of these tests, ideally we want to look at what the load generators see, what the server instances see, and what the metrics report.

If we get that far and things go well, we'll probably feel pretty good about things.  Items that we might want to test, but we're currently deferring:

* haproxy as a load balancer: we'll likely want to build a cueball-like connection pool on the client.  If haproxy can work instead, that may be a bonus.
* online schema changes: these are supposedly fine, traditionally painful everywhere, and probably time-consuming to test well in this context
* rolling upgrade: similar to schema changes
* Rust PostgreSQL client: we'll need this to work well, but it seems unlikely that another technology's client is better than the PostgreSQL one _and_ that we'd pick that technology for that reaso
* Clocks going backwards?

Other things we'll want to think through:

* Backup / restore.  (Does Changefeed help here?)
* Replication of part of the namespace elsewhere?  Does the fact that CockroachDB already replicates ranges (and can drain/decommission nodes) basically cover this, on the grounds that we can use that to move instances between servers, racks, filesystems, etc.?  Check with Arjen about this.
* When would a range need to be split using https://www.cockroachlabs.com/docs/v20.1/split-at[split-at] because of load?
** Go through https://www.cockroachlabs.com/docs/v20.1/performance.html[perf tuning exercises] in detail?

Other enhancements we could do:

* collect stats from workload runners to verify client-observed latencies.  `cockroach workload` makes this available.

== Using this repo

To deploy a cluster, you need to have:

- terraform configured using your AWS account
- an ssh key configured in AWS called "dap-terraform" OR change locals.ssh_key_name in terraform/nodes.tf to refer to your key's name
- a bunch of binaries downloaded by hand into this repo.  There's not a great way to assemble this yet, but .gitignore can tell you what they are and where they go.

**With those prerequisites in place**, you can construct the tarball to be used on each host:

[source,text]
----
$ cd vminit
$ make
----

Then upload these to the S3 bucket:

[source,text]
----
$ aws s3 cp vminit-common.tgz s3://oxide-cockroachdb-exploration/vminit-common.tgz
$ aws s3 cp vminit-cockroachdb.tgz s3://oxide-cockroachdb-exploration/vminit-cockroachdb.tgz
$ aws s3 cp vminit-mon.tgz s3://oxide-cockroachdb-exploration/vminit-mon.tgz
----

Then use terraform to deploy the cluster:

[source,text]
----
$ cd terraform
$ terraform apply
----

This will emit the public and private IPs of all the nodes in the cluster.  Note the private IP address of any of the database nodes, then log into the load generator and run:

[source,text]
----
$ ssh root@$LOADGEN_PUBLIC_IP
$ configure_cluster --host DB_PRIVATE_IP
----


== Known issues

* cockroachdb: We're currently working on a build from master from the summer.  We should switch to a release build and make sure we're exercising Pebble.  (We are exercising Pebble now, but if we switch to the latest release as of this writing, we will be back on RocksDB.)
* cockroachdb: Readline functionality (e.g., up arrow to see previous command) doesn't work in `cockroach sql` shell
* this repo: None of this is currently easily reproducible from scratch because setting up the VMs relies on several tarballs built from this repo, but the contents of them don't exist in this repo (because it would involve checking in a bunch of large binaries that we don't want to carry on forever).  The best solution I've come up with for this is to put these binaries into a submodule that's incorporated here.  That way, people casually working on the repo don't need to download these binaries (and we don't necessarily need to download them forever when we clone, even if we change the way all this works), but it'll still all be present.
* chrony setup: Sometimes a cold start of the VMs leaves CockroachDB in maintenance, having crashed because its clock was too far out of sync.  This should not be possible because we're starting chrony and configuring it to wait until it has successfully sync'd the clock (with step, not slew) _before_ starting CockroachDB on all nodes.  Still, it happens sometimes.
* cockroachdb: Before you've initialized the CRDB cluster, if you go to the adminui, you get a very blank 404 page
* terraform: we sometimes hit: https://github.com/terraform-providers/terraform-provider-aws/issues/12533. Retrying `terraform apply` has worked around the issue.
* cockroachdb: I tried activating statement diagnostics for an UPSERT that one of the workloads runs to see what that does.  This produced a bundle that was 23 bytes (0 bytes downloaded, for some reason).  This may have been a known bug (see raw notes file) but I'm not sure.  https://www.youtube.com/watch?v=xUw8dN-yJU4&feature=emb_logo[There's a good, short video showing the data in these bundles.]
* cockroachdb: flags for the `cockroach workload` command do not match the online docs


== General notes

CockroachDB recently changed the default from RocksDB to PebbleDB, despite the documentation (even for the build that I'm using) not having been updated to reflect that.

To make terraform forget about something: `terraform state rm aws_instance.db[0]`

To list _all_ instances created with a particular key:

[source,text]
----
aws ec2 describe-instances --filters 'Name=key-name,Values=dap-terraform' --query 'Reservations[*].Instances[*].{Name:Tags[?Key=='"'"'Name'"'"']|[0].Value,InstanceId:InstanceId,StateName:State.Name,Internal:PrivateIpAddress,Public:PublicIpAddress}' --output json  | json -a | json -ga InstanceId StateName Internal Public Name | column -t | sort -k7n
----

To list instances created for this exploration:

[source,text]
----
aws ec2 describe-instances --filters 'Name=tag:Project,Values=crdb_exploration' --query 'Reservations[*].Instances[*].{Name:Tags[?Key=='"'"'Name'"'"']|[0].Value,InstanceId:InstanceId,StateName:State.Name,Internal:PrivateIpAddress,Public:PublicIpAddress}' --output json  | json -a | json -ga InstanceId StateName Internal Public Name | column -t | sort -k5
----

To stop the instances:

[source,text]
----
aws ec2 describe-instances --filters 'Name=tag:Project,Values=crdb_exploration' 'Name=instance-state-name,Values=running' --query 'Reservations[*].Instances[*].{Instance:InstanceId}' | json -a | json -ga Instance | xargs -t aws ec2 stop-instances --instance-ids
----

To start the instances:

[source,text]
----
aws ec2 describe-instances --filters 'Name=tag:Project,Values=crdb_exploration' 'Name=instance-state-name,Values=stopped' --query 'Reservations[*].Instances[*].{Instance:InstanceId}' | json -a | json -ga Instance | xargs -t aws ec2 start-instances --instance-ids
----

== For further digging

* https://www.cockroachlabs.com/docs/v20.1/cluster-setup-troubleshooting#capacity-planning-issues[Capacity planning issues]
* https://www.cockroachlabs.com/docs/v20.1/cluster-setup-troubleshooting#memory-issues[Memory issues].

Has any work been done on ideal block size?  ZFS performance?  (ZIL/slog?)

Is it possible to split a cluster (e.g., to create a secondary copy for other purposes, like backup)?

What do all the metrics mean?  Many of them aren't well documented.

* range operations: why are ranges added and removed aside from splitting and growing?  I saw a bunch of removes and adds just adding a new node to the cluster.  The metric is ranges, not replicas.

Can you manually rebalance ranges (e.g., if there's some imbalance?)

Is it expected that we'd see such massive impacts to latency when adding or removing nodes?

It seems that `cockroach workload run kv` never reports a latency more than about 10 seconds, even when a node is partitioned for two minutes.  (It also reported no errors during that period.)  That seems wrong.  Relatedly? I have found that the p99 has only ever shot up to 10s, even when throughput seemed to go to zero for a few minutes (i.e., when expanding the cluster).  I haven't verified whether that's wrong or not.  I also saw 10s heartbeat latency for a node that was partitioned -- although most of the data points were actually 4.55s -- incredibly stable.

== Caveats

* Currently https://www.cockroachlabs.com/docs/v20.1/recommended-production-settings#storage[limited to 4 TiB of storage per node].
* https://www.cockroachlabs.com/docs/v20.1/recommended-production-settings#load-balancing[They expect clients to load balance for performance and reliability.]
* Regarding https://news.ycombinator.com/item?id=20098942[use of something like ZFS snapshots for backup].
* https://www.cockroachlabs.com/docs/v20.1/known-limitations.html#cold-starts-of-large-clusters-may-require-manual-intervention[Ugly looking bug around cluster startup]
* https://www.cockroachlabs.com/docs/v20.1/rename-table#table-renaming-considerations[Table renaming is not transactional]


== References

* https://www.cockroachlabs.com/docs/stable/deploy-cockroachdb-on-aws.html[CockroachDB on AWS]
* https://kbild.ch/blog/2019-02-18-awsprometheus/[Prometheus on AWS].
* https://www.slideshare.net/mitsuhirotanda/prometheus-on-aws-63736540[Prometheus on AWS] (slide deck)
* https://github.com/oxidecomputer/storage-exploration[Adam's Terraform config for storage exploration]
* https://aws.amazon.com/ec2/instance-types/[AWS Instance Types]
* https://github.com/oxidecomputer/confomat-oxide[Josh's confomat stuff]
* http://wiki.omniosce.org/GeneralAdministration[OmniOS administration]
* https://console.aws.amazon.com/ec2/v2/home?region=us-west-2#Instances:sort=instanceId[AWS EC2 console (us-west-2)]
* https://www.terraform.io/docs/cli-index.html[Terraform CLI docs]
* https://docs.aws.amazon.com/cli/latest/reference/ec2/describe-instances.html[AWS describe-instances CLI]
* https://github.com/prometheus/haproxy_exporter#official-prometheus-exporter[haproxy Prometheus support]
