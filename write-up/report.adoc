// Include a Table of Contents on the left hand side.
:toc: left
// ":icons: font" is needed for adminition and callout icons.
:icons: font

= CockroachDB testing summary

The Oxide system's control plane requires a low-touch, highly-available, horizontally-scalable, strongly-consistent data store (see <<rfd48>>) for storing a variety of data including user configuration, hardware inventory, and runtime state.  The specific requirements around scale, performance, and functional and non-functional requirements are discussed in <<rfd53>>.  A number of modern database systems (generally grouped under the term "NewSQL") seek to meet these requirements.  This document describes our initial experiences with CockroachDB.

This document starts out with our <<_goals>> and an <<_executive_summary_of_results>> for those less interested in the nitty details.  The rest of the document goes into much more detail, starting with <<_why_cockroachdb,why we focused on CockroachDB>> and some <<_cockroachdb_basics,basic concepts>> for evaluating CockroachDB and our test results.  Then we go into more detail about the tests run, caveats, areas of technical risk, etc.

== TODO in this document

* more detailed summary of key results
** configurations tested
** version of CockroachDB
** cluster sizes
** workload sizes
** no tuning done, including CockroachDB caches
** tests run
* double-check logs to verify no crashes

== Goals

Our primary goal here is to select a technology to use as a database for the control plane.  Naturally, we'd like to select an existing system that meets our requirements rather than build one from scratch.  The underlying technical problems are quite complex.  Even if the best candidate falls short, we may still be better off adapting it (and potentially our usage of it) rather than starting from scratch.

Secondarily, we want to uncover particular areas of focus for future testing and development to mitigate major problems found with whatever technology we choose.

Importantly, our goal here is _not_ to completely de-risk the choice of database, but to gain enough confidence to justify moving forward with it.

There is little here about absolute performance, as our specific requirements in this domain are not particularly hard.  (See <<rfd53>>.)

== "Executive" summary of results

(probably not brief enough to be called "executive")

We started as described in <<rfd53>> by surveying popular technologies, primarily CockroachDB, Yugabyte, and TiKV/TiDB.  Based on documentation and user experience reports, we chose CockroachDB as the most promising candidate for more serious testing.  See <<NewSQL-notes>> for raw notes from this initial survey, and see <<_why_cockroachdb>> below for why we started with CockroachDB. 

We tested basic operation, online expansion and contraction, and several fault scenarios.  Through all this, we found lots to like about CockroachDB.  Some of these might seem like low bars (e.g., no data loss), but they're nevertheless critical and so worth mentioning:

* We did not observe any data loss.
* We did not observe inconsistency (in the CAP sense), though we were not testing for this directly.
* We did not observe any unexpected crashes.  There's one category of issue where CockroachDB crashes by design (loss of clock synchronization) that we expect can be managed.
* CockroachDB never required operator intervention to re-establish availability after failures.  Nodes quickly and reliably rejoined the cluster and began serving traffic.
* Despite our inducing various failures both transient and extended, the cluster always converged to a state of sufficient replication (as measured by its own metrics) without operator intervention.
* The documentation has generally been good: it adequately describes the system's setup and operation.  We did have some <<_open_technical_questions, open questions>> not answered by the docs.
* Tooling has generally been good.  There are built-in tools for managing the cluster (e.g., decommissioning nodes), running a variety of workloads, etc.  The built-in web Admin UI gives good overall situational awareness.
* The system provides good coverage of metrics about its operation, plus other observability tools (e.g., query analysis, various reports about problem nodes or ranges, etc.).  Many of these are fairly unpolished (e.g., the report about hot ranges is just a JSON endpoint).

There's also some disappointment:

* In a lot of cases, both expansion and contraction of the cluster resulted in brief periods (2-3 minutes) where almost no requests completed at all and longer periods (20-30 minutes) where average and p95 latency were significantly increased (2x-3x).  Some of these tests were on systems that were underpowered in terms of IOPS, and things were better on systems with more IOPS available, but it still wasn't great.
* There can be considerable variance in performance (10% to 100%) depending on which node receives a particular query.  This is particularly true for workloads with a working set in memory.  Since the cluster automatically splits ranges, merges ranges, and moves leaseholders around, performance can be inconsistent over time even for a uniform workload.  This automatic adaptation is probably good overall, but capacity planning and SLO management could be more difficult as a result.
* In the few cases we wanted them, there didn't seem to be a lot of operational controls.  For example, we observed one node being particularly hot.  It did not seem that there was a way to tell CockroachDB to move some of its leases to other nodes.  (CockroachDB does try to be completely hands-off, and a fair objection to such a control is that if the system is managing leases, it may well decide to move those leases back right after the operator moves them.  Really, you don't _want_ to have to manually configure this.)
* The primary risk identified from user experience reports is that the built-in (non-Enterprise) backup/restore option is not suitable for many production clusters.  We did not dig into this and it's probably one of the biggest open risks.
* Most of the technical content that we found on the web comes directly from CockroachDB.  It's great that they have so much useful documentation, but is it worrisome that there _isn't_ more from a large, active user base?  Would we be too dependent on the company?  (It's not clear any of the other NewSQL systems are any better in this regard.)

Again, there are lots of functionality, fault conditions, and stress scenarios that we did not test.

IMPORTANT: **Our conclusion is that CockroachDB is solid enough to continue moving forward with and it's not worth spending comparable time right now evaluating other options.**

== Why CockroachDB

Why did we start with CockroachDB over the other NewSQL options?  Most of the NewSQL family of databases have similar properties:

* architecturally based on Google's Spanner
* SQL-like interface
* strong consistency (in the CAP sense)
* horizontal scalability, including expansion without downtime
* reasonably tight dependency on synchronized clocks
* support mutual authentication of both clients and other server nodes using TLS

**It seems fairly likely that any of the big options would work for us.  It also seems reasonably likely that any one of them might have some major issue that we won't discover until we're pretty far down the path of using it.**

For us, the most appealing, differentiated things about CockroachDB are:

* It has a https://www.cockroachlabs.com/docs/v20.1/architecture/overview.html#goals-of-cockroachdb[strong focus on hands-off operation].  Initial setup is a good example of this.  There's only one component to deploy, and you just need to point it at enough other instances to find the cluster.  By contrast, with TiDB, there are several components to deploy, which means independently monitoring their availability and utilization and independently scaling them out.  The https://docs.pingcap.com/tidb/dev/production-deployment-using-tiup[documented options] for TiDB deployment include Kubernetes, Ansible, and https://docs.pingcap.com/tidb/stable/tiup-overview[TiUP], the last of which appears to be a full-fledged package manager _and_ cluster management tool.
* It has a very strong <<CockroachDB-Jepsen-Report>>.  The Jepsen report for CockroachDB was glowing.  The reports for https://jepsen.io/analyses/yugabyte-db-1.3.1[Yugabyte] and https://jepsen.io/analyses/tidb-2.1.7[TiDB] showed some serious issues, including several operational issues.  It's important to remember that these reports are about a year old and the serious issues have likely been addressed.  Relatedly, Yugabyte's public blog post claimed (and as of September 2020 still claims) to have passed Jepsen, a claim so misleading that the Jepsen report added a note at the top saying that's not true.
* It's range-sharded, meaning that keys are sorted rather than hashed.  This is critical for enabling pagination in large collections.  https://www.cockroachlabs.com/blog/unpacking-competitive-benchmarks/[CockroachDB discusses this and other issues in a blog post] (obviously very biased by the source, but the technical details appear accurate).  By contrast, Yugabyte is primarily hash-sharded.  (Yugabyte supports range sharding but our notes show that as of May it appeared to lack active rebalancing for them.  This functionality https://docs.yugabyte.com/latest/architecture/docdb-sharding/tablet-splitting/#automatic-tablet-splitting-beta[appears to be supported in beta] now.)

Yugabyte is completely open-source (as opposed to CockroachDB, which is under the Business Source License).  It also https://blog.yugabyte.com/why-we-built-yugabytedb-by-reusing-the-postgresql-query-layer/[directly uses the PostgreSQL query execution engine, so it supports more PostgreSQL functionality out-of-the-box].  In the above-linked post, CockroachDB claims this makes it harder for Yugabyte to distribute query execution, but we did not dig into this claim.

TiDB is also open-source and the company, PingCap, has https://pingcap.com/blog/tag/Rust[written a lot about their use of Rust] (although only parts of TiDB are in Rust).  https://docs.pingcap.com/tidb/stable/mysql-compatibility[TiDB emphasizes MySQL compatibility] rather than PostgreSQL.


== CockroachDB basics

It's important to understand some fundamentals about CockroachDB just to know how to test it, let alone evaluate it in detail.

=== Architecture

CockroachDB exposes a SQL interface using the PostgreSQL wire protocol and consumers https://www.cockroachlabs.com/docs/stable/install-client-drivers.html[typically use a regular PostgreSQL client].  SQL queries are served by whatever node the client sends the request to, which is called the **gateway node**.  The expectation is that clients load-balance requests across nodes in the cluster or that the cluster is deployed behind a load balancer like haproxy or EBS. 

Internally, https://www.cockroachlabs.com/docs/stable/architecture/distribution-layer.html#overview[all data is kept in a key-value store].  The entire key space is sorted and divided into **Ranges**, primarily based on size (512 MiB by default).  Each Range has some number of **Replicas** corresponding to the configured **replication factor**.  Ranges are split based on size and https://www.cockroachlabs.com/docs/v20.1/load-based-splitting.html[load].  They can also be https://www.cockroachlabs.com/docs/v20.1/range-merges.html[merged based on size].

For **writes,** there's a separate instance of the Raft consensus algorithm for each Range, based on the nodes that hold Replicas for that Range.  Writes are directed to the Raft leader for the Range and write requests always run through Raft consensus algorithm to ensure strong consistency.

**Reads** do _not_ go through Raft: instead, there's a **leaseholder** for the Range.  This is one of the nodes with a Replica for this Range, and it's almost always the same node as the Raft leader.  All reads for a Range are directed to the leaseholder, which can generally serve the request from its own copy.  In cases where strong consistency might be violated, reads are sometimes delayed.

To summarize: the gateway node turns the request into key-value operations that are distributed to other nodes: the Raft leader (for writes) or leaseholder (for reads) for the Range associated with each key.  For more, see https://www.cockroachlabs.com/docs/v20.1/architecture/reads-and-writes-overview.html[Reads and Writes in CockroachDB] and https://www.cockroachlabs.com/docs/v20.1/architecture/life-of-a-distributed-transaction.html#overview["Life of a Distributed Transaction"].


=== Fault tolerance

Transient failures of individual nodes do not significantly affect reads or writes.  Based on the basics above, we'd expect that:

* For any Range where the failed node is not the Raft leader, writes would be largely unaffected, since the Raft cluster can quickly achieve consensus with the remaining nodes.
* For any Range where the failed node is not the leaseholder, reads would be unaffected, since only the leaseholder is used for reads.
* For a Range where the failed node is the Raft leader or leaseholder, write or read requests would be unavailable (respectively).  However, no data needs to be moved for the leader or leaseholder to be moved to one of the other Replicas.  (Again, we're talking about transient failures.)

CockroachDB declares a node dead if it https://www.cockroachlabs.com/docs/v20.1/cluster-setup-troubleshooting.html#node-liveness-issues[hasn't heartbeated to the cluster] for https://www.cockroachlabs.com/docs/v20.1/demo-fault-tolerance-and-recovery.html#step-5-simulate-a-single-node-failure[5 minutes].  When that happens, the Ranges that had Replicas on that node will be declared _under-replicated_.  The cluster picks new nodes to host replacement Replicas, and data is copied from the nodes that are still available.  This can have a notable performance impact while data is flying around.


=== Surviving multiple failures

It's critical to understand that **the number of nodes in the cluster is not the same as the replication factor**.  Suppose you have a cluster of 7 nodes configured with replication factor 3 (the default).  With 7 nodes, you might think that you could maintain availability even while losing two nodes.  That's wrong: consider the Ranges that have Replicas on both of those nodes.  (With enough Ranges in the system, it's likely that _some_ will have a replica on each of the two failed nodes.)  Those Ranges only have one Replica available, which is not enough for consensus.  Such Ranges will be unavailable.

It's important to remember that the replication factor determines how many failures you can survive.  Adding cluster nodes alone only increases capacity (in terms of storage and performance), not availability.


=== Client behavior and retries

As mentioned above, CockroachDB uses the PostgreSQL wire protocol so that you can use a standard PostgreSQL client.  Cockroach Labs provides https://www.cockroachlabs.com/docs/v20.1/third-party-database-tools["beta" level support for rust-postgres] and the team appears to have https://github.com/sfackler/rust-postgres/issues/171#issuecomment-218832633[contributed improvements to that crate].

Under some conditions, in order to maintain strong consistency when multiple transactions modify the same data, CockroachDB aborts a transaction with a retryable error.  In many cases, CockroachDB automatically retries the transaction.  In the rest of cases, it's up to the client to do so when it receives the appropriate error code.  According to the docs, some client libraries automatically handle these cases, and even if not, it's fairly straightforward: you just issue a `ROLLBACK` and try again.  For more, see the https://www.cockroachlabs.com/docs/v20.1/transactions#transaction-retries[documentation on transaction retries].  Server-side retries are automatic as long as the statements are issued to CockroachDB as a batch and the results are small enough that they're buffered rather than streamed.  These conditions are under the client's control.


=== Automatic background activities

CockroachDB automatically does a few things that have potentially significant impact on performance:

* https://www.cockroachlabs.com/docs/v20.1/frequently-asked-questions.html#how-does-cockroachdb-scale[splits ranges based on size]
* https://www.cockroachlabs.com/docs/v20.1/load-based-splitting.html[splits ranges based on load]
* https://www.cockroachlabs.com/docs/v20.1/range-merges.html[merges ranges based on size]
* https://www.cockroachlabs.com/docs/v20.1/architecture/replication-layer#load-based-replica-rebalancing[moves replicas based on load]
* (unverified) moves leases to other replicas?
* (unverified) moves replicas based on available capacity?

These can dramatically impact performance.  In particular, load-based splitting can split a busy Range into two less-busy Ranges.  If a different node becomes the new Range's leaseholder, then the original busy load can be successfully split across two nodes.


=== Implications for testing

CockroachDB's assumption that clients will distribute load evenly to available cluster nodes (which is generally a fine approach) complicates our testing.  If fault testing includes a load balancer, it would be easy to end up testing the behavior of that load balancer and not the cluster itself.  If we leave out the load balancer, then each client is directed at a particular cluster node, and that client will see failures whenever that node is offline.  We need to discount those failures if we're only trying to assess the cluster's behavior.  (In principle, we do care about the load balancer and client-side behavior as they relates to availability, but in practice, we have good reason to believe we can build this ourselves as long as the server behaves reasonably.  So we want to test the server's behavior now rather than build a perfect client first.)

Performance testing is affected by the way requests are distributed from gateway nodes.  Consider a 3-node cluster where clients are distributing requests evenly to all three nodes, but where the workload is concentrated on one Range.  In this case, we'd expect the Raft leader and leaseholder for the active Range to have notably lower latency (by at least one internal network round-trip) and higher throughput -- and this is what we observed.

When the cluster decides to split Ranges or move leaseholders, overall latency and throughput can suddenly change significantly, even though nothing is wrong.  If that happens during fault testing, care must be taken not to assume that the fault caused the change in performance.  We'd expect this effect to be small when the number of Ranges is high enough that any one split or leaseholder move is a small fraction of the overall load.


== Summary of the tests

**Online expansion**: while pointing one load generator at each node in a 3-node cluster, increase the cluster gradually to 6 nodes and observe latency, throughput, and error rate.  We were not looking for improved latency or throughput -- that winds up being complicated by various other factors and we decided that was better for a separate horizontal scalability test -- but just to know that latency and error rate were not significantly impacted.  Unfortunately, in most cases, the cluster did stop serving requests for a few minutes and then performed poorly for the next 20-30 minutes while data was moved around.

**Online contraction**: similar to online expansion, with similar results.  In this case, we started with one load generator for the first three nodes in a 6-node cluster.  Then we gradually decommissioned nodes and observed the latency, throughput, and error rate.  The results were similar to expansion.

**Long-running workload**: we ran one workload for 240 hours (over 9 days) to look for any major degradation.  Overall, this went well, though there were occasional brief spikes in latency and comparable degradation in throughput.

We also ran several kinds of **fault testing**:

* **`kill -9`** instances of CockroachDB.  This had virtually no affect on the cluster.  The killed node was serving requests again in single-digit seconds.  Only in-flight requests seemed to be affected.
* **Reboot the OS** on the system hosting one node.  This had virtually no affect on the cluster.  This node was back up and serving requests within 90 seconds, nearly all of that being OS reboot time.  Only in-flight requests seemed to be affected.
* Initiate an **OS panic** on the system hosting one node.  (This is similar to a reboot, but behaves more like a network partition, since TCP connections are not gracefully torn down.)  This looked nearly the same as an OS reboot except that it took a little longer for the OS to come back up.
* **Transient single-node partition**: use firewall rules to introduce a partition around a cluster node for less than the default 5-minute timeout for declaring a node "dead".  There were some oddities around the client-side reporting (see <<_open_technical_questions>>), but the overall impact was good.  There were no errors, and while latency rose, it was less than ambient flunctuations for the previous 30 minutes.  Queries per second dropped across the cluster and throughput on all nodes went down.  All nodes' CPU usage and disk throughput when down.  This is probably because one load generator was off, not because one node was down.
* **Long single-node partition**: use firewall rules to introduce a partition around a cluster node for longer than the default 5-minute timeout.  We saw similar oddities around client-side latency, but the overall impact was good.  There were some multi-second latency outliers on a bunch of nodes but they were mostly beyond p99.

See <<_details_on_tests_run>> for more details.

== Caveats and limitations on these results

We wound up doing a lot of _ad hoc_ testing (sometimes in response to unexpected issues with a given test).  While we tried to control variables, it's possible that some results are path-dependent.  For example, our long-running workload test was run on the same cluster that had been expanded and contracted again at least once, and it's possible it would have different performance characteristics than one that had not gone through that process.  Relatedly, although we were reasonably careful with data collection, a more fully-automated process that also collected data regularly from the load generators would reduce the possibility of problems we missed.

We did not end up directly verifying horizontal scalability (that is, in a controlled test).  We saw it in practice during expansion and contraction activities, but we didn't scale up or down the workload to really prove it.

We used a pretty limited number of workloads: primarily the "kv" (key-value) workload that ships with the https://www.cockroachlabs.com/docs/stable/cockroach-workload.html[`cockroach workload`] tool.  This was sufficient to exercise reads and writes, with some control over the size of writes and the fraction of read requests.  We also used the same tool to populate our large databases.  Results could be very different for data that looks very different, as might happen with larger payloads, more varying payload size, less well-distributed keys, use of secondary indexes, etc.

We only ran tests on AWS, using fairly small instance types, on illumos, using one version of CockroachDB.  This was a beta version using their new https://www.cockroachlabs.com/blog/pebble-rocksdb-kv-store/[PebbleDB], a custom reimplementation of RocksDB.  PebbleDB is the default in the next official version, which is why we wanted to test that.

We did not do any significant performance work like tuning the filesystem or networking stack or CockroachDB itself.  It's possible we could see improvements in absolute performance from that work.

There are lots of tests that we considered, but did not try out:

* Backup/restore.
* Online schema changes.
* Rolling upgrade.
* Horizontal scalability in a controlled experiment.  We saw this in practice during expansion and contraction, but we didn't scale up or down the workload to really prove it.
* Asymmetric network partitions (or even any partitions involving more than one node).
* System hangs (e.g., `pstop`).
* Running the clock backwards.
* ZFS snapshot rollback on one or more nodes.
* Recovery when one Replica has been offline for an extended period and lots of data has been written to the Range when it comes back.
* Any sort of storage GC stress-testing (e.g., deleting a very large amount of data in a short period and seeing the impact when it gets collected later).
* Any sort of testing of haproxy as a load balancer.

Some of these may be worth digging deeper into.  Others may be obviated by other choices we make.  For example, we may want to build a smarter client-side load balancer and not use haproxy.

== Areas of technical risk

These correspond with areas that we didn't test, described above.  Here we explain the big ones.

[cols="2,1,1,6",options="header"]
|===
| Area
| Likelihood
| Impact
| Details

| Backup/restore
| Moderate
| Moderate
| https://news.ycombinator.com/item?id=23154250[Users] https://www.openmymind.net/Migrating-To-CockroachDB/[report] that what's supported in the non-Enterprise CockroachDB is not suitable for production clusters, but we haven't dug into this.  Further, it's https://news.ycombinator.com/item?id=20098704[not clear that it would be valid to simply take ZFS snapshots and replicate them], as they couldn't be coordinated across the cluster.  It's possible that we'll need to implement our own backup/restore system.  On the other hand, while this is not a small project, it seems bounded in scope, particularly if we allow the backup to not represent a single point in time.

| Online schema changes
| Low-moderate
| Moderate
| This is https://www.cockroachlabs.com/docs/stable/online-schema-changes.html[supposed to work], but may be operationally complex.  In the worst case, we may have to build application-level awareness of these changes, which people have been doing for a long time with traditional RDBMSs.

| Rolling upgrade
| Low-moderate
| Moderate
| This is https://www.cockroachlabs.com/docs/v20.1/upgrade-cockroach-version[supposed to work], but may be operationally complex.  On the other hand, we don't have reason to believe other systems are substantially better here.  Sadly, many systems wind up taking planned downtime for upgrades.

| Horizontal scalability
| Low
| Moderate
| Horizontal scalability is a very fundamental part of the system here and everything we know about the design suggests that it will work.  Our non-controlled tests show it in action.

| Inconsistent performance due to debt
| Moderate
| Low-moderate
| Most database systems have background activities (like storage GC) that build up and can affect performance.  That CockroachDB partitions data into relatively small ranges (512 MiB by default) may mitigate how much of the database can be in such a state at once.  We can run lots of tests to smoke out these issues, but only running workloads comparable to production for very extended periods can give us high confidence here.

| Client functionality and reliability
| Moderate
| Low-moderate
| Good performance and availability requires robust and fully-functional client implementations, where our choice of language (Rust) may not have seen a lot of focus.  On the plus side, CockroachDB speaks the PostgreSQL wire protocol, so we can likely benefit from strong interest there, and CockroachDB supports rust-postgres as "beta".

It seems pretty likely that we'll want to build our own client-side load balancing system similar to Joyent's https://joyent.github.io/node-cueball/internals.html[cueball].  (A https://docs.rs/cueball/0.3.5/cueball/index.html[Rust implementation of cueball] does exist already, and there's also https://docs.rs/r2d2/0.8.9/r2d2/[r2d2].)

| Instability due to lack of clock sync
| Low
| Low
| A CockroachDB node crashes when its clock offset is more than 500ms from the cluster mean.  This was initially a major challenge on AWS, but use of chrony and NTP has easily kept clocks in sync within 1ms over a weeklong test.

|===

In all cases, we can mitigate the risks with more testing.

One area that's hard to assess is the lack of a replication escape hatch.  <<rfd53>> talks about "logical replication as a primary feature" because when a system is capable of replicating chunks of the namespace elsewhere, many difficult problems become much simpler, like moving databases between machines, reconfiguring storage, offline analysis, testing, etc.  It's unclear if CockroachDB has a mechanism like this.  "changefeed" is probably the most interesting area to explore here.  However, the replication that it _does_ have first-class does support a lot of these use cases.  For example, if we wanted to change the filesystem record size, we could bring up a fleet of nodes with the new filesystem configuration and decommission the old ones.  The question is whether there are important use cases where the built in replication isn't enough.  Examples might include: constructing a whole second copy of the cluster for testing purposes.

Other areas we didn't test that _should_ work include mutual client and server authentication using TLS.


== Open technical questions

Is it expected that we’d see such massive impacts to latency when adding or removing nodes?

Has any work been done on ideal block size? ZFS performance? Use of ZIL/slog?

Is it possible to split a cluster (e.g., to create a secondary copy for other purposes, like backup)?  You could almost do this by deploying 2x the nodes and temporarily doubling the replication factor.  This would result in something that it feels like you could split into two clusters.  However, the actual split would probably need to be coordinated via Raft: one side would necessarily wind up in a minority and there would need to be an explicit step to have it elect a new majority.

What do all the metrics mean? Many of them aren’t well documented.  Some are named confusingly.  For example: what are range "adds" and "removes"?  They don't seem to correlate with when a range is created.  They seem to correlate with when a replica is moved -- so maybe that reflects a new replica being craeted and an old one removed?  But the stat is definitely named with "range", not "replica".

Can you manually rebalance the set of replicas or leaseholders on a node?

In cases where the system has seemed totally stuck (no requests completing), we seem to see a latency of 10.2 seconds and 0 errors.  We saw this from `cockroach workload run kv`, even in the extreme case where the gateway node that that command was pointed at was partitioned via a firewall rule for two whole minutes.  In almost all cases, I've never seen the p99 exceed 10.2 seconds even when throughput went to zero for few minutes (e.g., when expanding the cluster).  I also saw 10s heartbeat latency for a node that was partitioned, although most of the data points were incredibly stable at 4.55s.  What gives?  Are these special timeout values?  Why do we see 0 errors in many of these cases?


== Details on tests run

We surveyed technologies for 1-2 weeks in mid-May, 2020.  As described in <<rfd53>>, we went through official documentation, Jepsen reports, public blog posts, and reports from users about their experiences with the technology.  We tested CockroachDB for about 6 weeks in late August to early October, 2020.  This process included:

* basic tooling and automation around deploying CockroachDB on illumos on AWS
* building and deploying other software we needed, including Prometheus, Grafana, haproxy with Prometheus support, etc.
* getting our feet wet with CockroachDB itself and learning enough about how it works to test it effectively
* iterating on various tests to eliminate irrelevant issues (like bottlenecks on I/O due to choice of AWS volume type)
* the actual tests that we wanted to run: moderately heavy workloads, online expansion, online contraction, and several fault scenarios

XXX XXX TODO more details here


== Other lessons learned

As part of this work, we also learned a bunch about AWS, largely related to I/O performance.

The typical baseline EBS volume is "gp2" class, a general-purpose SSD-based network volume.  We initially used these volumes for testing because it's fairly cheap and we weren't intending to measure absolute performance.  https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#EBSVolumeTypes_gp2[gp2 volumes provide a certain number of IOPS depending mostly on the volume's size]; what's tricky, though, is that they also support bursting way above their baseline performance, and worse (for our use case): they start with a significant "credit" ostensibly to speed up boot time, which might use more I/O than steady-state.  They can run significantly faster for the first several _hours_ than they will after that.  It took some time for us track this down as the cause of suddenly-dropping database performance.

To avoid bursting, we switched to more expensive https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#EBSVolumeTypes_piops["io1" class volumes], which provide more consistent performance at whatever level you specify.  We also did some testing using EC2 instance types with directly-attached NVME storage ("i3" instance types).  Those are nominally cheaper, but all data is lost when the instance is shut down, so it needs to remain running 24/7 as long as the cluster might ever remain in use, so it winds up being more expensive for this sort of testing.


[bibliography]
== References

There are many links in the text above (that are not included here) to official CockroachDB and AWS documentation.

* [[[rfd48, RFD 48]]] https://48.rfd.oxide.computer/[RFD 48 Control Plane Architecture]
* [[[rfd53, RFD 53]]] https://53.rfd.oxide.computer/[RFD 53 Control plane data storage requirements]
* [[[CockroachDB-Jepsen-Report, Jepsen report on CockroachDB]]] http://jepsen.io/analyses/cockroachdb-beta-20160829[Jepsen report on CockroachDB]
* [[[NewSQL-notes, Notes on NewSQL Databases]]] https://github.com/oxidecomputer/meta/blob/master/engineering/Notes-on-NewSQL-distributed-databases.adoc[Notes on NewSQL databases]
