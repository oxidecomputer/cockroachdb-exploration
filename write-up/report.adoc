// Include a Table of Contents on the left hand side.
:toc: left
// ":icons: font" is needed for adminition and callout icons.
:icons: font

= CockroachDB testing summary

The Oxide system's control plane requires a low-touch, highly-available, horizontally-scalable, strongly-consistent data store (see <<rfd48>>) for storing a variety of data including user configuration, hardware inventory, and runtime state.  The specific requirements are discussed in <<rfd53>>.  A number of modern database systems (generally grouped under the term "NewSQL") seek to meet these requirements.  This document describes our initial experiences with CockroachDB.

== TODO in this document

** more detailed summary of key results
*** configurations tested
*** tests run
* double-check logs to verify no crashes
* add links for everything that I can -- there ought to be quite a lot

== Goals

* Select a technology to use as a database for the control plane.  Naturally, we'd like to select an existing system that meets our requirements rather than build one from scratch.  The underlying technical problems are quite complex, and it seems likely that even if the best candidate falls short, we may still prefer to adopt that technology and adapt it (and potentially our usage of it) rather than start from scratch.
* Uncover particular areas of focus for future testing and development to mitigate major deficiencies found with the technology we choose.

Importantly, our goal here is not to completely de-risk the choice of database, but to gain enough confidence to justify moving forward with it.

We also spent very little time on absolute performance numbers, as our specific requirements in this domain are not particularly hard.

== Summary of results

We started as described in <<rfd53>> by surveying popular technologies, primarily CockroachDB, Yugabyte, and TiKV/TiDB.  Based on documentation and user experience reports, we chose CockroachDB as the most promising candidate for more serious testing.  We tested basic operation, online expansion and contraction, and several fault scenarios.

Through all of our testing, we found a number of promising things about CockroachDB.  Some of these might seem like low bars (e.g., no data loss), but they're nevetheless critical and so worth mentioning:

* We did not observe any data loss.
* We did not observe inconsistency (in the CAP sense), though we were not testing for this directly.
* We did not observe any unexpected crashes.  There's one category of issue that we expect can be easily managed where CockroachDB crashes by design (loss of clock synchronization).
* CockroachDB never required operator intervention to re-establish availability after failures.  Nodes quickly and reliably rejoined the cluster and began serving traffic.
* Despite various failures both transient and extended, the cluster always converged to a state of sufficient replication (at least in terms of its own metrics) without operator intervention.
* The documentation has generally been good: it adequately describes the system's setup and operation.  We did have some questions (described below) not answered by the docs.
* Tooling has generally been pretty good.  There are built-in tools for managing the cluster (e.g., decommissioning nodes), running a variety of workloads, etc.  The built-in web Admin UI gives good overall situational awareness.
* The system provides good coverage of metrics about its operation, plus other observability tools (e.g., query analysis, various reports about problem nodes or ranges, etc.).  Many of these are fairly unpolished (e.g., the report about hot ranges is just a JSON endpoint).

There are some disappointing signs too:

* In a lot of cases, both expansion and contraction of the cluster resulted in brief periods (2-3 minutes) where almost no requests completed at all and longer periods (20-30 minutes where average and p95 latency were significantly increased (2x-3x).  Some of these tests were on systems potentially underpowered in terms of IOPS, and things were better on systems with more IOPS, but it still wasn't great.
* There can be considerable variance in performance (10% to 100%) depending on which node receives a particular query.  This is particularly true for workloads with a working set in memory.  Since the cluster automatically splits ranges, merges ranges, and moves leaseholders around, performance can be inconsistent over time even for a uniform workload.  This is probably good overall, but capacity planning and SLO management could be more difficult as a result.
* In the few cases we wanted them, there didn't seem to be a lot of operational controls.  For example, we observed one node being particularly hot.  It did not seem that there was a way to tell CockroachDB to move some of its leases to other nodes.  (CockroachDB does try to be completely hands-off, and a fair objection to such a control is that if the system is managing leases, it may well decide to move those leases back.  You don't really _want_ to have to manually configure these things.)
* The primary risk identified from user experience reports is that the built-in (non-Enterprise) backup/restore option is not suitable for many production clusters.  We did not dig into this and it's probably one of the biggest open risks.
* Most of the technical content that we found on the web comes directly from CockroachDB.  It's great that they have so much useful documentation, but is it worrisome that there _isn't_ so much from a large, active user base?  Would we be too dependent on the company?  (It's not clear any of the other NewSQL systems are any better in this regard.)

Again, there are lots of functionality, fault conditions, and stress scenarios that we did not test.

Our conclusion is that CockroachDB is solid enough to continue moving forward with and it's not worth spending comparable time right now evaluating other options.


== More details on testing

We surveyed technologies for 1-2 weeks in mid-May, 2020.  As described in <<rfd53>>, we mostly went through official documentation, Jepsen reports, public blog posts, and reports from users about their experiences with the technology.

We tested CockroachDB for about 6 weeks in late August to early October, 2020.  This process included:

* basic tooling and automation around deploying CockroachDB on illumos on AWS
* building and deploying other software we needed, including Prometheus, Grafana, haproxy with Prometheus support, etc.
* getting our feet wet with CockroachDB itself and learning enough about how it works to test it effectively
* iterating on various tests to eliminate irrelevant issues (like bottlenecks on I/O due to choice of AWS volume type)
* the actual tests that we wanted to run: moderately heavy workloads, online expansion, online contraction, and several fault scenarios

The most useful tests we ran:

* Online expansion: while pointing one load generator at each node in a 3-node cluster, increase the cluster gradually to 6 nodes and observe latency, throughput, and error rate.  We were not looking for improved latency or throughput -- that winds up being complicated by various other factors and we decided that was better for a separate horizontal scalability test -- but just to know that latency and error rate were not significantly impcated.  Unfortunately, in most cases, the cluster did stop serving requests for a few minutes and then performed poorly for the next 20-30 minutes while data was moved around.
* Online contraction: similar to online expansion, with similar results.  In this case, we started with one load generator for the first three nodes in a 6-node cluster.  Then we gradually decommissioned nodes and observed the latency, throughput, and error rate.  The results were similar to expansion.
* Long-running workload: we ran one workload for 240 hours (over 9 days) to look for any major degradation.  Overall, this went well, though there were occasional brief spikes in latency and comparable degradation in throughput.
* Fault testing:
** `kill -9` instances of CockroachDB.  This had virtually no affect on the cluster.  The killed node was serving requests again in single-digit seconds.  Only in-flight requests seemed to be affected.
** Reboot the OS on the system hosting one node.  This had virtually no affect on the cluster.  This node was back up and serving requests within 90 seconds, nearly all of that being OS reboot time.  Only in-flight requests seemed to be affected.
** Initiate an OS panic on the system hosting one node.  (This is similar to a reboot, but appears more like a network partition, since TCP connections are not gracefully torn down.)  This looked nearly the same as an OS reboot except that it took a little longer for the OS to come back up.
** Transient partition: use firewall rules to introduce a partition around a cluster node for less than the default 5-minute timeout.  There were some oddities around the client-side reporting (see open questions below), but the overall impact was good.  There were no errors and while latency rose, it was less than ambient flunctuations for the previous 30 minutes.  Queries per second dropped across the clsuter and throughput on all nodes went down.  All nodes' CPU usage and disk throughput when down, probably because one load generator was off and not because one node was down.
** Long partition: use firewall rules to introduce a partition around a cluster node for longer than the default 5-minute timeout.  We saw similar oddities around client-side latency, but the overall impact was good.  There were some multi-second latency outliers on a bunch of nodes but they were mostly beyond p99.

For more details, see below.

=== Why CockroachDB

Most of the NewSQL family of databases have similar properties:

* architecturally based on Google's Spanner
* SQL-like interface
* strong consistency (in the CAP sense)
* horizontal scalability, including expansion without downtime
* reasonably tight dependency on synchronized clocks
* support mutual authentication of both clients and other server nodes using TLS

For us, the most appealing, differentiated things about of CockroachDB are:

* Strong focus on hands-off operation.  Initial setup is a good example of this.  There's only one component to deploy, and you just need to point it at enough other instances to find the cluster.  By contrast, with TiDB, there are several components to deploy, which means independently monitoring their availability and utilization and independently scaling them out.
* Very strong <<CockroachDB-Jepsen-Report>>.  The Jepsen report for CockroachDB was glowing.  The reports for Yugabyte and TiDB showed some serious issues, including several operational issues.  Further, Yugabyte's public blog post claimed (and as of September 2020 still claims) to have passed Jepsen, a claim so misleading that the Jepsen report added a note at the top saying that's not true.
* Range-sharded, meaning that keys are sorted rather than hashed.  This is critical for enabling pagination in large collections.  By contrast, Yugabyte is primarily hash-sharded.  (Yugabyte supports range sharding but currently lacks active rebalancing for them.  It seems decidedly not the primary use case.)

Yugabyte is completely open-source (as opposed to CockroachDB, which is under the Business Source License).  It also directly uses the PostgreSQL query execution engine, so it supports more PostgreSQL functionality out-of-the-box.

TiDB is also open-source and the company, PingCap, has posted a lot about their use of Rust (although only parts of TiDB are in Rust).  TiDB emphasizes MySQL compatibility rather than PostgreSQL.

=== Learning about CockroachDB

As mentioned above, we spent several weeks learning enough about CockroachDB to better understand how to test it.  Here's a selection of key lessons.

==== Basic architecture

CockroachDB exposes a SQL interface using the PostgreSQL wire protocol.

Internally, all data is kept in a key-value store.  The entire key space is sorted and divided into **Ranges**, primarily based on size (512 MiB by default).  Each Range has some number of **Replicas** corresponding to the configured **replication factor**.  Ranges are split based on size and load.  They can also be merged based on size.

There's a separate instance of the Raft consensus algorithm for each Range, based on the nodes that hold Replicas for that Range.  Writes are directed to the Raft leader for the Range and write reqeusts always run through Raft consensus algorithm to ensure strong consistency.

Reads do _not_ go through Raft: instead, there's a **leaseholder** for the Range.  This is one of the nodes with a Replica for this Range, and it's almost always the same node as the Raft leader.  All reads for a Range are directed to the leaseholder, which can generally serve the request from its own copy.  In cases where strong consistency might be violated, reads are sometimes delayed.

==== How requests are distributed to nodes

Quite simply: the expectation is that clients load-balance requests across nodes in the cluster or the cluster is deployed behind a load balancer like haproxy or EBS.  Absent one of these, requests are served by whatever node the client sends the request to.  This complicates fault testing because if the cluster is behind a load balancer, it would be easy to end up testing the behavior of the load balancer and not the cluster itself.

The node that receives the request is called the **gateway node**.  The request is turned into key-value operations that are distributed to the Raft leader or leaseholder for the Range associated with each key.

This further complicates any sort of performance testing, even very coarse testing to see how latency is affected by various faults.  Consider a 3-node cluster where clients distributed the workload evenly to all three nodes, but the workload is concentrated on one Range.  In this case, we'd expect the Raft leader and leaseholder for the active Range to have significantly lower latency (by at least one internal network round-trip) and higher throughput -- and this is what we observed.


==== Basic fault tolerance

Transient failures of individual nodes do not significantly affect reads or writes.  Based on the basics above, we'd expect that:

* For any Range where the failed node is not the Raft leader, writes would be largely unaffected, since the Raft cluster can quickly achieve consensus with the remaining nodes.
* For any Range where the failed node is not the leaseholder, reads would be unaffected, since only the leaseholder is used for reads.
* For a Range where the failed node is the Raft leader or leaseholder, write or read requests would be unavailable (respectively).  However, no data needs to be moved for the leader or leaseholder to be moved to one of the other Replicas.  (Again, we're talking about transient failures.)

CockroachDB declares a node dead if it hasn't heartbeated to the cluster for 5 minutes.  When that happens, the Ranges that had Replicas on that node will become _under-replicated_.  The cluster picks new nodes to host replacement Replicas, and data is copied from the nodes that are still available.  This can have a notable performance impact while data is flying around.


==== Surviving multiple failures

It's critical to understand that **the number of nodes in the cluster is not the same as the replication factor**.  Suppose you have a cluster of 7 nodes configured with replication factor 3 (the default).  With 7 nodes, you might think that you could maintain availability even while losing two nodes.  That's wrong: consider the Ranges that have Replicas on both of those nodes.  (With enough Ranges in the system, it's likely that _some_ will have a replica on each of the two failed nodes.)  Those Ranges only have one Replica available, which is not enough for concensus.  Such Ranges will be unavailable.

It's important to remember that the replication factor determines how many failures you can survive.  Adding cluster nodes alone only increases capacity (in terms of storage and performance), not availability.


==== Automatic background activities

CockroachDB automatically does a few things that have potentially significant impact on performance:

* splits ranges based on size
* splits ranges based on load
* merges ranges based on size
* moves leases to other replicas
* (unverified) moves replicas based on available capacity

These can dramatically impact performance.  In particular, load-based splitting can split a busy Range into two less-busy Ranges.  If a different node becomes the new Range's leaseholder, then the original busy load can be successfully split across two nodes.

As mentioned above, overall request latency can be much lower when a client happens to issue the request directly to the leaseholder for the Range.  Because of that, when all this background activity happens to potentially split Ranges or move leaseholders, the overall cluster's latency and throughput can vary significantly over time, even when nothing is wrong.  We'd expect this effect to be small when the number of Ranges is high enough that any one split or leaseholder move is a small fraction of the overall load.


=== Other lessons learned

This material is unrelated to CockroachDB, but worth mentioning and potentially useful to readers.  As part of this, we also learned a bunch about AWS, largely related to I/O performance.

The typical baseline EBS volume is "gp2" class, a general-purpose SSD-based network volume.  We initially used these volumes for testing because it's fairly cheap and we weren't intending to measure absolute performance.  gp2 volumes provide a certain number of IOPS depending mostly on the volume's size; what's tricky, though, is that they also support bursting way above their baseline performance, and worse (for our use case): they start with a significant "credit" ostensibly to speed up boot time, which might use more I/O than steady-state.  They can run significantly faster for the first several _hours_ than they will after that.  It took some time for us track this down as the cause of suddenly-dropping database performance.

To avoid bursting, we switched to more expensive "io1" class volumes, which provide more consistent performance at whatever level you specify.  We also did some testing using EC2 instance types with directly-attached NVME storage ("i3" instance types).  Those are nominally cheaper, but all data is lost when the instance is shut down, so it needs to remain running 24/7 as long as the cluster might ever remain in use, so it winds up being more expensive for this sort of testing.


== Caveats and limitations on these results

For details on the tests we _did_ run, see below.

We wound up doing a lot of _ad hoc_ testing (sometimes in response to unexpected issues with a given test).  While we tried to control variables, it's possible that some results are path-dependent.  For example, our long-running workload test was run on the same cluster that had been expanded and contracted again at least once, and it's possible it would have different performance characteristics than one that had not gone through that process.  Relatedly, although we were reasonably careful with data collection, a more fully-automated process that also collected data regularly from the load generators would reduce the possibility of problems we missed.

We did not end up directly verifying horizontal scalability (that is, in a controlled test).  We saw it in practice during expansion and contraction activities, but we didn't scale up or down the workload to really prove it.

We used a pretty limited number of workloads: primarily the "kv" (key-value) workload that ships with the `cockroach workload` tool.  This was sufficient to exercise reads and writes, with some control over the size of writes and the fraction of read requests.  We also used the same tool to populate our large databases.  Results could be very different for data that looks very different, as might happen with larger payloads, more varying payload size, less well-distributed keys, use of secondary indexes, etc.

We only ran tests on AWS, using fairly small instance types, on illumos, using one version of CockroachDB.  This was a beta version using their new PebbleDB, a custom reimplementation of RocksDB.  PebbleDB is the default in the next official version, which is why we wanted to test that.

We did not do any significant performance work like tuning the filesystem or networking stack or CockroachDB itself.  It's possible we could see improvements in absolute performance from that work.

There are lots of tests that we considered, but did not try out:

* Backup/restore.
* Online schema changes.
* Rolling upgrade.
* Horizontal scalability in a controlled experiment.  We saw this in practice during expansion and contraction, but we didn't scale up or down the workload to really prove it.
* Asymmetric network partitions (or even any partitions involving more than one node).
* System hangs (e.g., `pstop`).
* Running the clock backwards.
* ZFS snapshot rollback on one or more nodes.
* Recovery when one Replica has been offline for an extended period and lots of data has been written to the Range when it comes back.
* Any sort of storage GC stress-testing (e.g., deleting a very large amount of data in a short period and seeing the impact when it gets collected later).
* Any sort of testing of haproxy as a load balancer.

Some of these may be worth digging deeper into.  Others may be obviated by other choices we make.  For example, we may want to build a smarter client-side load balancer and not use haproxy.

== Areas of technical risk

These correspond with areas that we didn't test, described above.  Here we explain the big ones.

[cols="2,1,1,6",options="header"]
|===
| Area
| Likelihood
| Impact
| Details

| Backup/restore
| Moderate
| Moderate
| Consensus seems to be that what's supported in the non-Enterprise CockroachDB is not suitable for production clusters, but we haven't dug into this.  Further, it's not clear that it would be valid to simply take ZFS snapshots and replicate them, as they couldn't be coordinated across the cluster.  It's possible that we'll need to implement our own backup/restore system.  On the other hand, while this is not a small project, it seems bounded in scope, particularly if we allow the backup to not represent a single point in time.

| Online schema changes
| Low-moderate
| Moderate
| This is supposed to work, but may be operationally complex.  In the worst case, we may have to build application-level awareness of these changes, which people have been doing for a long time with traditional RDBMSs.

| Rolling upgrade
| Low-moderate
| Moderate
| This is supposed to work, but may be operationally complex.  On the other hand, we don't have reason to believe other systems are substantially better here.  Sadly, many systems wind up taking planned downtime for upgrades.

| Horizontal scalability.  
| Low
| Moderate
| Horizontal scalability is a very fundamental part of the system here and everything we know about the design suggests that it will work.  Our non-controlled tests show it in action.

| Inconsistent performance due to debt
| Moderate
| Low-moderate
| Most database systems have background activities (like storage GC) that build up and can affect performance.  That data is partitioned into relatively small ranges (512 MiB by default) may mitigate how much of the database can be in such a state at once.  We can run lots of tests to smoke out these issues, but only running workloads comparable to production for very extended periods can give us high confidence here.

| Client functionality and reliability
| Moderate
| Low-moderate
| Good performance and availability requires robust and fully-functional client implementations, where our choice of language (Rust) may not have seen a lot of focus.  On the plus side, CockroachDB speaks the PostgreSQL wire protocol, so we can likely benefit from strong interest there.

It seems pretty likely that we'll want to build our own client-side load balancing system similar to Joyent's cueball.  (A Rust implementation of cueball does exist already.)

| Instability due to lack of clock sync
| Low
| Low
| A CockroachDB node crashes when its clock offset is more than 500ms from the cluster mean.  This was initially a major challenge on AWS, but use of chrony and NTP has easily kept clocks in sync within 1ms over a weeklong test.

|===

In all cases, we can mitigate the risks with more testing.

There's another area that's hard to assess even qualitatively, which is the lack of a replication escape hatch.  <<rfd53>> talks about "logical replication as a primary feature" because when a system is capable of replicating chunks of the namespace elsewhere, many difficult problems become much simpler, like moving databases between machines, reconfiguring storage, offline analysis, testing, etc.  It's unclear if CockroachDB has a mechanism like this.  "changefeed" is probably the most interesting area to explore here.  However, the replication that it _does_ have first-class does support a lot of these use cases.  For example, if we wanted to change the filesystem record size, we could bring up a fleet of nodes with the new filesystem configuration and decommission the old ones.  The question is whether there are important use cases where the built in replication isn't enough.  Examples might include: constructing a whole second copy of the cluster for testing purposes.

Other areas we didn't test that _should_ work include mutual client and server authentication using TLS.

== Open technical questions

Is it expected that we’d see such massive impacts to latency when adding or removing nodes?

Has any work been done on ideal block size? ZFS performance? Use of ZIL/slog?

Is it possible to split a cluster (e.g., to create a secondary copy for other purposes, like backup)?  You could almost do this by deploying 2x the nodes and temporarily doubling the replication factor.  This would result in something that it feels like you could split into two clusters.  However, the actual split would probably need to be coordinated via Raft: one side would necessarily wind up in a minority and there would need to be an explicit step to have it elect a new majority.

What do all the metrics mean? Many of them aren’t well documented.  Some are named confusingly.  For example: what are range "adds" and "removes"?  They don't seem to correlate with when a range is created.  They seem to correlate with when a replica is moved -- so maybe that reflects a new replica being craeted and an old one removed?  But the stat is definitely named with "range", not "replica".

Can you manually rebalance the set of replicas or leaseholders on a node?

In cases where the system has seemed totally stuck (no requests completing), we seem to see a latency of 10.2 seconds and 0 errors.  We saw this from `cockroach workload run kv`, even in the extreme case where the gateway node that that command was pointed at was partitioned via a firewall rule for two whole minutes.  In almost all cases, I've never seen the p99 exceed 10.2 seconds even when throughput went to zero for few minutes (e.g., when expanding the cluster).  I also saw 10s heartbeat latency for a node that was partitioned, although most of the data points were incredibly stable at 4.55s.  What gives?  Are these special timeout values?  Why do we see 0 errors in many of these cases?


== Details on tests run

[bibliography]
== References

There are many links in the text above (that are not included here) to official CockroachDB and AWS documentation.

* [[[rfd48, RFD 48]]] https://48.rfd.oxide.computer/[RFD 48 Control Plane Architecture]
* [[[rfd53, RFD 53]]] https://53.rfd.oxide.computer/[RFD 53 Control plane data storage requirements]
* [[[CockroachDB-Jepsen-Report, Jepsen report on CockroachDB]]] http://jepsen.io/analyses/cockroachdb-beta-20160829[Jepsen report on CockroachDB]
* [[[NewSQL-notes, Notes on NewSQL Databases]]] https://github.com/oxidecomputer/meta/blob/master/engineering/Notes-on-NewSQL-distributed-databases.adoc[Notes on NewSQL databases]
